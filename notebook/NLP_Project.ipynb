{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKwSQ3yjxA6I"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiR2_ghKCpKk"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/EliaFeltrin/KIND_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lliCd_Qx_ZL6"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "! pip install pandas\n",
        "! pip install tqdm\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMeTdvZQ_Vz-"
      },
      "outputs": [],
      "source": [
        "# Clustering\n",
        "! pip install umap-learn\n",
        "! pip install sklearn\n",
        "! pip install nltk\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# embeddings for the clustering\n",
        "\n",
        "!pip install -U sentence-transformers\n",
        "! pip install plotly\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHxYEmkQXU71"
      },
      "outputs": [],
      "source": [
        "! pip install sklearn\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV7H9nhPIEkV"
      },
      "outputs": [],
      "source": [
        "# CRF\n",
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "!pip install eli5\n",
        "\n",
        "from sklearn_crfsuite import metrics as crf_metrics\n",
        "from sklearn_crfsuite import CRF\n",
        "import eli5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyE9wvX3OzDN"
      },
      "outputs": [],
      "source": [
        "# embeddings\n",
        "! pip install --upgrade gensim\n",
        "! pip install fasttext\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import fasttext\n",
        "import fasttext.util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Re8fDlYC585"
      },
      "outputs": [],
      "source": [
        "#spacy\n",
        "! pip install -U spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uSdgHpkxIGR"
      },
      "source": [
        "## Downloads and imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08G5PAYnAUOr"
      },
      "source": [
        "### Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLQhCXCbxy-f"
      },
      "outputs": [],
      "source": [
        "# Installing the library needed in the following part of the project\n",
        "\n",
        "# Installing the utlity packages\n",
        "#! pip install scipy\n",
        "#! pip install numpy\n",
        "! pip install pandas\n",
        "! pip install pickle\n",
        "\n",
        "# Installing a package for having the progress bar\n",
        "!pip install tqdm\n",
        "\n",
        "# Installing a natural language processing package\n",
        "! pip install nltk\n",
        "\n",
        "# Installing the packages for creating amazing plots\n",
        "#! pip install matplotlib\n",
        "! pip install wordcloud\n",
        "! pip install plotly\n",
        "! pip install --upgrade nbformat\n",
        "\n",
        "#Â Installing a package for sequence labeling, used for POS tagging and NER\n",
        "! pip install -U spacy\n",
        "\n",
        "# Installing the packages for creating the word embeddings\n",
        "! pip install --upgrade gensim\n",
        "! pip install fasttext\n",
        "\n",
        "# Installing one of the main machine learning libraries\n",
        "! pip install sklearn\n",
        "\n",
        "# Installing the packages for doing dimensionality reduction\n",
        "! pip install umap-learn\n",
        "\n",
        "# Installing packages for indexing the dataset\n",
        "! pip install python-terrier\n",
        "\n",
        "# Installing packages implementing conditional random fields\n",
        "! pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "! pip install eli5\n",
        "\n",
        "# Installing packages for transformers\n",
        "! pip install transformers==4.28.0\n",
        "! pip install datasets\n",
        "! pip install evaluate\n",
        "! pip install seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfZAu7CMRzP7"
      },
      "source": [
        "### Models download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PbNJyzNAUOs"
      },
      "outputs": [],
      "source": [
        "# Dowloading an italian model from spacy\n",
        "! spacy download it_core_news_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtEjHNHVAUOs"
      },
      "source": [
        "### Package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXMMltVMVFQO"
      },
      "outputs": [],
      "source": [
        "# Importing the main packages\n",
        "\n",
        "# Importing the utlity packages\n",
        "import string\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import pickle    \n",
        "\n",
        "# Importing a package for having the progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importing a natural language processing library\n",
        "import nltk\n",
        "\n",
        "# Importing the packages for creating amazing plots\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "\n",
        "# Importing the packages for creating the word embeddings\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "# Importing the packages for doing dimensionality reduction\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "# Importing a package for the tf-idf representation of the sentences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Importing a package for clustering\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "\n",
        "# Importing the packages for POS tagging\n",
        "import spacy as spc\n",
        "import it_core_news_lg\n",
        "\n",
        "# Importing packages for indexing the dataset\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()\n",
        "\n",
        "# Importing packages implementing conditional random fields\n",
        "from sklearn_crfsuite import metrics as crf_met\n",
        "from sklearn_crfsuite import CRF\n",
        "import eli5\n",
        "\n",
        "# Importing packages for Transformers\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import load_metric\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import evaluate\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u32r8WVcKps"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLYGT2fNAUOt"
      },
      "outputs": [],
      "source": [
        "# Defining the names of the datasets\n",
        "dataset_names_train_IO = ['degasperi_train.tsv', 'fiction_train.tsv', 'moro_train.tsv', 'wikinews_train.tsv']\n",
        "dataset_names_test_IO = ['degasperi_test.tsv', 'fiction_test.tsv', 'moro_test.tsv', 'wikinews_test.tsv']\n",
        "dataset_names_train_BIO = ['degasperi_train_BIO.tsv', 'fiction_train_BIO.tsv','moro_train_BIO.tsv', 'wikinews_train_BIO.tsv']\n",
        "dataset_names_test_BIO = ['degasperi_test_BIO.tsv', 'fiction_test_BIO.tsv','moro_test_BIO.tsv', 'wikinews_test_BIO.tsv']\n",
        "\n",
        "# Defining the path to datasets\n",
        "PATH_TO_DATASETS_IO = '/content/KIND_project/datasets/Inside_outside_NER_notation'\n",
        "PATH_TO_DATASETS_BIO = '/content/KIND_project/datasets/BIO_tag_NER_notation'\n",
        "\n",
        "# Importing all the datasets in a dictionary\n",
        "datasets_train_dict_IO = {name: pd.read_csv(PATH_TO_DATASETS_IO+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names_train_IO}\n",
        "datasets_test_dict_IO = {name: pd.read_csv(PATH_TO_DATASETS_IO+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names_test_IO}\n",
        "datasets_train_dict_BIO = {name: pd.read_csv(PATH_TO_DATASETS_BIO+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names_train_BIO}\n",
        "datasets_test_dict_BIO = {name: pd.read_csv(PATH_TO_DATASETS_BIO+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names_test_BIO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIj4qBulAUOt"
      },
      "source": [
        "## Functions for dealing with the datasets\n",
        "\n",
        "In this session we define some funtions useful for having the correct structure of the dataset in order to use in the various techniques we implement in the following part of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbJQ8lSLAUOu"
      },
      "outputs": [],
      "source": [
        "def get_string_from_df(dataframe_df, puntuaction, column_names=['Token', 'Entity']):\n",
        "  '''\n",
        "  Transforms the tokenized dataset into a single string.\n",
        "  It is assumed that the dataframe has two columns: one column containing the \n",
        "  tokens, so the words of the various sentences, by default it is named 'Token',\n",
        "  and the second containing the labels associated with that tokens, by default \n",
        "  it is named 'Entity'.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataframe_df: DataFrame\n",
        "    dataframe containing the tokenized dataset.\n",
        "  puntuaction: list\n",
        "    list containing the puntuaction, it is used in order to manage in a better\n",
        "    way the spaces in case of an element of puntuaction.\n",
        "  column_names: list, optional\n",
        "    ordered names of the columns, the one with index zero is used for extracting\n",
        "    the text from the DataFrame.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  text: str\n",
        "    string concatenating all the tokens of the dataset.\n",
        "\n",
        "  '''\n",
        "\n",
        "  text_df = dataframe_df.loc[:,column_names[0]]\n",
        "  text = text_df[0]\n",
        "  for token in text_df[1:]:\n",
        "    text += (' ' + token) if token not in puntuaction else token\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAUrIMICAUOu"
      },
      "outputs": [],
      "source": [
        "def get_sentences_list_from_df(dataset_df, key=None, column_names=['Token', 'Entity'],\n",
        "                               lower=False):\n",
        "  '''\n",
        "  Extracts and returns the sentences from the DataFrame given as input.\n",
        "  It is assumed that the dataframe has two columns: one column containing the \n",
        "  tokens, so the words of the various sentences, by default it is named 'Token',\n",
        "  and the second containing the labels associated with that tokens, by default \n",
        "  it is named 'Entity'.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_df: DataFrame\n",
        "    dataframe containing the tokenized dataset\n",
        "  key: str, optional\n",
        "    name of the dataset from which the dataframe comes from.\n",
        "  column_names: list, optional\n",
        "    ordered names of the columns, the one with index zero is used for extracting\n",
        "    the text from the DataFrame.\n",
        "  lower: bool, optional\n",
        "    if True, the all the words will be transformed into lowercase words, if\n",
        "    False, they will not.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  sentences_list: list\n",
        "    list of the sentences contained in the dataset.\n",
        "  labels_list: list\n",
        "    list of lists of labels associated to the words of the sentences contained \n",
        "    in the dataset.\n",
        "  keys_list: list\n",
        "    list containing an entry for each sentence with the name of the dataset it\n",
        "    comes from.\n",
        "\n",
        "  '''\n",
        "  \n",
        "  print('Converting the DataFrame of the dataset' + key if key != None else '' \\\n",
        "        + 'into a list of strings')\n",
        "  \n",
        "  punctuation = string.punctuation\n",
        "  sentences_list = [[]]\n",
        "  labels_list = [[]]\n",
        "  keys_list = None\n",
        "  count = 0\n",
        "\n",
        "  for element in tqdm(dataset_df.iterrows()):\n",
        "    if str(element[1]['Token']) == '.':\n",
        "      sentences_list.append([])\n",
        "      labels_list.append([])\n",
        "      count += 1\n",
        "    elif str(element[1]['Token']) not in punctuation:\n",
        "      sentences_list[count].append(element[1]['Token'].lower() if lower else element[1]['Token'])\n",
        "      labels_list[count].append(element[1]['Entity'])\n",
        "  if key != None:\n",
        "    keys_list = [key for sentence in range(len(sentences_list))]\n",
        "  return sentences_list, labels_list, keys_list\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def remove_short_sentences(sentences_list, labels_list, keys_list = None, min_length=3):\n",
        "  '''\n",
        "  Removes the sentences with a length lower than a certain threshold.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentences_list: list\n",
        "    list of the sentences to be analyzed.\n",
        "  labels_list: list\n",
        "    list of lists of labels associated to the sentences.\n",
        "  keys_list: list, optional\n",
        "    list containing the name of the dataset from which each sentence.\n",
        "  min_lenght: int, optional\n",
        "    minimum number of words a sentence has to have to be kept.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  new_sentences_list: list\n",
        "    list of the sentences contained in the dataset.\n",
        "  new_labels_list: list\n",
        "    list of lists of labels associated to the words of the sentences contained \n",
        "    in the dataset.\n",
        "  new_keys_list: list\n",
        "    list containing an entry for each sentence with the name of the dataset it\n",
        "    comes from.\n",
        "\n",
        "  '''\n",
        "\n",
        "  print('Removing short sentences from the dataset')\n",
        "    \n",
        "  new_sentences_list = list()\n",
        "  new_labels_list = list()\n",
        "  new_keys_list = list()\n",
        "  for idx in tqdm(range(len(sentences_list))):\n",
        "    if len(sentences_list[idx]) >= min_length:\n",
        "      new_sentences_list.append(sentences_list[idx])\n",
        "      new_labels_list.append(labels_list[idx])\n",
        "      if keys_list != None:\n",
        "        new_keys_list.append(keys_list[idx])\n",
        "  return new_sentences_list, new_labels_list, new_keys_list\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_all_sentences_from_datasets(datasets, lower=False):\n",
        "  '''\n",
        "  Extracts and returns the sentences from every DataFrame in the dictionary\n",
        "  given as input.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  datasets: dict\n",
        "    dictionary of DataFrames containing the tokenized datasets.\n",
        "  lower: bool, optional\n",
        "    if True, the all the words will be transformed into lowercase words, if\n",
        "    False, they will not.\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  overall_sentences: list\n",
        "    list of the sentences contained in the datasets given in input.\n",
        "  overall_labels: list\n",
        "    list of lists of labels associated to the words of the sentences contained \n",
        "    in the datasets.\n",
        "  overall_keys: list\n",
        "    list containing an entry for each sentence with the name of the dataset it\n",
        "    comes from.\n",
        "\n",
        "  '''\n",
        "\n",
        "  overall_sentences = list()\n",
        "  overall_labels = list()\n",
        "  overall_keys = list()\n",
        "  for key in datasets.keys():\n",
        "    sentences, labels, keys = get_sentences_list_from_df(datasets[key], key=key, lower=lower)\n",
        "    sentences, labels, keys = remove_short_sentences(sentences, labels, keys_list=keys)\n",
        "    overall_sentences += sentences\n",
        "    overall_labels += labels\n",
        "    overall_keys += keys\n",
        "  return overall_sentences, overall_labels, overall_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwV71KREBz0C"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(sentences, labels, stopwords):\n",
        "  '''\n",
        "  Removes the stopwords from the dataset updating the label list.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentences: list\n",
        "    list of the tokenized sentences.\n",
        "  labels: list\n",
        "    list of lists of labels associated to the sentences.\n",
        "  stopwords: list\n",
        "    list of stopwords.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  overall_sentences: list\n",
        "    list of the sentences contained in the datasets given in input.\n",
        "  overall_labels: list\n",
        "    list of lists of labels associated to the words of the sentences contained \n",
        "    in the datasets.\n",
        "\n",
        "  '''\n",
        "\n",
        "  sentences_no_stopwords = []\n",
        "  labels_no_stopwords = []\n",
        "\n",
        "  for i,sentence in enumerate(sentences):\n",
        "    new_sentence = []\n",
        "    new_labels = []\n",
        "    for j,word in enumerate(sentence):\n",
        "      if word not in stopwords:\n",
        "        new_sentence.append(word)\n",
        "        new_labels.append(labels[i][j])\n",
        "    \n",
        "    sentences_no_stopwords.append(new_sentence)\n",
        "    labels_no_stopwords.append(new_labels)\n",
        "  return sentences_no_stopwords, labels_no_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnEtUn5rAUOu"
      },
      "outputs": [],
      "source": [
        "# Defining some function useful for having the correct structure of the dataset\n",
        "# in order to define the tf-idf representation\n",
        "\n",
        "def concatenate_sentences_tokens(sentences):\n",
        "  '''\n",
        "  Recombines the tokenized senteces into complete strings concatenating the\n",
        "  tokens of each sentence.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentences: list\n",
        "    list of tokenized sentences.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  concatenated_sentences: list\n",
        "    list of sentences.\n",
        "\n",
        "  '''\n",
        "  \n",
        "  concatenated_sentences = list()\n",
        "  for sentence in sentences:\n",
        "    new_sentence = ''\n",
        "    for token in sentence:\n",
        "      new_sentence += (token + ' ')\n",
        "    concatenated_sentences.append(new_sentence)\n",
        "  return concatenated_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k66DgZvZVv9y"
      },
      "outputs": [],
      "source": [
        "def get_datasets_names():\n",
        "  '''\n",
        "  Returns a list containing the names of the datasets\n",
        "  \n",
        "  '''\n",
        "  \n",
        "  return ['degasperi', 'fiction', 'moro', 'wikinews']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_c5NAUYUbX1"
      },
      "outputs": [],
      "source": [
        "def get_numerical_mapping():\n",
        "  '''\n",
        "  Returns a dictionary containing the mapping between the names of the datasets \n",
        "  and a numerical value\n",
        "\n",
        "  '''\n",
        "\n",
        "  return {'degasperi': 0, 'fiction': 1, 'moro': 2, 'wikinews': 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQZyGI0uQwYM"
      },
      "source": [
        "## Functions for doing some plots\n",
        "In this session we define some funtions useful for creating 3D plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttvEmfEORKqB"
      },
      "outputs": [],
      "source": [
        "def plot_3D(x, y, z, labels, mapping=None, title=''):\n",
        "  '''\n",
        "  Makes the 3D plot of some vectors using different colors for different classes\n",
        "  of labels.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    coordinates on x of the points.\n",
        "  y: ndarray\n",
        "    coordinates on x of the points.\n",
        "  z: ndarray\n",
        "    coordinates on x of the points.\n",
        "  labels: list\n",
        "    list of labels, one label for each point.\n",
        "  mapping: dict\n",
        "    mapping between each label and numerical value.\n",
        "  title: str\n",
        "    title of the plot.\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Mapping the textual labels of the datasets to a number each in order to \n",
        "  # display them using different colors\n",
        "  label_mapping = {}\n",
        "  inverse_label_mapping = {}\n",
        "  if mapping == None:\n",
        "    for i, label in enumerate(list(set(labels))):\n",
        "      label_mapping[label] = i\n",
        "      inverse_label_mapping[i] = label\n",
        "  else:\n",
        "    label_mapping = mapping\n",
        "    for key in label_mapping:\n",
        "      value = label_mapping[key]\n",
        "      inverse_label_mapping[value] = key\n",
        "  numeric_labels = [label_mapping[label] for label in labels]\n",
        "\n",
        "  # Creating an empty list to store the traces\n",
        "  traces = []\n",
        "  # Iterating over the numbers labelling the datasets and creating the traces of\n",
        "  # their points\n",
        "  for numeric_label in np.unique(numeric_labels):\n",
        "    # Keeping only the points having the label considered in the iteration\n",
        "    label_indices = np.where(numeric_labels == numeric_label)[0]\n",
        "    x_label = x[label_indices]\n",
        "    y_label = y[label_indices]\n",
        "    z_label = z[label_indices]\n",
        "\n",
        "    # Creating a trace for the 3D scatter plot\n",
        "    trace = go.Scatter3d(\n",
        "        x=x_label,\n",
        "        y=y_label,\n",
        "        z=z_label,\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=3,\n",
        "            color=numeric_label,\n",
        "            opacity=0.9\n",
        "        ),\n",
        "        name=str(inverse_label_mapping[numeric_label]),\n",
        "        #text=labels[inverse_label_mapping[numeric_label]],\n",
        "        #legendgroup=labels[inverse_label_mapping[numeric_label]]\n",
        "    )\n",
        "    traces.append(trace)\n",
        "\n",
        "  # Creating the layout\n",
        "  layout = go.Layout(\n",
        "      scene=dict(\n",
        "          xaxis=dict(title='x'),\n",
        "          yaxis=dict(title='y'),\n",
        "          zaxis=dict(title='z')\n",
        "      ),\n",
        "      title=title,\n",
        "      showlegend=True\n",
        "  )\n",
        "\n",
        "  # Creating the figure and add the trace\n",
        "  fig = go.Figure(data=traces, layout=layout)\n",
        "\n",
        "  # Enabling scene manipulation\n",
        "  fig.update_layout(scene=dict(camera=dict(eye=dict(x=1, y=1, z=1))))\n",
        "\n",
        "  # Showing the plot\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdsB2Wg9393M"
      },
      "source": [
        "## Functions for computing some metrics\n",
        "\n",
        "In this session we define some funtions useful for computing the main performance indeces usually avaluated for understanding the goodness of a classificator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OysvY3j4FkV"
      },
      "outputs": [],
      "source": [
        "def get_metrics(predictions, true_labels, metrics_df=None, class_label='NotLabeled'):\n",
        "  '''\n",
        "  Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  predictions: ndarray\n",
        "    predictions of samples obtained with a model\n",
        "  true_labels: ndarray\n",
        "    true labels of the samples\n",
        "  metrics_df: DataFrame, optional\n",
        "    DataFrame to which the computed statistics have to be put\n",
        "  class_label: str, optional\n",
        "    label identifing the belonging of the statistcs to its class\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  metrics_df: DataFrame\n",
        "    DataFrame containing the statistics contained in the parameter metrics_df\n",
        "    plus the statistics computed on the new predictions\n",
        "\n",
        "  '''\n",
        "\n",
        "  TP = np.sum(np.logical_and(predictions == 1., true_labels == 1.)) # Attacks accurately flagged as attacks\n",
        "  TN = np.sum(np.logical_and(predictions == 0., true_labels == 0.)) # Normal traffic accurately flagged as normal\n",
        "  FP = np.sum(np.logical_and(predictions == 1., true_labels == 0.)) # Normal traffic incorrectly flagged as attacks\n",
        "  FN = np.sum(np.logical_and(predictions == 0., true_labels == 1.)) # Attacks incorrectly flagged as normal \n",
        "\n",
        "  AC = ((TN + TP) / len(predictions)) * 100 # Accuracy\n",
        "  RC = (TP / (FN + TP)) * 100 # Recall or sensitivity\n",
        "  PR = (TP / (FP + TP)) * 100 # Precision\n",
        "  F1 = 2 * PR * RC / (PR + RC) # F1-Score\n",
        "\n",
        "  if metrics_df is None:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = pd.DataFrame([[class_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)\n",
        "  else:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = pd.concat([metrics_df,pd.DataFrame([[class_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)])\n",
        "  \n",
        "  return metrics_df\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def performance_metrics(predicted_labels, real_labels):\n",
        "  '''\n",
        "  Computes performance metrics given the predictions and the true labels\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  predictions: ndarray\n",
        "    predictions of samples obtained with a model\n",
        "  true_labels: ndarray\n",
        "    true labels of the samples\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  metrics_df: DataFrame\n",
        "    DataFrame containing the statistics contained in the parameter metrics_df\n",
        "    plus the statistics computed on the new predictions\n",
        "\n",
        "  '''\n",
        "  \n",
        "  metrics_df = None\n",
        "  classes = set(element for sentence in real_labels for element in sentence)\n",
        "  for tag in classes:\n",
        "    new_predicted_labels = np.array([1 if element == tag else 0 for sentence in predicted_labels for element in sentence])\n",
        "    new_real_labels = np.array([1 if element == tag else 0 for sentence in real_labels for element in sentence])\n",
        "    metrics_df = get_metrics(new_predicted_labels, new_real_labels, metrics_df=metrics_df, class_label=tag)\n",
        "  \n",
        "  return metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUVSRpgmEVNV"
      },
      "source": [
        "## KIND dataset\n",
        "\n",
        "### Brief Description\n",
        "KIND (Kessler Italian Named-entities Dataset) is a dataset released in 2022 by researchers from Fondazione Bruno Kessler and the University of Trento. It contains 1 million tokens, of which 600K name-entities are manually annotated. The entities belong to 3 classes (people, location, organization). The texts come from various sources of the Italian language, such as news articles, literature, and political speeches, making it a multi-domain dataset. \n",
        "The following table shows the dataset composition:\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th rowspan=\"2\">Dataset</th>\n",
        "      <th rowspan=\"2\">Documents</th>\n",
        "      <th rowspan=\"2\">Tokens</th>\n",
        "      <th colspan=\"4\">Train</th>\n",
        "      <th colspan=\"4\">Test</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Total</th>\n",
        "      <th>PER</th>\n",
        "      <th>ORG</th>\n",
        "      <th>LOC</th>\n",
        "      <th>Total</th>\n",
        "      <th>PER</th>\n",
        "      <th>ORG</th>\n",
        "      <th>LOC</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Wikinews</td>\n",
        "      <td>1,000</td>\n",
        "      <td>308,622</td>\n",
        "      <td>247,528</td>\n",
        "      <td>8,928</td>\n",
        "      <td>7,593</td>\n",
        "      <td>6,862</td>\n",
        "      <td>61,094</td>\n",
        "      <td>1,802</td>\n",
        "      <td>1,823</td>\n",
        "      <td>1,711</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Fiction</td>\n",
        "      <td>86</td>\n",
        "      <td>192,448</td>\n",
        "      <td>170,942</td>\n",
        "      <td>3,439</td>\n",
        "      <td>182</td>\n",
        "      <td>733</td>\n",
        "      <td>21,506</td>\n",
        "      <td>636</td>\n",
        "      <td>284</td>\n",
        "      <td>463</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Aldo Moro</td>\n",
        "      <td>250</td>\n",
        "      <td>392,604</td>\n",
        "      <td>309,798</td>\n",
        "      <td>1,459</td>\n",
        "      <td>4,842</td>\n",
        "      <td>2,024</td>\n",
        "      <td>82,806</td>\n",
        "      <td>282</td>\n",
        "      <td>934</td>\n",
        "      <td>807</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Alcide De Gasperi</td>\n",
        "      <td>158</td>\n",
        "      <td>150,632</td>\n",
        "      <td>117,997</td>\n",
        "      <td>1,129</td>\n",
        "      <td>2,396</td>\n",
        "      <td>1,046</td>\n",
        "      <td>32,635</td>\n",
        "      <td>253</td>\n",
        "      <td>533</td>\n",
        "      <td>274</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td><strong>Total</strong></td>\n",
        "      <td><strong>1494</strong></td>\n",
        "      <td><strong>1,044,306</strong></td>\n",
        "      <td><strong>846,265</strong></td>\n",
        "      <td><strong>14,955</strong></td>\n",
        "      <td><strong>15,013</strong></td>\n",
        "      <td><strong>10,665</strong></td>\n",
        "      <td><strong>198,041</strong></td>\n",
        "      <td><strong>2,973</strong></td>\n",
        "      <td><strong>3,574</strong></td>\n",
        "      <td><strong>3,255</strong></td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "### Annotation Tagging scheme\n",
        "The tokens of all datasets, except for Aldo Moro, have been manually labeled using the IOB (Inside-Outside-Beginning) convention: each entity is labeled as begin-of-entity (B-[ent]) or continuation-of-entity (I-[ent]). The annotations of the Aldo Moro dataset, instead, were carried out with a mixed process that used both manual and automatic annotations (subsequently checked by hand); due to some differences in the convention for annotation, this dataset does not contain information for composite entities (beginning, continuation). For more details, please refer to the paper related with the dataset release: https://arxiv.org/abs/2112.15099\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annotation Tagging scheme\n",
        "The tokens of all datasets, except for Aldo Moro, have been manually labeled using the IOB (Inside-Outside-Beginning) convention: each entity is labeled as begin-of-entity (B-[ent]) or continuation-of-entity (I-[ent]). The annotations of the Aldo Moro dataset, instead, were carried out with a mixed process that used both manual and automatic annotations (subsequently checked by hand); due to some differences in the convention for annotation, this dataset does not contain information for composite entities (beginning, continuation). For more details, please refer to the paper related with the dataset release: https://arxiv.org/abs/2112.15099\n"
      ],
      "metadata": {
        "id": "Qw4epPoM8n_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset analysis\n"
      ],
      "metadata": {
        "id": "Xny9ERYY8q6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Same plots"
      ],
      "metadata": {
        "id": "sHpQ7aEQBsAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUxTIXvXdgoV"
      },
      "outputs": [],
      "source": [
        "BIO_PATH = './KIND_project/datasets/BIO_tag_NER_notation/'\n",
        "nonBIO_PATH = './KIND_project/datasets/Inside_outside_NER_notation/'\n",
        "\n",
        "def to_lowerCase(df):\n",
        "    return pd.DataFrame({'Token': df['Token'].str.lower(), 'Entity': df['Entity']})\n",
        "\n",
        "def add_column_names(df):\n",
        "    return  df.rename(columns={0: 'Token', 1: 'Entity'})\n",
        "\n",
        "def spider_plot(df, group, title, subplot_idx):\n",
        "    \"\"\"\n",
        "    Generate a spider plot based on the given DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        The input DataFrame containing the data for the spider plot.\n",
        "    group : list\n",
        "        A list of group names corresponding to each line in the plot.\n",
        "    title : str\n",
        "        The title of the spider plot.\n",
        "    subplot_idx : int\n",
        "        The index of the subplot in the figure.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    SPIDER_ROW_N = 2\n",
        "    SPIDER_COL_N = 2\n",
        "\n",
        "\n",
        "    #plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "    categories=list(df)[:]\n",
        "    N = len(categories)\n",
        "    \n",
        "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
        "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "    \n",
        "    # Initialise the spider plot\n",
        "    ax = plt.subplot(SPIDER_ROW_N, SPIDER_COL_N, subplot_idx, polar=True)\n",
        "    \n",
        "    # first axis to be on top:\n",
        "    ax.set_theta_offset(pi / 2)\n",
        "    ax.set_theta_direction(-1)\n",
        "    \n",
        "    # Draw one axe per variable + add labels\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "    \n",
        "    # Draw ylabels\n",
        "    ax.set_rlabel_position(0)\n",
        "    min = df.min().min()\n",
        "    max = df.max().max()\n",
        "    plt.ylim(min -(max-min)/10, max + (max-min)/10)\n",
        " \n",
        "    # Plot each individual = each line of the data\n",
        "    # I don't make a loop, because plotting more than 3 groups makes the chart unreadable\n",
        "    \n",
        "    for i in range(len(group)):\n",
        "        values=df.loc[i].values.flatten().tolist()\n",
        "        values += values[:1]\n",
        "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=group[i])\n",
        "        ax.fill(angles, values, 'b', alpha=0.1)\n",
        "\n",
        "    \n",
        "    # Add legend\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "    plt.title(title)\n",
        "\n",
        "def stacked_bar_plot(col_names, data, labels_name, title, subplot_idx):\n",
        "    BAR_ROW_N = 1 \n",
        "    BAR_COL_N = 2\n",
        "    '''\n",
        "    Create a stacked bar plot to visualize data across multiple categories.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col_names : list\n",
        "        A list of column names representing the categories for the x-axis.\n",
        "    data : list of lists\n",
        "        A nested list containing the data values for each category.\n",
        "        Each inner list corresponds to a specific group and contains the data values for the corresponding category.\n",
        "    labels_name : list\n",
        "        A list of labels representing the names of the groups.\n",
        "        The length of this list should match the number of groups in the data.\n",
        "   title : str\n",
        "       The title of the plot.\n",
        "   subplot_idx : int\n",
        "        The index of the subplot for the plot.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Names of group and bar width\n",
        "    barWidth = 1\n",
        "    bars = np.zeros(len(col_names))\n",
        "    n_col = np.arange(len(col_names))\n",
        "    plt.subplot(BAR_ROW_N, BAR_COL_N, subplot_idx)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        plt.bar(n_col, data[i], bottom=bars, edgecolor='white', width=barWidth, label=labels_name[i])\n",
        "        bars = np.add(bars, data[i]).tolist()\n",
        "\n",
        "    bars = np.add(bper, iper).tolist()\n",
        "    \n",
        "    # Custom X axis\n",
        "    plt.xticks(n_col, col_names, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ reading datasets ############\n",
        "\n",
        "ds = {#'ds_mr'           : pd.read_csv(nonBIO_PATH + 'moro_train.tsv', sep='\\t', header=None),\n",
        "      #'ds_mr_test'      : pd.read_csv(nonBIO_PATH + 'moro_test.tsv', sep='\\t', header=None),\n",
        "      \n",
        "      'ds_dg_IOB'       : pd.read_csv(BIO_PATH + 'degasperi_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_fc_IOB'       : pd.read_csv(BIO_PATH + 'fiction_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_wn_IOB'       : pd.read_csv(BIO_PATH + 'wikinews_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_mr_IOB'       : pd.read_csv(BIO_PATH + 'moro_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_dg_IOB_test'  : pd.read_csv(BIO_PATH + 'degasperi_test_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_fc_IOB_test'  : pd.read_csv(BIO_PATH + 'fiction_test_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_wn_IOB_test'  : pd.read_csv(BIO_PATH + 'wikinews_test_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_mr_IOB_test'       : pd.read_csv(BIO_PATH + 'moro_test_BIO.tsv', sep='\\t', header=None)\n",
        "}\n",
        "\n",
        "############ calculating stats ############\n",
        "'''\n",
        "    computing a dictionary containing statistics for each dataset.\n",
        "    The keys of the dictionary correspond to the dataset keys in the input dictionary.\n",
        "    Each value in the dictionary is a nested dictionary with the following statistics:\n",
        "    - 'doc_len': The total number of tokens in the dataset.\n",
        "    - 'voc_size': The number of unique tokens in the dataset.\n",
        "    - 'n_punct': The number of tokens that are punctuation.\n",
        "    - 'n_PER': The total number of 'PER' entities in the dataset.\n",
        "    - 'n_ORG': The total number of 'ORG' entities in the dataset.\n",
        "    - 'n_LOC': The total number of 'LOC' entities in the dataset.\n",
        "    If the dataset key contains 'IOB', additional statistics related to IOB tagging are included.\n",
        "\n",
        "'''\n",
        "\n",
        "stats = {}\n",
        "\n",
        "for i in ds.keys():     \n",
        "    ds[i] = add_column_names(ds[i])\n",
        "    ds[i] = to_lowerCase(ds[i])\n",
        "\n",
        "    stats[i] = {\n",
        "        'doc_len' : ds[i]['Token'].count(),\n",
        "        'voc_size' : ds[i]['Token'].nunique(),\n",
        "        'n_punct': sum(1 for k in ds[i]['Token'] if all(char in string.punctuation for char in k))\n",
        "    }\n",
        "\n",
        "    if('IOB' in str(i)):\n",
        "        \n",
        "        stats[i]['n_I-PER'] = sum(1 for k in ds[i]['Entity'] if k == 'I-PER') \n",
        "        stats[i]['n_I-ORG'] = sum(1 for k in ds[i]['Entity'] if k == 'I-ORG')\n",
        "        stats[i]['n_I-LOC'] = sum(1 for k in ds[i]['Entity'] if k == 'I-LOC')\n",
        "\n",
        "        stats[i]['n_B-PER'] = sum(1 for k in ds[i]['Entity'] if k == 'B-PER') \n",
        "        stats[i]['n_B-ORG'] = sum(1 for k in ds[i]['Entity'] if k == 'B-ORG')\n",
        "        stats[i]['n_B-LOC'] = sum(1 for k in ds[i]['Entity'] if k == 'B-LOC')\n",
        "\n",
        "        stats[i]['n_PER'] = stats[i]['n_I-PER'] + stats[i]['n_B-PER']\n",
        "        stats[i]['n_ORG'] = stats[i]['n_I-ORG'] + stats[i]['n_B-ORG']\n",
        "        stats[i]['n_LOC'] = stats[i]['n_I-LOC'] + stats[i]['n_B-LOC']\n",
        "        \n",
        "    else:\n",
        "        stats[i]['n_PER'] = sum(1 for k in ds[i]['Entity'] if k == 'PER') \n",
        "        stats[i]['n_ORG'] = sum(1 for k in ds[i]['Entity'] if k == 'ORG')\n",
        "        stats[i]['n_LOC'] = sum(1 for k in ds[i]['Entity'] if k == 'LOC')\n",
        "\n",
        "    stats[i]['n_O'] = sum(1 for k in ds[i]['Entity'] if k == 'O')\n",
        "        \n",
        "\n",
        "avg_doc_len = sum(stats[i]['doc_len'] for i in stats.keys()) / len(stats.keys())\n",
        "avg_voc_size = sum(stats[i]['voc_size'] for i in stats.keys()) / len(stats.keys())\n",
        "\n",
        "############ preparing data for plotting ############\n",
        "'''\n",
        "Extract and organize various statistics from the 'stats' dictionary for analysis.\n",
        "\n",
        "The code separates the statistics into different lists based on specific conditions.\n",
        "The code distinguishes between training and test datasets using the presence of the keyword 'test' in the dataset key.\n",
        "'''\n",
        "\n",
        "# Values of each group\n",
        "iper = [stats[i]['n_I-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iorg = [stats[i]['n_I-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iloc = [stats[i]['n_I-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "\n",
        "bper = [stats[i]['n_B-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "borg = [stats[i]['n_B-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "bloc = [stats[i]['n_B-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "\n",
        "iob_punct = [stats[i]['n_punct'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iob_o = [stats[i]['n_O'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iob_o = [iob_o[i] - iob_punct[i] for i in range(len(iob_o))]\n",
        "\n",
        "iper_test = [stats[i]['n_I-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iorg_test = [stats[i]['n_I-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iloc_test = [stats[i]['n_I-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "\n",
        "bper_test = [stats[i]['n_B-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "borg_test = [stats[i]['n_B-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "bloc_test = [stats[i]['n_B-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "\n",
        "iob_punct_test = [stats[i]['n_punct'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iob_o_test = [stats[i]['n_O'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iob_o_test = [iob_o_test[i] - iob_punct_test[i] for i in range(len(iob_o_test))]\n",
        "\n",
        "iob_voc_size = [stats[i]['voc_size'] for i in stats.keys() if 'IOB' in str(i)]\n",
        "\n",
        "per = [stats[i]['n_PER'] for i in stats.keys() if 'test' not in str(i)]\n",
        "org = [stats[i]['n_ORG'] for i in stats.keys() if 'test' not in str(i)]\n",
        "loc = [stats[i]['n_LOC'] for i in stats.keys() if 'test' not in str(i)]\n",
        "\n",
        "per_test = [stats[i]['n_PER'] for i in stats.keys() if 'test' in str(i)]\n",
        "org_test = [stats[i]['n_ORG'] for i in stats.keys() if 'test' in str(i)]\n",
        "loc_test = [stats[i]['n_LOC'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "voc_size = [stats[i]['voc_size'] for i in stats.keys() if 'test' not in str(i)]\n",
        "voc_size_test = [stats[i]['voc_size'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "doc_len = [stats[i]['doc_len'] for i in stats.keys() if 'test' not in str(i)]\n",
        "doc_len_test = [stats[i]['doc_len'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "n_punct = [stats[i]['n_punct'] for i in stats.keys() if 'test' not in str(i)]\n",
        "n_punct_test = [stats[i]['n_punct'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "o = [stats[i]['n_O'] for i in stats.keys() if 'test' not in str(i)]\n",
        "punct = [stats[i]['n_punct'] for i in stats.keys() if 'test' not in str(i)]\n",
        "\n",
        "\n",
        "o_test = [stats[i]['n_O'] for i in stats.keys() if 'test' in str(i)]\n",
        "punct_test = [stats[i]['n_punct'] for i in stats.keys() if 'test' in str(i)]"
      ],
      "metadata": {
        "id": "XIjpOhaVCS-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ spider plots ############\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': bper,\n",
        "    'I-PER': iper,\n",
        "    'B-ORG': borg,\n",
        "    'I-ORG': iorg,\n",
        "    'B-LOC': bloc,\n",
        "    'I-LOC': iloc }),\n",
        "    ['deGasperi', 'Fiction', 'Wikinews', 'Moro'],\n",
        "    'Train set IOB tags',\n",
        "    1)\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': bper_test,\n",
        "    'I-PER': iper_test,\n",
        "    'B-ORG': borg_test,\n",
        "    'I-ORG': iorg_test,\n",
        "    'B-LOC': bloc_test,\n",
        "    'I-LOC': iloc_test }),\n",
        "    ['deGasperi', 'Fiction', 'Wikinews', 'Moro'],\n",
        "    'Test set IOB tags',\n",
        "    2)\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': per,\n",
        "    'B-ORG': org,\n",
        "    'B-LOC': loc}),\n",
        "    ['deGasperi', 'Fiction', 'Wikinews', 'Moro'],\n",
        "    'Train set non-IOB tags',\n",
        "    3)\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': per_test,\n",
        "    'B-ORG': org_test,\n",
        "    'B-LOC': loc_test}),\n",
        "    ['deGasperi', 'Fiction', 'Wikinews', 'Moro'],\n",
        "    'Test set non-IOB tags',\n",
        "    4)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c50C-g1WUsxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dims_train = [117997, 170942, 247528, 309798]\n",
        "\n",
        "dims_test = [32635, 21506, 61094, 82806]\n",
        "stacked_bar_plot(['deGasperi', 'Fiction', 'Wikinews', 'Moro'],\n",
        "                 [[punct[i]/dims_train[i] for i in range(len(o))], [(o[i]-punct[i])/dims_train[i] for i in range(len(o))]],\n",
        "                 ['punct', 'O'],\n",
        "                 'Train sets',\n",
        "                 1)\n",
        "\n",
        "stacked_bar_plot(['deGasperi', 'Fiction', 'Wikinews', 'Moro'],\n",
        "                 [[punct_test[i]/dims_test[i] for i in range(len(o))], [(o_test[i]-punct_test[i])/dims_test[i] for i in range(len(o_test))]],\n",
        "                 ['punct', 'O'],\n",
        "                 'Test sets',\n",
        "                 2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sYcr8aJkUlp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations\n",
        "We can observe that all the datasets are heavily biased towards the 'O' class (as was easily predictable). The most balanced dataset in terms of entity classes is Wikinews, while the other three are strongly biased towards one class. The test sets provide a good representation of the train sets, with the exception of the 'moro' dataset, where the 'organization' class is poorly represented compared to the train set. However, overall, the test set is balanced across the classes."
      ],
      "metadata": {
        "id": "M268df5NS_sH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PMsuTVlfsNw"
      },
      "source": [
        "## Word embeddings representation\n",
        "\n",
        "A word embedding is the vectorial representation of a word that contains its sintactic and semantic information. It is used for achieving a numerical and dense representation of the words in an high dimensional space. Other types of representation of the words, such as the bag-of-words one, are sparse compared to word embeddings.\n",
        "The word embeddings allow to achieve better results in many fields of natural language processing.\n",
        "\n",
        "In this section, we train a Word2Vec model in order to obtain the word embeddings of the words contained in the dataset and we also load a model from FastText and find the representation of the dataset in terms of its vectors.\n",
        "\n",
        "The basic version of the word embeddings model is a artificial neural network composed by two layers.\n",
        "\n",
        "The inputs of the neural network are the words of the sentence to convert in the word embeddings representation.\n",
        "The first layer of the neural network is a fully connected linear layer with a number of neurons equals to the dimension of the embeddings.\n",
        "This layer is used to feed the last layer of the neural network that has the same number of outputs as inputs. On the output layer is applied a softmax function.\n",
        "The neural network is then trained in order to make it predict, for each word in input, the next word in a sentence.\n",
        "This is done, in the basic implementation by setting the input corresponding to the considered word to 1 and the other inputs to 0 and expecting as result the probability 1 on the expected word (the output of the softmax layer can be interpreted as a probability).\n",
        "The training can be performed using the cross entropy as loss function. At the end of the training the weights connecting each input to the first hidden layer are the values of the dimensions of the word embedding corresponding to the input word.\n",
        "\n",
        "Two of the most used architecture of the Word2Vec embedding model are CBOW and Skip-Gram.\n",
        "\n",
        "The Continuous Bag of Words (CBOW) method uses many words surrounding the considered word for predicting it. In the training step, the model sets to 1 the inputs corresponding to the words in the context of a word, the neural network has to predict the word inside the context. In this way the embedding is used to embed the context of a word for representing it.\n",
        "The Skip-gram uses a word to predict the words in the surroundings. In the training step, the input of the considered word will be set to 1 and the outputs the neural network has to learn are the probabilities of the words in its context, so the probabilities of the words in the surroundings.\n",
        "\n",
        "There are then optimizations of the training such as the negative sampling method or the computation of only some selected and meaningful outputs for each word in the train set.\n",
        "\n",
        "In some way the distribution in the various dimension is based on the similarity of the words in terms of semantics and usage. With word embeddings we can embed the context of the word inside its representation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijraczejAUOv"
      },
      "source": [
        "### Word embeddings trained on all the data\n",
        "\n",
        "The choosen pipeline for the creation of the embeddings starts from the dataframe containing the tokens of the datasets.\n",
        "\n",
        "The preprocessing applied to the datasets in order to define the input of the Word2vec model is the following:\n",
        "- tokenization of the dataset (previously done by the providers of the datasets);\n",
        "- transformation of the input DataFrame into a list of lists of tokens, this is done merging the elements of the datasets into sentences. Since the datasets did not contain specific separator for the sentences, it is assumed that a sentence ends when in the DataFrane there is a dot. Therefor, the split in sentences of the tokens is done basing on the single dots.\n",
        "- lowercasing all the tokens\n",
        "\n",
        "We choose to try the definition of the word embeddings model on different portions of the dataset in order to compare the results. We expect that the more data are considered in the model definition, the more good will be the model in terms of information catched inside it.\n",
        "\n",
        "Firstly we try to define the word embeddings on the entire data we have since, as it is already been said, the larger is the dataset the better will be the our model and the wider will be the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UOkb_6GAUOv"
      },
      "outputs": [],
      "source": [
        "# Merging the dictionaries of the trainsets and testsets\n",
        "datasets_for_embeddings = datasets_train_dict_IO.copy()\n",
        "datasets_for_embeddings.update(datasets_test_dict_IO)\n",
        "# Merging the lists of the names of the trainsets and testsets\n",
        "dataset_names_for_embeddings = dataset_names_train_IO + dataset_names_test_IO\n",
        "\n",
        "# Getting the sentences, the labels and the origin dataset of the sentences in\n",
        "# the correct format in order to create the embeddings representation of the words\n",
        "sentences, labels, sentences_keys = get_all_sentences_from_datasets(datasets_for_embeddings)\n",
        "\n",
        "# Printing 10 lists of token\n",
        "print('\\n\\nHere some examples of the shape of the sentences:\\n')\n",
        "for i in range(10):\n",
        "  print(str(sentences[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ks3ebJKAUOv"
      },
      "outputs": [],
      "source": [
        "# To check that there are no tabs or new lines inside the tokens\n",
        "# It was used because there were issues with the reading of the datasets\n",
        "found = False\n",
        "for name in dataset_names_for_embeddings:\n",
        "  for i, el in datasets_for_embeddings[name].iterrows():\n",
        "    if '\\t' in el['Token'] or '\\n' in el['Token']:\n",
        "      found = True\n",
        "      print(el['Token'])\n",
        "if found:\n",
        "  print('There is something wrong, there tab or new line characters, are check the import of the dataset :(')\n",
        "else:\n",
        "  print('No tab or new line characters found, Great job!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwWF6C3sAUOv"
      },
      "outputs": [],
      "source": [
        "# Printing some information about the list of lists of token\n",
        "print('The total number of sentences in the dataset is ' + str(len(sentences)))\n",
        "length_list= []\n",
        "for idx,i in enumerate(sentences):\n",
        "  length_list.append(len(i))\n",
        "print('The maximum lenght of a sentence is ' + str(max(length_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hli5YYiaAUOv"
      },
      "outputs": [],
      "source": [
        "# Definition of the Word2Vec model\n",
        "embeddings_model = Word2Vec(sentences, vector_size=50, min_count=5, window=10)\n",
        "# Printing the length of the vocabulary\n",
        "print('The length of the vocabulary is ' + str(len(embeddings_model.wv)) + ' words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT1DIssKAUOv"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "term = 'governo'\n",
        "embeddings_model.wv.most_similar(term.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPLJx-kdMeEj"
      },
      "outputs": [],
      "source": [
        "# Definition of a function to get the word2vec vector representation of a word\n",
        "def get_word2vec_vector(word, model):\n",
        "  '''\n",
        "  Returns the word2vec vector representation of a word.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  word: str\n",
        "    word to convert into its vectorial representation.\n",
        "  model: Word2Vec\n",
        "    word2Vec model from gensim library for creating the embedding.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  word_vector: ndarray\n",
        "    word2Vec vector representation of a word.\n",
        "\n",
        "  '''\n",
        "  \n",
        "  word_vector = np.zeros(model.vector_size)\n",
        "  try:\n",
        "    word_vector = model.wv[word]\n",
        "  except KeyError:\n",
        "    try:\n",
        "      word_vector = model.wv[model.wv.most_similar(word)[0][0]]\n",
        "    except:\n",
        "      return word_vector\n",
        "    return word_vector\n",
        "  return word_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3RkL3FhOHU3"
      },
      "source": [
        "Here we can try to find the most similar words to the given one. Using different values we can understand that the model is quite good and retrives words very related to the selected one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiLRR5MzAUOw"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of embedding vectors\n",
        "word_samples = random.sample(list(embeddings_model.wv.key_to_index), 500)\n",
        "word_vectors = embeddings_model.wv[word_samples]\n",
        "\n",
        "# Computing the dimensionality reduction of the word embeddings space\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embeddings = tsne.fit_transform(word_vectors)\n",
        "x, y, z = np.transpose(tsne_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHcBd5-zAUOw"
      },
      "outputs": [],
      "source": [
        "# Plotting the word embeddings of the model\n",
        "fig = px.scatter_3d(x=x, y=y, z=z, text=word_samples)\n",
        "fig.update_traces(marker=dict(size=3, line=dict(width=2)), textfont_size=6)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8At4tFwAUOw"
      },
      "source": [
        "The second analysis is done on the word embeddings trained only on the training datasets that could be useful in the next part of the project since they can be used for performing named entity recognition. \n",
        "\n",
        "This is indeed a more realistic scenario, since train set and test set must be kept separate as much as possible in order to have an unbiased assessment of the models when evaluating it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wArKi3xiAUOw"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences, the labels and the origin dataset of the sentences in\n",
        "# the correct format in order to create the embeddings representation of the words\n",
        "sentences_train, labels_train, sentences_keys_train = get_all_sentences_from_datasets(datasets_train_dict_IO, lower=False)\n",
        "\n",
        "# Definition of the Word2Vec model\n",
        "embeddings_model_train = Word2Vec(sentences_train, vector_size=50, min_count=5, window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9PDHWg7AUOw"
      },
      "outputs": [],
      "source": [
        "# Printing the length of the vocabulary\n",
        "print('The length of the vocabulary is ' + str(len(embeddings_model_train.wv)) + ' words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaUHxOhOAUOw"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "term = 'governo'\n",
        "embeddings_model_train.wv.most_similar(term.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWYjNVIYAUOw"
      },
      "outputs": [],
      "source": [
        "# Choosing a subset of embedding vectors\n",
        "word_samples_train = random.sample(list(embeddings_model_train.wv.key_to_index), 500)\n",
        "word_vectors_train = embeddings_model_train.wv[word_samples_train]\n",
        "\n",
        "# Computing the dimensionality reduction of the word embeddings space\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embeddings = tsne.fit_transform(word_vectors_train)\n",
        "x_train, y_train, z_train = np.transpose(tsne_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcQAoHSnAUOx"
      },
      "outputs": [],
      "source": [
        "# Plotting the word embeddings of the model\n",
        "fig = px.scatter_3d(x=x_train, y=y_train, z=z_train, text=word_samples_train)\n",
        "fig.update_traces(marker=dict(size=3, line=dict(width=2)), textfont_size=6)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcJ9uK3yAUOx"
      },
      "source": [
        "The dictionary is a bit smaller but still big and the results are quite similar to the previous ones, in terms of understanding and search of the similar words and also the embedding space has a similar shape. \n",
        "\n",
        "Since train set and test set are usually separate and so I could create the embeddings model for the train set first and then model inside it the test set.\n",
        "\n",
        "However it has to be noted that the vocabulary is a bit smaller with respect to the one of the model trained over all the data. This is believable since the test set can have token that the train set doesn't present. This aspect could be a problem when the models fed by the embeddings have to be applied on the representation of the test set. Some words will not be converted in embeddings and the that words will could not be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJYKN3AkQCG2"
      },
      "source": [
        "The last embedding models trained are the ones computed on the train datasets separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q7Qqy1wAUOx"
      },
      "outputs": [],
      "source": [
        "# Separating the train sets and collecting their sentences, labels and origin\n",
        "# dataset into a dictionary\n",
        "sentences_train_separate = {}\n",
        "labels_train_separate = {}\n",
        "embeddings_model_train_separate = {}\n",
        "keys_train_separate = {}\n",
        "for name in dataset_names_train_IO:\n",
        "  # Getting the sentences, the labels and the origin dataset of the sentences in\n",
        "  # the correct format in order to create the embeddings representation of the words\n",
        "  sentences_train_sep, labels_train_sep, keys_train_sep = get_all_sentences_from_datasets({name:datasets_train_dict_IO[name]})\n",
        "  sentences_train_separate[name] = sentences_train_sep\n",
        "  labels_train_separate[name] = labels_train_sep\n",
        "  keys_train_separate[name]  = keys_train_sep \n",
        "\n",
        "  # Definition of the Word2Vec models\n",
        "  embeddings_model_train_separate[name] = Word2Vec(sentences_train_sep, vector_size=50, min_count=3, window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyBDbF5LAUOx"
      },
      "outputs": [],
      "source": [
        "# Printing the length of the vocabularies of the various datasets\n",
        "for name in dataset_names_train_IO:\n",
        "    print('The dictionary of the dataset ' + name + ' is long ' + str(len(embeddings_model_train_separate[name].wv)) + ' words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "monjRwjuAUOx"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "for name in dataset_names_train_IO:\n",
        "    term = 'governo'\n",
        "    print(name + ':' + str(embeddings_model_train_separate[name].wv.most_similar(term.lower())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9W3_o-AUOx"
      },
      "source": [
        "It is easy to understand that the dictionaries in this case are smaller since the overall words are splitted and analysed in many models.\n",
        "\n",
        "Trying to search the most similar embeddings to a given word in many different fields and for the different datasets, it can be seen that better performace are achieved by the datasets that are specialized in that field, e.g. fiction_train finds worst results (less related word embeddings) for the word 'governo' than the other datasets, which deal with news and politics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKHUhOncAUOx"
      },
      "outputs": [],
      "source": [
        "points_train_separate = {}\n",
        "word_samples_train_separate = {}\n",
        "for key in embeddings_model_train_separate:\n",
        "  # Choosing a subset of embedding vectors\n",
        "  word_samples_train_separate[key] = random.sample(list(embeddings_model_train_separate[key].wv.key_to_index), 500)\n",
        "  model_separate = embeddings_model_train_separate[key]\n",
        "  word_vector_train_separate = model_separate.wv[word_samples_train_separate[key]]\n",
        "\n",
        "  # Computing the dimensionality reduction of the word embeddings space\n",
        "  tsne = TSNE(n_components=3, n_iter=1000)\n",
        "  tsne_embeddings = tsne.fit_transform(word_vector_train_separate)\n",
        "  points_train_separate[key] = np.transpose(tsne_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpI-ond6AUOx"
      },
      "outputs": [],
      "source": [
        "# Plotting the word embeddings of the model\n",
        "for key in points_train_separate:\n",
        "  fig = px.scatter_3d(x=points_train_separate[key][0], y=points_train_separate[key][1],\n",
        "                      z=points_train_separate[key][2], text=word_samples_train_separate[key])\n",
        "  fig.update_traces(marker=dict(size=3, line=dict(width=2)), textfont_size=6)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6FpGcvnAUOy"
      },
      "source": [
        "### Computing the embeddings using a pretrained model FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPReFIjnAUOy"
      },
      "outputs": [],
      "source": [
        "# Downloading the pretrained model from the fasttext library\n",
        "!wget http://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.bin.gz\n",
        "!gzip -d cc.it.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyIeWHLxAUOy"
      },
      "outputs": [],
      "source": [
        "# Instantiating the model\n",
        "ft_embedding_model = fasttext.load_model('cc.it.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_C3wIdLAUOy"
      },
      "outputs": [],
      "source": [
        "# Printing some statistics about the model\n",
        "print('The size of the vocabulary is ' + str(len(ft_embedding_model.get_words())))\n",
        "print('The length of the embeddings is '+ str(ft_embedding_model.get_dimension()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v46gvlZGrrPL"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "term = 'legge'\n",
        "ft_embedding_model.get_nearest_neighbors(term.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vwzunT8sE0v"
      },
      "outputs": [],
      "source": [
        "def get_fasttext_vector(word, model=None):\n",
        "  '''\n",
        "  Returns the fastText vector representation of a word.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  word: str\n",
        "    word to convert into its vectorial representation.\n",
        "  model: fasttext.FastText._FastText\n",
        "    fastText model for creating the embedding. Providing it the computation of \n",
        "    the embedding is faster.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  word_vector: ndarray\n",
        "    fastText vector representation of a word.\n",
        "\n",
        "  '''\n",
        "  \n",
        "  if model == None:\n",
        "    print('ciao')\n",
        "    model = fasttext.load_model('cc.it.300.bin')\n",
        "  word_vector = model.get_word_vector(model.get_nearest_neighbors(word.lower())[0][1])\n",
        "  return word_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sbY2_S1tirf"
      },
      "outputs": [],
      "source": [
        "# Finding the embedding representation of a word, assuming to represent a word\n",
        "# with its similar word in the model\n",
        "term = 'legge'\n",
        "get_fasttext_vector(term, ft_embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLuEwO-5dkbT"
      },
      "source": [
        "The following snippets of code try to transform the whole dataset in its fastText embedding representation. The time required to run this code is very long and so this part has been commented.\n",
        "\n",
        "The idea was to use these vectors instead of the Word2Vec ones in the NER performed by means of CRF since the fastText model is trained on more data than the one trained on the KIND dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDVUY6y6o88J"
      },
      "outputs": [],
      "source": [
        "# Creating the embedding representation of the dataset\n",
        "#ft_embedding_train = []\n",
        "#for sentence in sentences_train:\n",
        "#  sentence_embeddings = []\n",
        "#  for word in sentence:\n",
        "#    sentence_embeddings.append(get_fasttext_vector(word, ft_embedding_model))\n",
        "#    print(word)\n",
        "#  ft_embedding_train.append(sentence_embeddings)\n",
        "# This operation takes a very long time to be computed but when it is done the \n",
        "# embeddings can be saved and loaded whenever they are needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgn1LECxAUOy"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "The clustering is an unsupervided machine learning technique useful for inspecting non-labeled data in order to find inside them hidden patterns or other information.\n",
        "In our case clustering is not needed for performing named entity recognition but it could useful for gain some additional knowledge about the dataset before proceeding in the sequence labeling task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbrC9zJ_AUOy"
      },
      "source": [
        "Given the KIND dataset it is not possible to identify the different documents because no document separators are present in the data. Since it is not possible to identify the various documents, we decide to apply the clustering techniqes on the sentences. As we did in the embeddings section, we assume that the sentences were separate by the dot and we extract in this way them from the input files.\n",
        "\n",
        "In this section we try to investigate whether the four dataset are distinguishable basing on the representation of their sentences and so in some way they have different characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuFzAvx2QZIM"
      },
      "source": [
        "### Importing the italian stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUMklbsdAUOy"
      },
      "outputs": [],
      "source": [
        "# Importing the italian stopwords (taken from https://github.com/stopwords-iso/stopwords-it.git)\n",
        "with open('/content/KIND_project/notebook/stopwords-it.txt', 'r') as f:\n",
        "    italian_stopwords = f.read()\n",
        "italian_stopwords_github = italian_stopwords.split('\\n')\n",
        "print('Number of stopwords in the repository https://github.com/stopwords-iso/stopwords-it.git: ' + str(len(italian_stopwords_github)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoyJuKM9AUOz"
      },
      "outputs": [],
      "source": [
        "# Downloading and importing the italian stopwords in the package nltk\n",
        "nltk.download('stopwords')\n",
        "italian_stopwords_nltk = nltk.corpus.stopwords.words('italian')\n",
        "print('Number of stopwords in the nltk library: ' + str(len(italian_stopwords_nltk)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlcGWK72AUOz"
      },
      "outputs": [],
      "source": [
        "# Checking that all the stopwords of nltk are in the other dataset, better result with more stopwords\n",
        "count = 0\n",
        "for i in italian_stopwords_nltk:\n",
        "    if i in italian_stopwords_github:\n",
        "        count += 1\n",
        "print(str(count) + ' stopwords of the nltk library are in the Github repository')\n",
        "italian_stopwords = italian_stopwords_github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMao28g8nOpJ"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences, the labels and the origin dataset of the sentences in\n",
        "# the correct format in order to create the embeddings representation of the words\n",
        "sentences, labels, sentences_keys = get_all_sentences_from_datasets(datasets_train_dict_BIO, lower=False)\n",
        "\n",
        "# Defining the labels of the clustering\n",
        "for i, key in enumerate(sentences_keys):\n",
        "  sentences_keys[i] = key.replace('_train_BIO.tsv','').replace('_test_BIO.tsv', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W1cjvc7k2c_"
      },
      "source": [
        "### Clustering TF-IDF vectors\n",
        "\n",
        "The Term Frequency-Inverse Document Frequency is a numerical statistic used to evaluate the importance of a term within a document or a corpus. In our case, we evaluate the importance of the words in the sentences.\n",
        "The first step is to obtain a TF-IDF representation of the dataset so from  the collection of text we achieve a matrix representation containing the TF-IDF values of each word in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWG5z2noAUOz"
      },
      "outputs": [],
      "source": [
        "# Defining the vectorizer model\n",
        "vectorizer = TfidfVectorizer(max_df=0.2, min_df=0, stop_words=italian_stopwords_github, use_idf=True)\n",
        "# Fitting the vectorizer model\n",
        "vectorizer.fit(concatenate_sentences_tokens(sentences))\n",
        "\n",
        "# Given that we are evaluating sentences the frequency can be lower for saying that we have a stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bh3XX3aAUOz"
      },
      "outputs": [],
      "source": [
        "# Extracting the vocabulary\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "print('The vocabulary is long ' + str(len(vocabulary)) + ' words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdsVliRNAUOz"
      },
      "outputs": [],
      "source": [
        "# The words look reasonable and the vocabulary seems to not have many stopwords inside\n",
        "sorted(random.sample(vocabulary.tolist(), 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52kRljRQAUOz"
      },
      "outputs": [],
      "source": [
        "# Converting the strings into vectors\n",
        "sentences_vector = vectorizer.transform(concatenate_sentences_tokens(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "023nFyxlAUOz"
      },
      "outputs": [],
      "source": [
        "sentences_vector[0].multiply(sentences_vector[0]).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_NeyGPrAUOz"
      },
      "outputs": [],
      "source": [
        "max_score = 0\n",
        "sentence_idx = 45\n",
        "for i in range(sentences_vector.shape[0]):\n",
        "    if i != sentence_idx:\n",
        "        score = sentences_vector[sentence_idx].multiply(sentences_vector[i]).sum()\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            max_score_idx = i\n",
        "print('Sentence index ' + str(sentence_idx) + ': ' + str(sentences[sentence_idx]))\n",
        "print('Most similar sentence is the one with index ' + str(max_score_idx) + ': ' + str(sentences[max_score_idx]))\n",
        "print('The score is ' + str(max_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JykECfayAUO0"
      },
      "source": [
        "Trying to find the most similar sentences to a given sentence, it is possible to see that the metrics of similarity are quite low, very often in the interval (0.20, 0.40). Moreover the retrieved sentences have some sort of similarities, they have some words in common, but they are not very similar in the semantics or in the field of application of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNXFHEy7AUO0"
      },
      "source": [
        "The second step is to actual cluster the TF-IDF representation of the sentences.\n",
        "In general it is not known the number of clusters that are present. It is possible to approximately define a range or deduce the number from the problem.\n",
        "In the considered case, the clustering is applied for checking whether the 4 inputs datasets are distinguishable in some way, so initially 4 cluster are considered.\n",
        "\n",
        "The K-means clustering method is the one used in the following part section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnzfig-tAUO0"
      },
      "outputs": [],
      "source": [
        "num_clusters= len(set(sentences_keys))\n",
        "# Defining the K-means model\n",
        "kmeans_model = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=3, verbose=True,\n",
        "                      random_state=2307)\n",
        "# Applying K-means clustering to the tf-idf representation of the sentences\n",
        "kmeans_model.fit(sentences_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRfuh3a3AUO0"
      },
      "outputs": [],
      "source": [
        "# Displaying the centroids in terms of their words with highest score\n",
        "for i in range(num_clusters):\n",
        "  # Getting the centroid for the cluster of index i\n",
        "  centroid = kmeans_model.cluster_centers_[i]\n",
        "  sorted_terms = centroid.argsort()[::-1]\n",
        "  \n",
        "  print('Centroid of cluster ' + str(i)+':')\n",
        "  print([vocabulary[j] for j in sorted_terms[:20]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7JMAL4so9dc"
      },
      "source": [
        "Even if some difference in the words of the centroids of the clusters can be seen, it is evident that the centroids are not very different one from the others meaning that the clustering of the sentences in 4 clusters does not obtain very good results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzGqRTzOhrh0"
      },
      "outputs": [],
      "source": [
        "# Printing the number of sentences contained in the various datasets\n",
        "print('Number of sentences in the various datasets: ')\n",
        "counter = Counter(sentences_keys)\n",
        "for dataset_name, count in counter.items():\n",
        "  print('Dataset ' + dataset_name + ': ' + str(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFnTQM3vAUO0"
      },
      "outputs": [],
      "source": [
        "# Printing the number of sentences contained in the various clusters\n",
        "print('Number of sentences in: ')\n",
        "for i in range(kmeans_model.n_clusters):\n",
        "    print(f\"Cluster {i}: {np.sum(kmeans_model.labels_ == i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcGDnhv4AUO0"
      },
      "outputs": [],
      "source": [
        "# Evaluating the clustering performance using intrinsic evaluation measures.\n",
        "print(\"Intrinsic evaluation measures:\")\n",
        "print(\"Within-cluster sum-of-squares:\", str(kmeans_model.inertia_))\n",
        "print(\"Silhouette coefficient:\", str(metrics.silhouette_score(sentences_vector, kmeans_model.labels_)))\n",
        "# These metrics assess how much the clusters are internally coherent and how much \n",
        "# they are different one with respect to the others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvvsSvHxAUO1"
      },
      "outputs": [],
      "source": [
        "# Evaluating the clustering performance using extrinsic evaluation measures\n",
        "print('Extrinsic evaluation measures:')\n",
        "print(\"Homogeneity:\", str(metrics.homogeneity_score(sentences_keys, kmeans_model.labels_)))\n",
        "print(\"Completeness:\", str(metrics.completeness_score(sentences_keys, kmeans_model.labels_)))\n",
        "print(\"V-measure:\", str(metrics.v_measure_score(sentences_keys, kmeans_model.labels_)))\n",
        "print(\"Adjusted Rand-Index:\", str(metrics.adjusted_rand_score(sentences_keys, kmeans_model.labels_)))\n",
        "# These metrics assess how good the clusters have classified the elements of the\n",
        "# dataset with respect to the known labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyLwFwl7Zbky"
      },
      "outputs": [],
      "source": [
        "# Applying a SVD dimensionality reduction to the embedding representation of the\n",
        "# sentences, here it is a linear dimensionality reduction operator for computational\n",
        "# performance reason\n",
        "svd = TruncatedSVD(3)\n",
        "reduced_data = svd.fit_transform(sentences_vector)\n",
        "\n",
        "[x,y,z] = np.transpose(reduced_data)\n",
        "[x,y,z]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKY7BCmGZbpC"
      },
      "outputs": [],
      "source": [
        "# Plotting the 3D-dimensionality reduction of the the embeddings coloring in \n",
        "# different ways data coming from different datasets\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=LabelEncoder().fit_transform(sentences_keys), marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nrdrQPfZbsf"
      },
      "outputs": [],
      "source": [
        "# Plotting the 3D-dimensionality reduction of the the embeddings coloring in \n",
        "# different ways the elements of the different clusters\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, z, c=kmeans_model.labels_, marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFJUJwK91NoL"
      },
      "outputs": [],
      "source": [
        "# Plotting, in interactive mode, the 3D-dimensionality reduction of the the \n",
        "# embeddings coloring in different ways data coming from different datasets\n",
        "plot_3D(x, y, z, sentences_keys, mapping=get_numerical_mapping(), title='3D-dimensionality reduction of the dataset plotted by dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3MeZlRi4-ok"
      },
      "outputs": [],
      "source": [
        "# Plotting, in interactive mode, the 3D-dimensionality reduction of the the \n",
        "# embeddings coloring in different ways the elements of the different clusters\n",
        "plot_3D(x, y, z, kmeans_model.labels_, title='3D-dimensionality reduction of the dataset plotted considering the clusters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJAOdYITAUO0"
      },
      "source": [
        "Using TF-IDF vectors it is not possible to distinguish the datasets, the clusters found are not the ones of the "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmVvg6gDgoI"
      },
      "source": [
        "### Clustering embedding vectors\n",
        "\n",
        "In the following section it is provided the application of the clustering to the embeddings representation af the dataset in order to see whether the results are better in this case or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnGQe22B4aAi"
      },
      "outputs": [],
      "source": [
        "# Removing the italian stopwords from the dataset\n",
        "#sentences, labels = remove_stopwords(sentences, labels, italian_stopwords)\n",
        "# Not needed, the embeddings wants the context, transformers works well with them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzyEJTp4gtRn"
      },
      "outputs": [],
      "source": [
        "# Defining the pretrained BERT model used to create the embeddings starting from\n",
        "# the sentences\n",
        "sbert_model = SentenceTransformer('dbmdz/bert-base-italian-xxl-cased')\n",
        "# Joining the tokens of the sentences in order to obtain a list of sentences\n",
        "sentences_list = [' '.join(sentence) for sentence in sentences]\n",
        "# Create the embeddings starting from the sentences\n",
        "sentence_embeddings_bert = sbert_model.encode(sentences_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bot5arzdPwjR"
      },
      "outputs": [],
      "source": [
        "# Defining the K-means model\n",
        "kmeans_model_embeddings = KMeans(n_clusters=4, max_iter=2000, n_init=10, verbose=True, random_state=2307)\n",
        "# Applying K-means clustering to the tf-idf representation of the sentences\n",
        "kmeans_model_embeddings.fit(sentence_embeddings_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO676QtJLJgu"
      },
      "outputs": [],
      "source": [
        "# Printing the number of sentences contained in the various datasets\n",
        "print('Number of sentences in the various datasets: ')\n",
        "counter = Counter(sentences_keys)\n",
        "for dataset_name, count in counter.items():\n",
        "  print('Dataset ' + dataset_name + ': ' + str(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhK-QWbjl6Re"
      },
      "outputs": [],
      "source": [
        "# Printing the number of sentences contained in the various clusters\n",
        "print('Number of sentences in: ')\n",
        "for i in range(kmeans_model_embeddings.n_clusters):\n",
        "    print(f\"Cluster {i}: {np.sum(kmeans_model_embeddings.labels_ == i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GZzJRtZl7Ek"
      },
      "outputs": [],
      "source": [
        "# Evaluating the clustering performance using intrinsic evaluation measures\n",
        "print(\"Intrinsic evaluation measures:\")\n",
        "print(\"Within-cluster sum-of-squares:\", str(kmeans_model_embeddings.inertia_))\n",
        "print(\"Silhouette coefficient:\", str(metrics.silhouette_score(sentence_embeddings_bert, kmeans_model_embeddings.labels_)))\n",
        "# These metrics assess how much the clusters are internally coherent and how much \n",
        "# they are different one with respect to the others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_86FQjel7HM"
      },
      "outputs": [],
      "source": [
        "# Evaluating the clustering performance using extrinsic evaluation measures\n",
        "print('Extrinsic evaluation measures:')\n",
        "print(\"Homogeneity:\", str(metrics.homogeneity_score(sentences_keys, kmeans_model_embeddings.labels_)))\n",
        "print(\"Completeness:\", str(metrics.completeness_score(sentences_keys, kmeans_model_embeddings.labels_)))\n",
        "print(\"V-measure:\", str(metrics.v_measure_score(sentences_keys, kmeans_model_embeddings.labels_)))\n",
        "print(\"Adjusted Rand-Index:\", str(metrics.adjusted_rand_score(sentences_keys, kmeans_model_embeddings.labels_)))\n",
        "# These metrics assess how good the clusters have classified the elements of the\n",
        "# dataset with respect to the known labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAw72zMROJSj"
      },
      "outputs": [],
      "source": [
        "# Applying a SVD dimensionality reduction to the embedding representation of the\n",
        "# sentences, here it is a linear dimensionality reduction operator for computational\n",
        "# performance reason\n",
        "svd = TruncatedSVD(3)\n",
        "reduced_data = svd.fit_transform(sentence_embeddings_bert)\n",
        "\n",
        "[x, y, z] = np.transpose(reduced_data)\n",
        "[x, y, z]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ajktJMZmOjV"
      },
      "outputs": [],
      "source": [
        "# Plotting the 3D-dimensionality reduction of the the embeddings coloring in \n",
        "# different ways data coming from different datasets\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, c=LabelEncoder().fit_transform(sentences_keys), marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJnDygCTmOmM"
      },
      "outputs": [],
      "source": [
        "# Plotting the 3D-dimensionality reduction of the the embeddings coloring in \n",
        "# different ways the elements of the different clusters\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x, y, c=kmeans_model_embeddings.labels_, marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7VHhZJ054mP"
      },
      "outputs": [],
      "source": [
        "# Plotting, in interactive mode, the 3D-dimensionality reduction of the the \n",
        "# embeddings coloring in different ways data coming from different datasets\n",
        "plot_3D(x, y, z, sentences_keys, mapping=get_numerical_mapping(), title='3D-dimensionality reduction of the dataset plotted by dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frKsqMiw1mRf"
      },
      "outputs": [],
      "source": [
        "# Plotting, in interactive mode, the 3D-dimensionality reduction of the the \n",
        "# embeddings coloring in different ways the elements of the different clusters\n",
        "plot_3D(x, y, z, kmeans_model_embeddings.labels_, title='3D-dimensionality reduction of the dataset plotted considering the clusters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGdyk9ksE-8E"
      },
      "source": [
        "To summarize, it is possible to see that the dataset is not meant for appying clustering and we cannot state that the sentences in the various datasets are very distinguishable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrD3aLyxE--m"
      },
      "source": [
        "Here it is done a brief analysis about the number of clusters that could be find in the problem...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhoDTYq5y1rw"
      },
      "outputs": [],
      "source": [
        "# Applying multiple times the K-means with mini-batch using a different number\n",
        "# of clusters in different iterations\n",
        "performance = [MiniBatchKMeans(n_clusters=k, batch_size=500, n_init=2, random_state=2307).fit(sentence_embeddings_bert).inertia_ for k in range(1,50)]\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6ZV9RrTPwof"
      },
      "outputs": [],
      "source": [
        "# Plotting the performances for various values of the number of clusters\n",
        "plt.figure()\n",
        "plt.plot(performance[:15])\n",
        "plt.ylabel('Within-cluster sum-of-squares')\n",
        "plt.xlabel('k')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isCaG8ivfgwV"
      },
      "source": [
        "It can be seen that the elbow is approximatively in the interval [4,7]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfbNEkJZgDJe"
      },
      "source": [
        "## POS tagging\n",
        "\n",
        "**Part-of-speech** tagging is the process of marking up a word in a text with the corresponding part of speech, based on both its definition and its context.\n",
        "The POS Tagging here plays a crucial role to understand in what context the word is used in the sentence.\n",
        "\n",
        "We will perform POS tagging using the **spaCy** libary.\n",
        "After the tokenization, spaCy can parse and tag a given Doc. This is made thanks to a trained pipeline and its statistical models, which enable spaCy to make predictions of which tag o label most likely applies in this context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ls1yQMgQ_GL"
      },
      "source": [
        "### Creation of the POS tags files\n",
        "\n",
        "In the following part of code we created some files containing the POS tags for each dataset and also che combination of them. This to improve the performances for the next steps and to avoid every time to do everything from the start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp81_qUJRElm"
      },
      "outputs": [],
      "source": [
        "def write_pos_tags_on_file(filename, dataset):\n",
        "\n",
        "  '''\n",
        "  Takes as input a dataset and writes into a file the pos tags for each sentence\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename: Str\n",
        "    File name that has to be used to save the pos tags\n",
        "\n",
        "  dataset: DataFrame\n",
        "    Dataset that contains the tokens of the text.\n",
        "\n",
        "  '''\n",
        "\n",
        "  sentences, labels, _ = get_sentences_list_from_df(dataset, lower=False)\n",
        "  pos_tags = get_pos_tags(sentences)\n",
        "  with open(f'{filename}.txt', 'w+') as f:\n",
        "    for line in pos_tags:\n",
        "      f.write(' '.join(line)+'\\n')\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxs2WcCHRIM2"
      },
      "outputs": [],
      "source": [
        "# For every dataset it writes the respective file with the pos tags\n",
        "for dataset_name, dataset in datasets_train_dict_BIO.items():\n",
        "  write_pos_tags_on_file(f'POS_tags_{dataset_name[:-4]}', dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm7NrWvbROGp"
      },
      "outputs": [],
      "source": [
        "# Write the pos tags for the train and test datasets merged\n",
        "df_merged_train = pd.concat(datasets_train_dict_BIO.values(), ignore_index=True)\n",
        "df_merged_test = pd.concat(datasets_test_dict_BIO.values(), ignore_index=True)\n",
        "\n",
        "write_pos_tags_on_file('trainsets', df_merged_train)\n",
        "write_pos_tags_on_file('testsets', df_merged_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObHE-HlrRST4"
      },
      "source": [
        "### Plot stats POS tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJeZrL-NJdYq"
      },
      "outputs": [],
      "source": [
        "# Importing the model here for being sure the model is available\n",
        "import it_core_news_lg\n",
        "\n",
        "def get_pos_tags(sentences):\n",
        "  \n",
        "  '''\n",
        "  Computes and returns the pos tags of the sentences of the dataset.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentences: list\n",
        "    list of lists of words that is the list of the tokenized sentences.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pos_tags: list\n",
        "    list of lists of part-of-speech tags, in the same format of the input \n",
        "    sentences, there is a mapping one-to-one between words and tags.\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Loading the model for POS tagging\n",
        "  model = it_core_news_lg.load()\n",
        "  \n",
        "  # Join the tokens into sentences\n",
        "  sentences_tokens_merged = [' '.join(sentence) for sentence in sentences]\n",
        "\n",
        "  pos_tags = []\n",
        "  for sentence in sentences_tokens_merged: \n",
        "    # Process the sentence with SpaCy\n",
        "    doc = model(sentence)\n",
        "    sentence_pos = []\n",
        "    for token in doc:\n",
        "      sentence_pos.append(token.pos_)\n",
        "    pos_tags.append(sentence_pos)\n",
        "\n",
        "  return pos_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m86rJDWSJhBR"
      },
      "outputs": [],
      "source": [
        "def get_pos_tags_file(train_or_test=True):\n",
        "  '''\n",
        "  Retrieves the part-of-speech tags of the dataset in a format easy to use in\n",
        "  our analysis.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train_or_test: bool, optional\n",
        "    if True, the method returns the tags of the train set, if False, the method \n",
        "    returns the tags of the test set.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pos_tags: list\n",
        "    part-of-speech tags of the requested dataset.\n",
        "\n",
        "  '''\n",
        "\n",
        "  with open('/content/KIND_project/pos_tags/POS_tags_'+('train' if train_or_test else 'test')+'sets.txt', 'r') as f:\n",
        "    pos_tags = [line.replace('\\n', '').split(' ') for line in f]\n",
        "    return pos_tags\n",
        "  f.close()\n",
        "\n",
        "def get_pos_tags_specific_file(dataset_name):\n",
        "\n",
        "  '''\n",
        "  Retrieves the part-of-speech tags of a specific dataset in a format easy to use in our analysis.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  file_name: Str\n",
        "    Name of the dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pos_tags: list\n",
        "    part-of-speech tags of the requested dataset.\n",
        "\n",
        "  '''\n",
        "\n",
        "  with open(f'/content/KIND_project/pos_tags/POS_tags_{dataset_name}.txt', 'r') as f:\n",
        "    pos_tags = [line.replace('\\n', '').split(' ') for line in f]\n",
        "    return pos_tags\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPRbor2ATNqR"
      },
      "outputs": [],
      "source": [
        "def merge_counters(counter1, counter2):\n",
        "\n",
        "  '''\n",
        "  Takes 2 counters with different shapes and in the smallest one ad also the key that are currently inside with a value of 0\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  counter1: Counter\n",
        "    Counter with smallest shape that has to be incremented\n",
        "  counter2: Counter\n",
        "    Counter with the larger shape\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  new_counter: Counter\n",
        "    Counter containing all the tuple of the smallest one and the tuples (key, 0) of the larger one that were no present in the small one\n",
        "  \n",
        "  '''\n",
        "  \n",
        "  new_counter = counter1    \n",
        "\n",
        "  for key, value in counter2.items():\n",
        "    if key not in new_counter.keys():\n",
        "      new_counter[key] = 0 \n",
        "\n",
        "  return new_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzmUyHYDw-eR"
      },
      "outputs": [],
      "source": [
        "def plot_wordCloud_counters(counters):\n",
        "\n",
        "  '''\n",
        "  Takes as input a list of counters and it plots the wordCloud\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  cunters: list(Counter)\n",
        "    List of counters that has to be plotted. It does not require that all the counters has the same shape\n",
        "\n",
        "  '''\n",
        "\n",
        "  word_cloud_counter = Counter()\n",
        "  for counter in list_counters:\n",
        "    word_cloud_counter.update(counter)\n",
        "  # Generate a word cloud from the POS counts\n",
        "  wordcloud = WordCloud(background_color='white').generate_from_frequencies(word_cloud_counter)\n",
        "\n",
        "  # Plot the word cloud\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oFiL_p2unFX"
      },
      "outputs": [],
      "source": [
        "def plot_groupedBar_counters(counters):\n",
        "\n",
        "  '''\n",
        "  Takes as input a list of counters and it plots in the same bar chart the counts\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  cunters: list(Counter)\n",
        "    List of counters that has to be plotted. It does not require that all the counters has the same shape\n",
        "\n",
        "  '''\n",
        "  \n",
        "  max_length = max(map(len, list_counters))                         # max length of the counters\n",
        "  max_position = list(map(len, list_counters)).index(max_length)    # position in the list of the counter with max length\n",
        "\n",
        "  # For each counter that is not the one of maximum dimension I merge it with all the other ones. The result is a list with counters having all the same keys\n",
        "  for i in range(len(list_counters)):\n",
        "    if i != max_position:\n",
        "      list_counters[i] = merge_counters(list_counters[i], list_counters[max_position])\n",
        "\n",
        "  # We plot each counter inside the bar chart\n",
        "  x = np.arange(max_length)\n",
        "  width=0.2\n",
        "  multiplier = 0\n",
        "  for counter in list_counters:\n",
        "    offset = width * multiplier\n",
        "    labels, values = zip(*sorted(counter.items()))\n",
        "    plt.bar(x + offset, values, width=width)\n",
        "    multiplier += 1\n",
        "\n",
        "  plt.title(\"POS Tag Frequency Distribution for datasets\")\n",
        "  plt.xlabel(\"POS Tag\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.xticks(x + width, sorted(list_counters[max_position]), rotation='vertical')\n",
        "  plt.legend(get_datasets_names())\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RxvxxorJs-U"
      },
      "source": [
        "In the following code, we load all the tags for considering all the datasets of train as a unique one and we count the number of occurrence for each of them.\n",
        "After that, we plot their occurrences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef15ar7pJ2Ne"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each tag and plot them\n",
        "pos_freq_all_datasets = Counter([tag for sentence in get_pos_tags_file() for tag in sentence])\n",
        "list_counters = [pos_freq_all_datasets]\n",
        "plot_groupedBar_counters(list_counters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsDpin4LJ7CU"
      },
      "source": [
        "Here we do the same as before, but for any training dataset to visualize the \n",
        "differences between the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ5Kn_4HJ9PN"
      },
      "outputs": [],
      "source": [
        "list_counters = []\n",
        "# For each train dataset we take the save the occurrences of the tags\n",
        "for dataset_name in datasets_train_dict_BIO.keys():\n",
        "  tags = get_pos_tags_specific_file(dataset_name[:-4])\n",
        "  pos_freq = Counter([tag for sentence in tags for tag in sentence])\n",
        "  list_counters.append(pos_freq)\n",
        "\n",
        "# Plot of all the counter in the same plot to compare them\n",
        "plot_groupedBar_counters(list_counters)\n",
        "plot_wordCloud_counters(list_counters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnJ0Rfx5AUO2"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Indexing is the process by which search engines organize information before a search to enable super-fast responses to queries. \n",
        "Searching through individual pages for keywords and topics would be a very slow process for search engines to identify relevant information. Instead, search engines use an inverted index, also known as a reverse index.\n",
        "An inverted index is a system wherein a database of text elements is compiled along with pointers to the documents which contain those elements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYtA4wp9LXrN"
      },
      "source": [
        "First of all, since we don't have a dataset of documents we decided to index the sentences inside our datasets.\n",
        "To do that we have to create the sentences from the dictionary of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjbbdpR0AUO2"
      },
      "outputs": [],
      "source": [
        "sentences, labels, keys = get_all_sentences_from_datasets(datasets_train_dict_BIO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOedSyMGMnVQ"
      },
      "source": [
        "Then, we create the dataset of sentences adding also the Id to indetify them while the query part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIJ856g5AUO2"
      },
      "outputs": [],
      "source": [
        "sentences_df = pd.DataFrame(columns=['docno', 'text'])\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences_df.loc[i] = [f's{i}', ' '.join(sentences[i])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zimvJV0gM3Kj"
      },
      "source": [
        "In the following cell, we index the sentences dataframe. The index, with all its data structures, is written into a directory that we will call `index`.\n",
        "`Index_Ref` provides the location where the index is stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeAsq2sHAUO5"
      },
      "outputs": [],
      "source": [
        "index_path = \"./index\"\n",
        "\n",
        "indexer = pt.DFIndexer(index_path, overwrite=True)\n",
        "index_ref = indexer.index(sentences_df['text'], sentences_df['docno'])\n",
        "index_ref.toString()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBT349HRNRmO"
      },
      "source": [
        "Now we can now load the index and print it.\n",
        "This is a Terrier index structure, which provides methods such as:\n",
        " - `getCollectionStatistics()`\n",
        " - `getInvertedIndex()`\n",
        " - `getLexicon()`\n",
        "\n",
        " Let's see what is returned by the `CollectionStatistics()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTfdsLQ8AUO5"
      },
      "outputs": [],
      "source": [
        "index = pt.IndexFactory.of(index_ref)\n",
        "print(index.getCollectionStatistics().toString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkadPwj6O0Qk"
      },
      "source": [
        "Now that we have indexed our documents, we can run a search over the document collection.\n",
        "Here we used the TF-IDF weighting formula to rank the results. \n",
        "\n",
        "The `search()` method returns a dataframe with columns:\n",
        " - `qid`: the query identifier\n",
        " - `docid`: integer identifier for document \n",
        " - `docno`: string identifier for document\n",
        " - `rank`: rank position\n",
        " - `score`: tf-idf score\n",
        " - `query`: the input query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjvmRsOdG4QU"
      },
      "outputs": [],
      "source": [
        "br = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "\n",
        "queries = pd.DataFrame([[\"query1\", \"festa\"], [\"query2\", \"ammiraglio\"], [\"query3\", \"messaggio audio\"]], columns=[\"qid\", \"query\"])\n",
        "br(queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbca80El2prc"
      },
      "source": [
        "## Conditional random fields\n",
        "\n",
        "The conditional random fields model is a type of discriminative undirected probabilistic model. It can be seen as an extention of the Hidden Markov Models.\n",
        "\n",
        "HMM take a sequence of data, in our case a sequence of words, and assumes that the observations are given by a set of states that we don't observe but which we are interested to tag. There are transition probabilities between the states in the sequence (probability of going into a specific next state) and there are emission probability for each state that states which is the probability of a observation in a given state.\n",
        "\n",
        "HMM is a generative model: it aims to model the joint probability of hidden state and observation simultaniously. In this framework the aim is to understand the way the data we have are generated.\n",
        "\n",
        "HMM has some limitation:\n",
        "- transition and emission probabilities are static, in every position of the sentence in our case;\n",
        "- each state depends only on the previous state but in natural language processing this is not the case, also the observations can depend on previous states in the speech.\n",
        "\n",
        "Conditional random fields model can be imagined as an extension of the HMM where it is possible to define whatever dependency between the states and between the states and the observations. The graphical representation is an undirected graph where the nodes are the states and the observations. The states are connected with each other in an arbitrary way and a state depends on the state to which it is connected through an edge.\n",
        "\n",
        "It is clear that it is a more general framework for catching the dependencies between the state and also between states and observations.\n",
        "\n",
        "Conditional random fields want to model the probability of the state conditioned the observations. The model is not generative. In the case of named entity recognition the model has to discriminate whether a token is part of an entity or not, given the words of a sentence.\n",
        "Often a variant of the method, called linear chain CRF, is used, in particular when dealing with big datasets. In case of LCCRF only the dependencies between consequent states are considered whereas all the relations between observations and hidden states are possible.\n",
        "\n",
        "The underlying idea is similar to the one of the logistic regression.\n",
        "\n",
        "$$\\textbf{Mathematical model}$$\n",
        "\n",
        "Consider a set generic function, in this setting called feature function $$f_{j}(\\textbf{x}, y_{i-1}, y_{i}, i),$$\n",
        "with $\\textbf{x}$ observations of the problem, so in the NER case the words of the sentences, $y_{i-1}$ the i-1-th state in the hidden states of the CRF, so the previous state, $y_{i}$ the i-th state in the hidden states of the CRF, so subsequent to $y_{i-1}$, and $i$ is the index of the current state in the states chain, in the NER task it is the position in the sentence.\n",
        "\n",
        "The $f_{j}$ functions can be any possible function defined over that inputs, obviously for obtaining good results in the sequence labeling task they should be meaningful to the application.\n",
        "\n",
        "Given a set of feature functions, is then possible to define $F_j$ as\n",
        "$$F_j(\\textbf{y},\\textbf{x}) = \\sum_{i=1}^{N} f_j(\\textbf{x}, y_{i-1}, y_{i}, i)$$\n",
        "with $N$ length of the provided sequence of observations.\n",
        "\n",
        "In this setting, we define $\\theta_j$ as the weights of each function $F_j$, so $\\theta_j$ is giving more or less importance to some functions with respect to others. These will be the variables to optimize in the optimization step.\n",
        "\n",
        "The conditioned probability of the sequence of states $\\textbf{y}$ given the observations $\\textbf{x}$ and the parameters $\\boldsymbol{\\theta}$ is\n",
        "\n",
        "$$p(\\textbf{y}|\\textbf{x}, \\boldsymbol{\\theta}) = \\frac{1}{Z(\\textbf{x})}e^{\\boldsymbol{\\theta}^T \\cdot \\textbf{F}(\\textbf{y},\\textbf{x})},$$\n",
        "\n",
        "considering that $\\textbf{F}(\\textbf{y},\\textbf{x})$ is the vectorial representation of the $F_j$ and $\\boldsymbol{\\theta}$ is the vectorial representation of the $\\theta_j$ and $Z(\\textbf{x})$ a normalizing factor used to obtain a probability distribution.\n",
        "\n",
        "The weights of the feature funtions are determined in the training phase of the CRF, e.g. through gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubxUwELuuXda"
      },
      "source": [
        "### Application of the CRF to the dataset\n",
        "\n",
        "In this section the CRF method for named entity recognition is applied.\n",
        "\n",
        "The model that is used is the one coming from the library sklearn-crfsuite tha provides the funcitons for defining the model, for training and using it.\n",
        "\n",
        "The conditional random fields of sklearn-crfsuite requires as input a list o list of dictionaries. In the case of the named entity recognition task, each dictionary is a word, the elements of the dictionary are the features of the word that will be considered in the training and prediction step. A single list of dictionaries is a sentence.\n",
        "\n",
        "To summarize the input of the CRF training method will be a list of sentences, each one shaped as a list of dictionaries.\n",
        "\n",
        "In the following some function for creating this representation are defined and then it is done the actual training and usage of the CRF.\n",
        "\n",
        "The choise that has been done is to embed some features, defined \"standard\", in each trial and then enrich the representation by adding more elaborated features.\n",
        "\n",
        "The features defined as \"standard\" are:\n",
        "- word,\n",
        "- lowercased word,\n",
        "- whether the first letter is capitalized or not,\n",
        "- whether the whole word is capitalized or not,\n",
        "- whether the whole word is a number,\n",
        "- position of the word in the sentence.\n",
        "\n",
        "The additional features that can be added are:\n",
        "- whether the word contains symbol of punctuation or not,\n",
        "- features of the words that come before and after the considered word.\n",
        "\n",
        "In the following snippets of code are analyzed additional, in principle promising, features:\n",
        "- part-of-speech tags of the words,\n",
        "- belonging of the words and of the sequence of words to the provided gazetteer,\n",
        "- word2Vec embeddings of the words.\n",
        "\n",
        "The analysis is provided for the BIO-tagged dataset but at the end a single CRF model is trained and tested on the not BIO-tagged dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iW_yMemOo1l"
      },
      "source": [
        "#### Function definition\n",
        "\n",
        "Here we define some function needed for performing the named entity recognition task using crf-suite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etWsTi-_pshB"
      },
      "outputs": [],
      "source": [
        "def standard_features_from_word(sentence, position, prefix='', punctuation_evaluation=False):\n",
        "  '''\n",
        "  Converts a word of the sentence into its representation given by a standard\n",
        "  set of features that we consider the basis from which the feature vector can\n",
        "  be enlarged\n",
        "\n",
        "  This method has been implemented for having a crfsuite format representation\n",
        "  of the dataset in order to apply use the conditional random fields framework\n",
        "  provided by the sklearn_crfsuite package.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentence: list\n",
        "    list of the words composing the sentence.\n",
        "  position: integer\n",
        "    position in the list of the word that has to be converted into its feature \n",
        "    vector representation.\n",
        "  prefix: str, optional\n",
        "    string that can be appended before the names of the attributes of the \n",
        "    dictionary of features for specifying additional information.\n",
        "  punctuation_evaluation: bool, optional\n",
        "    True, to insert a feature about the fact that the word contains some\n",
        "    punctiation elments, False to avoid inserting it.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  standard_word_features: dict\n",
        "    dictionary of the standard features of the word\n",
        "\n",
        "  '''\n",
        "\n",
        "  standard_word_features = None\n",
        "  if position < len(sentence) and position >= 0:\n",
        "    word = sentence[position]\n",
        "    standard_word_features = {\n",
        "          prefix+'word' : word,\n",
        "          prefix+'word_lowercase' : word.lower(),\n",
        "          prefix+'first_letter_upper' : word[0].isupper(),\n",
        "          prefix+'all_letters_upper' : word.isupper(),\n",
        "          prefix+'all_digits' : word.isdigit(),\n",
        "          prefix+'position' : position,\n",
        "    }\n",
        "    if punctuation_evaluation:\n",
        "      contains_punctuation = False\n",
        "      for character in word:\n",
        "        if character in string.punctuation:\n",
        "          contains_punctuation = True\n",
        "          break\n",
        "      \n",
        "      standard_word_features.update({prefix+'punctuation_inside' : contains_punctuation})\n",
        "\n",
        "  return standard_word_features\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def get_gazetteer_features(sentence, position, gazetteer_dict, gazetteer_entity_lenght=3,\n",
        "                           prefix='', bio_tags=True):\n",
        "  '''\n",
        "  Returns the features related to the belonging of a word of the sentence to the \n",
        "  list of known entities.\n",
        "\n",
        "  This method has been implemented for having a crfsuite format representation\n",
        "  of the dataset in order to apply use the conditional random fields framework\n",
        "  provided by the sklearn_crfsuite package.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentence: list\n",
        "    list of the words composing the sentence.\n",
        "  position: integer\n",
        "    position in the list of the word that has to be converted into its feature \n",
        "    vector representation.\n",
        "  gazetteer_dict: dict\n",
        "    dictionary containing a elements the sets of the known entities, each set \n",
        "    contains a different type of entity.\n",
        "  gazetteer_entity_lenght: integer\n",
        "    maximum length, in terms of number of words, of the known named entities.\n",
        "    Incresing the length of the names of the entities will end up in a bigger \n",
        "    computational complexity but could end up in better results.\n",
        "  prefix: str, optional\n",
        "    string that can be appended before the names of the attributes of the \n",
        "    dictionary of features for specifying additional information.\n",
        "  bio_tags: bool, optional\n",
        "    True, for having a sort of begin-inside-outside notation in features related\n",
        "    to the known entities, False, for having only the inside-outside notation.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  gazetteer_features: dict\n",
        "    dictionary of the features of the word reguarding the belonging of the word\n",
        "    to the know entity lists.\n",
        "    \n",
        "  '''\n",
        "\n",
        "  gazetteer_features = {}\n",
        "  for key in gazetteer_dict:\n",
        "    found = False\n",
        "    for ent_len in range(1, gazetteer_entity_lenght+1):\n",
        "      for i in range(ent_len):\n",
        "        if position - i >= 0 and position + ent_len - i - 1 < len(sentence):\n",
        "          entity = ' '.join(sentence[(position - i):(position + ent_len - i - 1 + 1)])\n",
        "          if i == 0:\n",
        "            if entity in gazetteer_dict[key]:\n",
        "              if bio_tags:\n",
        "                gazetteer_features.update({prefix+'B-'+key: True})\n",
        "              else:\n",
        "                gazetteer_features.update({prefix+key: True})\n",
        "              found = True\n",
        "          else:\n",
        "            if entity in gazetteer_dict[key]:\n",
        "              if bio_tags:\n",
        "                gazetteer_features.update({prefix+'I-'+key: True})\n",
        "              else:\n",
        "                gazetteer_features.update({prefix+key: True})\n",
        "              found = True\n",
        "    if not found:\n",
        "      gazetteer_features.update({key: False})\n",
        "\n",
        "  return gazetteer_features\n",
        "    \n",
        "################################################################################\n",
        "\n",
        "def transform_word_to_features(sentence, position, postags=None, embeddings=None,\n",
        "                               previous_words_to_embed=1, next_words_to_embed=1,\n",
        "                               gazetteer=None, gazetteer_entity_lenght=3, bio_tags=True,\n",
        "                               punctuation_evaluation=False):\n",
        "  '''\n",
        "  Converts a word of the sentence into its representation given by a set of\n",
        "  features. Some features are inserted by default some others can be requested\n",
        "  through setting the optional parameters.\n",
        "\n",
        "  This method has been implemented for having a crfsuite format representation\n",
        "  of the dataset in order to apply use the conditional random fields framework\n",
        "  provided by the sklearn_crfsuite package.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentence: list\n",
        "    list of the words composing the sentence.\n",
        "  position: integer\n",
        "    position in the list of the word that has to be converted into its feature \n",
        "    vector representation.\n",
        "  postags: list, optional\n",
        "    list of the part-of-speech tags related to the words of the sentence. This\n",
        "    list has to be of the same length of the sentence list.\n",
        "  embeddings: list, optional\n",
        "    list of the embeddings related to the words of the sentence. This list has \n",
        "    to be of the same length of the sentence list.\n",
        "  previous_words_to_embed: integer, optional\n",
        "    number of words preceeding the considered word that hava to be embedded in \n",
        "    the feature representation of the considered word.\n",
        "  next_words_to_embed: integer, optional\n",
        "    number of words following the considered word that hava to be embedded in \n",
        "    the feature representation of the considered word.\n",
        "  gazetteer: dict, optional\n",
        "    dictionaty containing the known named entities. If None, it is assumed that\n",
        "    gazetteer cannot be used.\n",
        "  gazetteer_entity_lenght: integer, optional\n",
        "    maximum length, in terms of number of words, of the known named entities.\n",
        "    Incresing the length of the names of the entities will end up in a bigger \n",
        "    computational complexity but could end up in better results.\n",
        "  bio_tags: bool, optional\n",
        "    True, for having a sort of begin-inside-outside notation in features related\n",
        "    to the known entities, False, for having only the inside-outside notation.\n",
        "  punctuation_evaluation: bool, optional\n",
        "    True, to insert a feature about the fact that the word contains some\n",
        "    punctiation elments, False to avoid inserting it.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  word_features: dict\n",
        "    dictionary of the features of the word\n",
        "\n",
        "  '''\n",
        "\n",
        "  word_features = None\n",
        "  if position < len(sentence) and position >= 0:\n",
        "    # Getting the standard features for the word to convert into features\n",
        "    word_features = standard_features_from_word(sentence, position, punctuation_evaluation=punctuation_evaluation)\n",
        "\n",
        "    # Adding the embedding representation of the word to the features\n",
        "    if embeddings != None:\n",
        "      embedding = embeddings[position]\n",
        "      length = len(embedding)\n",
        "      for i in range(length):\n",
        "        word_features.update({'dim'+str(i)+'-embedding' : embedding[i]})\n",
        "\n",
        "    # Adding the POS tag to the features of the word\n",
        "    if postags != None:\n",
        "      word_features.update({'postag': postags[position]})\n",
        "    \n",
        "    # Embedding the context of the sentence inside the feature representation of\n",
        "    #Â the word, adding the features of the previous words in the sentence\n",
        "    if previous_words_to_embed > 0:\n",
        "      i = 1\n",
        "      while i < previous_words_to_embed + 1 and position-i >= 0:\n",
        "        word_features.update(standard_features_from_word(sentence, position-i, prefix='-'+str(i)+':', punctuation_evaluation=punctuation_evaluation))\n",
        "        if postags != None:\n",
        "          word_features.update({'-'+str(i)+':postag': postags[position-i]})\n",
        "        i += 1\n",
        "\n",
        "    # Embedding the context of the sentence inside the feature representation of\n",
        "    #Â the word, adding the features of the previous words in the sentence\n",
        "    if next_words_to_embed > 0:\n",
        "      i = 1\n",
        "      while i < next_words_to_embed + 1 and position+i <= len(sentence)-1:\n",
        "        word_features.update(standard_features_from_word(sentence, position+i, prefix='+'+str(i)+':', punctuation_evaluation=punctuation_evaluation))\n",
        "        if postags != None:\n",
        "          word_features.update({'+'+str(i)+':postag': postags[position+i]})\n",
        "        i += 1\n",
        "\n",
        "    if gazetteer != None:\n",
        "      word_features.update(get_gazetteer_features(sentence, position, gazetteer, bio_tags=bio_tags))\n",
        "  else:\n",
        "    print('Wrong input, the index of the word in the sentence is not valid.')\n",
        "  return word_features \n",
        "\n",
        "################################################################################\n",
        "\n",
        "def transform_sentences_to_features(sentences, postags=None, embeddings=None, \n",
        "                                    previous_words_to_embed=1, next_words_to_embed=1,\n",
        "                                    gazetteer=None, gazetteer_entity_lenght=3, bio_tags=True,\n",
        "                                    punctuation_evaluation=False):\n",
        "  '''\n",
        "  Converts the sentences into their representation given by a list of \n",
        "  dictionaries of features. Some features are inserted by default some others \n",
        "  can be requested through setting the optional parameters.\n",
        "\n",
        "  This method has been implemented for having a crfsuite format representation\n",
        "  of the dataset in order to apply use the conditional random fields framework\n",
        "  provided by the sklearn_crfsuite package.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentence: list\n",
        "    list of lists of the words composing the sentences.\n",
        "  postag: list, optional\n",
        "    list of lists of the part-of-speech tags related to the words of the \n",
        "    sentences. This list of lists has to have the same dimensions of the \n",
        "    sentence list.\n",
        "  embeddings: list, optional\n",
        "    list of lists of the embeddings related to the words of the sentences. This \n",
        "    list of lists has to have the same dimensions of the sentence list.\n",
        "  previous_words_to_embed: integer, optional\n",
        "    number of words preceeding the considered word that have to be embedded in \n",
        "    the feature representation of the considered word.\n",
        "  next_words_to_embed: integer, optional\n",
        "    number of words following the considered word that havae to be embedded in \n",
        "    the feature representation of the considered word.\n",
        "  gazetteer: dict, optional\n",
        "    dictionaty containing the known named entities. If None, it is assumed that\n",
        "    gazetteer cannot be used.\n",
        "  gazetteer_entity_lenght: integer, optional\n",
        "    maximum length, in terms of number of words, of the known named entities.\n",
        "    Incresing the length of the names of the entities will end up in a bigger \n",
        "    computational complexity but could end up in better results.\n",
        "  bio_tags: bool, optional\n",
        "    True, for having a sort of begin-inside-outside notation in features related\n",
        "    to the known entities, False, for having only the inside-outside notation.\n",
        "  punctuation_evaluation: bool, optional\n",
        "    True, to insert a feature about the fact that the word contains some\n",
        "    punctiation elments, False to avoid inserting it.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  sentence_features: list\n",
        "    representation of the sentences as a list of lists of dictionaries of features\n",
        "  \n",
        "  '''\n",
        "  \n",
        "  all_sentences_features = []\n",
        "  for index,sentence in tqdm(enumerate(sentences)):\n",
        "    sentence_features = []\n",
        "    for i in range(len(sentence)):\n",
        "      sentence_features.append(transform_word_to_features(sentence, i, postags=(postags[index] if postags != None else None), \n",
        "            embeddings=(embeddings[index] if embeddings != None else None), previous_words_to_embed=previous_words_to_embed, \n",
        "            next_words_to_embed=next_words_to_embed, gazetteer=gazetteer,\n",
        "            gazetteer_entity_lenght=gazetteer_entity_lenght, bio_tags=bio_tags,\n",
        "            punctuation_evaluation=punctuation_evaluation))\n",
        "    all_sentences_features.append(sentence_features)\n",
        "  return all_sentences_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHbzjBH9TEja"
      },
      "outputs": [],
      "source": [
        "# Providing example of the format of input that takes the functions generating\n",
        "# the features from the datasets\n",
        "sentences_input_example = [\n",
        "    ['Mark', 'Carman', 'Ã¨', 'fantastico!'],\n",
        "    ['Anche', 'il', 'dottor', 'Scotti', 'Ã¨', 'fantastico!'], \n",
        "    ['Ciao', 'sono', 'Mike', 'e', 'sono', 'a', 'Milano']]\n",
        "\n",
        "# Providing example of the output given by the functions generating the features\n",
        "# from the datasets\n",
        "output_features_example = transform_sentences_to_features(sentences_input_example, punctuation_evaluation=True)\n",
        "output_features_example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creation of the sentences and of the gazetteer sets"
      ],
      "metadata": {
        "id": "ax5JhSgR7lJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezEL64YtO_6y"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences and the labels from the training dataset\n",
        "sentences_merged_train_BIO, labels_merged_train_BIO, _ = get_all_sentences_from_datasets(datasets_train_dict_BIO, lower=False)\n",
        "# Getting the sentences and the labels from the test dataset\n",
        "sentences_merged_test_BIO, labels_merged_test_BIO, _ = get_all_sentences_from_datasets(datasets_test_dict_BIO, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpOtthwaTMTO"
      },
      "outputs": [],
      "source": [
        "# Definition of a gazetteer sets\n",
        "PATH_TO_GAZETTEER = '/content/KIND_project/datasets/Entities'\n",
        "gazetteer_loc = set(line.replace('LOC','').strip() for line in open(PATH_TO_GAZETTEER + '/it-LOC-wikipedia.txt', encoding='utf-8'))\n",
        "gazetteer_org = set(line.replace('ORG','').strip() for line in open(PATH_TO_GAZETTEER + '/it-ORG-wikipedia.txt', encoding='utf-8'))\n",
        "gazetteer_per = set(line.replace('PER','').strip() for line in open(PATH_TO_GAZETTEER + '/it-PER-wikipedia.txt', encoding='utf-8'))\n",
        "gazetteer_dict = {\n",
        "    'LOC': gazetteer_loc,\n",
        "    'ORG': gazetteer_org,\n",
        "    'PER': gazetteer_per,\n",
        "}\n",
        "gazetteer = gazetteer_loc.union(gazetteer_org).union(gazetteer_per)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ck75C9xO1cF"
      },
      "source": [
        "#### Fitting the model without POS tags and without gazetteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKI0m1gJqID_"
      },
      "outputs": [],
      "source": [
        "# Converting the training dataset in the one required for training the model to\n",
        "# perform the named entity recognition task through the conditional random \n",
        "# fields method\n",
        "sentences_features_merged_train_BIO = transform_sentences_to_features(sentences_merged_train_BIO, \n",
        "                                                                      previous_words_to_embed=0,\n",
        "                                                                      next_words_to_embed=0)\n",
        "\n",
        "# Converting the test dataset in the one required for performing the named \n",
        "# entity recognition task through the conditional random fields method\n",
        "sentences_features_merged_test_BIO = transform_sentences_to_features(sentences_merged_test_BIO, \n",
        "                                                                     previous_words_to_embed=0,\n",
        "                                                                     next_words_to_embed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIQ2LLKQh2k9"
      },
      "outputs": [],
      "source": [
        "#Creating the CRF model for NER\n",
        "ner_crf_model_BIO = CRF(algorithm='lbfgs',\n",
        "                    c1=0.1,\n",
        "                    c2=0.1,\n",
        "                    max_iterations=170,\n",
        "                    verbose=True,\n",
        "                    all_possible_transitions=False)\n",
        "\n",
        "# Fitting the CRF model using the training set\n",
        "ner_crf_model_BIO.fit(sentences_features_merged_train_BIO, labels_merged_train_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf8_Mou9Py1n"
      },
      "outputs": [],
      "source": [
        "# Defining the labels on which the performance evaluation has to be done\n",
        "classes_BIO = list(ner_crf_model_BIO.classes_)\n",
        "# Removing the class of words that are not entities in order to have the \n",
        "# evaluation of the performance considering only the classes of the entities\n",
        "classes_BIO.remove('O')\n",
        "classes_BIO.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNkvBdDBMnyw"
      },
      "outputs": [],
      "source": [
        "# Predicting the lables on the train set\n",
        "prediction_merged_train_BIO = ner_crf_model_BIO.predict(sentences_features_merged_train_BIO)\n",
        "# Predicting the lables on the test set\n",
        "prediction_merged_test_BIO = ner_crf_model_BIO.predict(sentences_features_merged_test_BIO)\n",
        "\n",
        "# Computing the flat f1-score on the train set\n",
        "f1_score_train_BIO = crf_metrics.flat_f1_score(labels_merged_train_BIO, prediction_merged_train_BIO, \n",
        "                                               average='weighted', labels=classes_BIO)\n",
        "# Computing the flat f1-score on the test set\n",
        "f1_score_test_BIO = crf_metrics.flat_f1_score(labels_merged_test_BIO, prediction_merged_test_BIO,\n",
        "                                              average='weighted', labels=classes_BIO)\n",
        "\n",
        "print('Flat f1-score on the train set: ' + str(f1_score_train_BIO))\n",
        "print('Flat f1-score on the test set: ' + str(f1_score_test_BIO))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the main performance metrics on the train set\n",
        "classification_report_train_BIO = crf_metrics.flat_classification_report(labels_merged_train_BIO, \n",
        "                                                                        prediction_merged_train_BIO,\n",
        "                                                                        labels=classes_BIO)\n",
        "print('Classification report on the train set: \\n')\n",
        "print(classification_report_train_BIO)"
      ],
      "metadata": {
        "id": "mszVRbaV60p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i04aWQtgJdTz"
      },
      "outputs": [],
      "source": [
        "# Computing the main performance metrics on the test set\n",
        "classification_report_test_BIO = crf_metrics.flat_classification_report(labels_merged_test_BIO, \n",
        "                                                                        prediction_merged_test_BIO,\n",
        "                                                                        labels=classes_BIO)\n",
        "print('Classification report on the test set: \\n')\n",
        "print(classification_report_test_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lJ-Rw9-Bvy2"
      },
      "outputs": [],
      "source": [
        "# Displaying the most important features in terms of weights assigned to them\n",
        "eli5.explain_weights(ner_crf_model_BIO, top=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mlVEmvMSG6I"
      },
      "source": [
        "From the metrics it is possible to see that the model reaches good performances even using just the standard features defined before and embedding the features of a single word before and after the considered word inside its features.\n",
        "\n",
        "Embedding more words of the context improves a bit the results but the marginal improvement, in terms of performance measures, decreases with the number of words embedded. Moreover inserting in the features of a word the ones of a lot of words next to it, increases a lot the computational complexity of the training.\n",
        "\n",
        "From the tables displayed using the eli5 packet, it is possible to recognise the fact that the transition probability between the BIO tags are catched by the model. In the subsequent table there are the weights associated to the possible values of the features. The model is mainly learning the words to recognise as entities and it is learning also the ones that are not entities, in particular we can identify a lot of stopwords in the outside class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZOf_7G_WKHD"
      },
      "source": [
        "#### Fitting the model with also gazetteer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgkGo3-BfwJV"
      },
      "outputs": [],
      "source": [
        "# Converting the training dataset in the one required for training the model to\n",
        "# perform the named entity recognition task through the conditional random \n",
        "# fields method, here it is considered also whether a word, or sequence of words,\n",
        "# is in the set of known entities.\n",
        "sentences_features_merged_train_gaz_BIO = transform_sentences_to_features(sentences_merged_train_BIO,\n",
        "                                                                          previous_words_to_embed=1,\n",
        "                                                                          next_words_to_embed=1,\n",
        "                                                                          gazetteer = gazetteer_dict,\n",
        "                                                                          gazetteer_entity_lenght=6)\n",
        "\n",
        "# Converting the test dataset in the one required for test the model to perform\n",
        "# the named entity recognition task through the conditional random fields method,\n",
        "# here it is considered also whether a word, or sequence of words, is in the set\n",
        "# of known entities.\n",
        "sentences_features_merged_test_gaz_BIO = transform_sentences_to_features(sentences_merged_test_BIO,\n",
        "                                                                         previous_words_to_embed=1,\n",
        "                                                                         next_words_to_embed=1,\n",
        "                                                                         gazetteer = gazetteer_dict,\n",
        "                                                                         gazetteer_entity_lenght=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15uRy39Of6Az"
      },
      "outputs": [],
      "source": [
        "#Creating the CRF model for NER\n",
        "ner_crf_model_gaz_BIO = CRF(algorithm='lbfgs',\n",
        "                            c1=0.1,\n",
        "                            c2=0.1,\n",
        "                            max_iterations=200,\n",
        "                            verbose=True,\n",
        "                            all_possible_transitions=False)\n",
        "\n",
        "# Fitting the CRF model using the training set\n",
        "ner_crf_model_gaz_BIO.fit(sentences_features_merged_train_gaz_BIO, labels_merged_train_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq-fbaVxf_Ov"
      },
      "outputs": [],
      "source": [
        "# Defining the labels on which the performance evaluation has to be done\n",
        "classes_gaz_BIO = list(ner_crf_model_gaz_BIO.classes_)\n",
        "# Removing the class of words that are not entities in order to have the \n",
        "# evaluation of the performance considering only the classes of the entities\n",
        "classes_gaz_BIO.remove('O')\n",
        "classes_gaz_BIO.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASBuESzVf_YD"
      },
      "outputs": [],
      "source": [
        "# Predicting the lables on the train set\n",
        "prediction_merged_train_gaz_BIO = ner_crf_model_gaz_BIO.predict(sentences_features_merged_train_gaz_BIO)\n",
        "# Predicting the lables on the test set\n",
        "prediction_merged_test_gaz_BIO = ner_crf_model_gaz_BIO.predict(sentences_features_merged_test_gaz_BIO)\n",
        "\n",
        "# Computing the flat f1-score on the train set\n",
        "f1_score_train_gaz_BIO = crf_metrics.flat_f1_score(labels_merged_train_BIO, prediction_merged_train_gaz_BIO,\n",
        "                                                   average='weighted', labels=classes_gaz_BIO)\n",
        "# Computing the flat f1-score on the test set\n",
        "f1_score_test_gaz_BIO = crf_metrics.flat_f1_score(labels_merged_test_BIO, prediction_merged_test_gaz_BIO,\n",
        "                                                  average='weighted', labels=classes_gaz_BIO)\n",
        "\n",
        "print('Flat f1-score on the train set: ' + str(f1_score_train_gaz_BIO))\n",
        "print('Flat f1-score on the test set: ' + str(f1_score_test_gaz_BIO))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the main performance metrics on the train set\n",
        "classification_report_train_gaz_BIO = crf_metrics.flat_classification_report(labels_merged_train_BIO,\n",
        "                                                                            prediction_merged_train_gaz_BIO,\n",
        "                                                                            labels=classes_gaz_BIO)\n",
        "print('Classification report on the train set: \\n')\n",
        "print(classification_report_train_gaz_BIO)"
      ],
      "metadata": {
        "id": "iEvH3P37CR29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dail1b8KDq0"
      },
      "outputs": [],
      "source": [
        "# Computing the main performance metrics on the test set\n",
        "classification_report_test_gaz_BIO = crf_metrics.flat_classification_report(labels_merged_test_BIO,\n",
        "                                                                                prediction_merged_test_gaz_BIO,\n",
        "                                                                                labels=classes_gaz_BIO)\n",
        "print('Classification report on the test set: ')\n",
        "print(classification_report_test_gaz_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GyfsMX2Ms12"
      },
      "outputs": [],
      "source": [
        "# Displaying the most important features in terms of weights assigned to them\n",
        "eli5.explain_weights(ner_crf_model_gaz_BIO, top=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance measure increase significantly adding also the gazetteer, this means that the model correctly learns some rules about the new added features.\n",
        "\n",
        "The learning of the model goes in the direction of assigning a big weight to words that in the learning appears to be entities, as in the previous case.\n",
        "\n",
        "The introduction of the feature of belonging to the gazetteer is correcty learnt by the model that assigns an high weight to the feature of the belonging of a word to an entity class inside the correct class. For example, it is possible to see that it assigns a big walue to the fact that 'B-LOC' is set to True for the words classifies as 'B-LOC'. This happens for all the classes of entities, for the person class it is a lot more important for the model to know the name and the previous word for finding a person entity that to read that the word belongs to the gazetteer.\n",
        "\n",
        "It can be noticed that the model also learns the fact that a named entity of one type cannot be of another type and so, for example, it assigns a negative weight to the features 'B-ORG' and 'I_LOC' for the classification in the class 'I_PER'.\n",
        "\n",
        "In order to improve more the meaninfulness of the usage of the gazetteer, it could be possible to add the named entity of the train set inside the gazetteer since they are manually annotated. This could be useful since some entities could not be in the gazetteer and some others could the written using a different form, e.g. using spaces or lines in different ways."
      ],
      "metadata": {
        "id": "8hkVDS7UE95J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTPlai-0O14n"
      },
      "source": [
        "#### Fitting the model with also POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M_p5xdGxGor"
      },
      "outputs": [],
      "source": [
        "# Converting the training dataset in the one required for training the model to\n",
        "# perform the named entity recognition task through the conditional random \n",
        "# fields method, here it is considered also whether a word, or sequence of words,\n",
        "# is in the set of known entities. The features in this section include also the\n",
        "# part-of-speech tags\n",
        "sentences_features_merged_train_gaz_pos_BIO = transform_sentences_to_features(sentences_merged_train_BIO,\n",
        "                                                                              postags = get_pos_tags(sentences_merged_train_BIO),\n",
        "                                                                              previous_words_to_embed=1,\n",
        "                                                                              next_words_to_embed=1,\n",
        "                                                                              gazetteer = gazetteer_dict,\n",
        "                                                                              gazetteer_entity_lenght=6)\n",
        "\n",
        "# Converting the test dataset in the one required for test the model to perform\n",
        "# the named entity recognition task through the conditional random fields method,\n",
        "# here it is considered also whether a word, or sequence of words, is in the set\n",
        "# of known entities. The features in this section include also the \n",
        "# part-of-speech tags\n",
        "sentences_features_merged_test_gaz_pos_BIO = transform_sentences_to_features(sentences_merged_test_BIO,\n",
        "                                                                             postags = get_pos_tags(sentences_merged_test_BIO),\n",
        "                                                                             previous_words_to_embed=1,\n",
        "                                                                             next_words_to_embed=1,\n",
        "                                                                             gazetteer = gazetteer_dict,\n",
        "                                                                             gazetteer_entity_lenght=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zibis_XCxI6_"
      },
      "outputs": [],
      "source": [
        "#Creating the CRF model for NER\n",
        "ner_crf_model_gaz_pos_BIO = CRF(algorithm='lbfgs',\n",
        "                                c1=0.1,\n",
        "                                c2=0.1,\n",
        "                                max_iterations=100,\n",
        "                                verbose=True,\n",
        "                                all_possible_transitions=False)\n",
        "\n",
        "# Fitting the CRF model using the training set\n",
        "ner_crf_model_gaz_pos_BIO.fit(sentences_features_merged_train_gaz_pos_BIO, labels_merged_train_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGUKJqOdaq5L"
      },
      "outputs": [],
      "source": [
        "# Defining the labels on which the performance evaluation has to be done\n",
        "classes_gaz_pos_BIO = list(ner_crf_model_gaz_pos_BIO.classes_)\n",
        "# Removing the class of words that are not entities in order to have the \n",
        "# evaluation of the performance considering only the classes of the entities\n",
        "classes_gaz_pos_BIO.remove('O')\n",
        "classes_gaz_pos_BIO.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4dHb7cZAFcN"
      },
      "outputs": [],
      "source": [
        "# Predicting the lables on the train set\n",
        "prediction_merged_train_gaz_pos_BIO = ner_crf_model_gaz_pos_BIO.predict(sentences_features_merged_train_gaz_pos_BIO)\n",
        "# Predicting the lables on the test set\n",
        "prediction_merged_test_gaz_pos_BIO = ner_crf_model_gaz_pos_BIO.predict(sentences_features_merged_test_gaz_pos_BIO)\n",
        "\n",
        "# Computing the flat f1-score on the train set\n",
        "f1_score_train_gaz_pos_BIO = crf_metrics.flat_f1_score(labels_merged_train_BIO, prediction_merged_train_gaz_pos_BIO,\n",
        "                                       average='weighted', labels=classes_gaz_pos_BIO)\n",
        "# Computing the flat f1-score on the test set\n",
        "f1_score_test_gaz_pos_BIO = crf_metrics.flat_f1_score(labels_merged_test_BIO, prediction_merged_test_gaz_pos_BIO,\n",
        "                                      average='weighted', labels=classes_gaz_pos_BIO)\n",
        "\n",
        "print('Flat f1-score on the train set: ' + str(f1_score_train_gaz_pos_BIO))\n",
        "print('Flat f1-score on the test set: ' + str(f1_score_test_gaz_pos_BIO))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9qlzvzkKKse"
      },
      "outputs": [],
      "source": [
        "# Computing the main performance metrics on the train set\n",
        "classification_report_train_gaz_pos_BIO = crf_metrics.flat_classification_report(labels_merged_train_BIO,\n",
        "                                                                                 prediction_merged_train_gaz_pos_BIO,\n",
        "                                                                                 labels=classes_gaz_pos_BIO)\n",
        "print('Classification report on the train set: \\n')\n",
        "print(classification_report_train_gaz_pos_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the main performance metrics on the test set\n",
        "classification_report_test_gaz_pos_BIO = crf_metrics.flat_classification_report(labels_merged_test_BIO,\n",
        "                                                                                prediction_merged_test_gaz_pos_BIO,\n",
        "                                                                                labels=classes_gaz_pos_BIO)\n",
        "print('Classification report on the test set: ')\n",
        "print(classification_report_test_gaz_pos_BIO)"
      ],
      "metadata": {
        "id": "LtXBZaODEEuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew5jAVSY-Xo6"
      },
      "outputs": [],
      "source": [
        "# Displaying the most important features in terms of weights assigned to them\n",
        "eli5.explain_weights(ner_crf_model_gaz_pos_BIO, top=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performace measures slightly improve adding also the part-of-speech tags.\n",
        "\n",
        "The learning of the model still goes in the direction of assigning a big weight to words that in the learning phase appear to be entities, as in the previous cases.\n",
        "\n",
        "All the part-of-speech tags have a very big weight for the outside class, the one that is assigned to the words that are not entities. This could be cause by the fact that the model sees many more words that are not named entities with that features than the ones that are entities.\n",
        "Even if this is not so informative, and even if a lot of pos tags have negative weights in the other classes, it is interesting to notice that pos tags such as 'VERB' and 'ADJ' have a very negative weights in the case of classes that are not the 'O' one. The model has learnt that the words that are verbs or adjectives, with high probability, are not named entities."
      ],
      "metadata": {
        "id": "WChLkFhePGm1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCuMssv9yT-v"
      },
      "source": [
        "#### Fitting the model using embeddings representation of the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNxJUdbCF4bR"
      },
      "outputs": [],
      "source": [
        "# Creating the embedding representation of each word keeping the same structure\n",
        "# and ordering of the list of sentences in order to be able to create the \n",
        "# features of the words starting from the list of the embeddings\n",
        "embedding_train = [[get_word2vec_vector(sentences_merged_train_BIO[i][j], embeddings_model_train) for j in range(len(sentences_merged_train_BIO[i]))] for i in range(len(sentences_merged_train_BIO))]\n",
        "embedding_test = [[get_word2vec_vector(sentences_merged_test_BIO[i][j], embeddings_model_train) for j in range(len(sentences_merged_test_BIO[i]))] for i in range(len(sentences_merged_test_BIO))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zpj972wya_1"
      },
      "outputs": [],
      "source": [
        "# Converting the training dataset in the one required for training the model to\n",
        "# perform the named entity recognition task through the conditional random \n",
        "# fields method with as features also the embedding representation of the words\n",
        "sentences_features_merged_train_emb_BIO = transform_sentences_to_features(sentences_merged_train_BIO,\n",
        "                                                                          previous_words_to_embed=0,\n",
        "                                                                          next_words_to_embed=0,\n",
        "                                                                          embeddings = embedding_train)\n",
        "\n",
        "# Converting the test dataset in the one required for performing the named \n",
        "# entity recognition task through the conditional random fields method with as \n",
        "# features also the embedding representation of the words\n",
        "sentences_features_merged_test_emb_BIO = transform_sentences_to_features(sentences_merged_test_BIO,\n",
        "                                                                         previous_words_to_embed=0,\n",
        "                                                                         next_words_to_embed=0,\n",
        "                                                                         embeddings = embedding_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FomjSaVH2ay"
      },
      "outputs": [],
      "source": [
        "#Creating the CRF model for NER\n",
        "ner_crf_model_emb_BIO = CRF(algorithm='lbfgs',\n",
        "                            c1=0.1,\n",
        "                            c2=0.1,\n",
        "                            max_iterations=200,\n",
        "                            verbose=True,\n",
        "                            all_possible_transitions=False)\n",
        "\n",
        "# Fitting the CRF model using the training set\n",
        "ner_crf_model_emb_BIO.fit(sentences_features_merged_train_emb_BIO, labels_merged_train_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiVr2pZNISkQ"
      },
      "outputs": [],
      "source": [
        "# Defining the labels on which the performance evaluation has to be done\n",
        "classes_emb_BIO = list(ner_crf_model_emb_BIO.classes_)\n",
        "# Removing the class of words that are not entities in order to have the \n",
        "# evaluation of the performance considering only the classes of the entities\n",
        "classes_emb_BIO.remove('O')\n",
        "classes_emb_BIO.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79hAoxhCIVQZ"
      },
      "outputs": [],
      "source": [
        "# Predicting the lables on the train set\n",
        "prediction_merged_train_emb_BIO = ner_crf_model_emb_BIO.predict(sentences_features_merged_train_emb_BIO)\n",
        "# Predicting the lables on the test set\n",
        "prediction_merged_test_emb_BIO = ner_crf_model_emb_BIO.predict(sentences_features_merged_test_emb_BIO)\n",
        "\n",
        "# Computing the flat f1-score on the train set\n",
        "f1_score_train_emb_BIO = crf_metrics.flat_f1_score(labels_merged_train_BIO, prediction_merged_train_emb_BIO,\n",
        "                                                   average='weighted', labels=classes_emb_BIO)\n",
        "# Computing the flat f1-score on the test set\n",
        "f1_score_test_emb_BIO = crf_metrics.flat_f1_score(labels_merged_test_BIO, prediction_merged_test_emb_BIO,\n",
        "                                                  average='weighted', labels=classes_emb_BIO)\n",
        "\n",
        "print('Flat f1-score on the train set: ' + str(f1_score_train_emb_BIO))\n",
        "print('Flat f1-score on the test set: ' + str(f1_score_test_emb_BIO))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the main performance metrics on the train set\n",
        "classification_report_train_emb_BIO = crf_metrics.flat_classification_report(labels_merged_train_BIO,\n",
        "                                                                             prediction_merged_train_emb_BIO,\n",
        "                                                                             labels=classes_emb_BIO)\n",
        "print('Classification report on the train set: \\n')\n",
        "print(classification_report_train_emb_BIO)"
      ],
      "metadata": {
        "id": "58TWnwoqWlnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLZkKIYEDTIa"
      },
      "outputs": [],
      "source": [
        "# Computing the main performance metrics on the test set\n",
        "classification_report_test_emb_BIO = crf_metrics.flat_classification_report(labels_merged_test_BIO,\n",
        "                                                                             prediction_merged_test_emb_BIO,\n",
        "                                                                             labels=classes_emb_BIO)\n",
        "print('Classification report on the test set: \\n')\n",
        "print(classification_report_test_emb_BIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVCFLSnOhuIY"
      },
      "outputs": [],
      "source": [
        "# Displaying the most important features in terms of weights assigned to them\n",
        "eli5.explain_weights(ner_crf_model_emb_BIO, top=3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that for the model is more important to learn the words and the weights associated to the embedding representation of the words are not among the highest ones. It is more inportant the word itself with respect to the context in which it is contained.\n",
        "\n",
        "The improvement of the metrics with respect to the first and basic case is very low and the time required for the training of the CRF model in the case of embeddings is longer compared to the one taken by the others, even with a similar amount of total features.\n",
        "\n",
        "Due to this results we can state that for the model is more important to have the features of the closest words instead of a more wide context in the sentence contained in the embeddings.\n",
        "\n",
        "Probably using embedding models trained over a bigger amout of data would have improved the performance in the recognition of the named entity. The choise of Word2Vec embeddings is due to the higher time complexity of the translation of the words using other models."
      ],
      "metadata": {
        "id": "HHPUquJcXHtN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db99sXL081Lp"
      },
      "source": [
        "#### Fitting the model using the data with only the inside-outside notation\n",
        "\n",
        "In this section the named entity recognition task is performed on the dataset without inside-outside notation.\n",
        "\n",
        "The features used in this section are the standard ones, the belonging to the gazetteers and the part-of-speech tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnbuRsU79DyT"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences and the labels from the training dataset\n",
        "sentences_merged_train_IO, labels_merged_train_IO, _ = get_all_sentences_from_datasets(datasets_train_dict_IO, lower=False)\n",
        "# Getting the sentences and the labels from the test dataset\n",
        "sentences_merged_test_IO, labels_merged_test_IO, _ = get_all_sentences_from_datasets(datasets_test_dict_IO, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9otyACKr9D5O"
      },
      "outputs": [],
      "source": [
        "# Converting the training dataset without BIO notation in the format required\n",
        "# for training the model to perform the named entity recognition task through \n",
        "# the conditional random fields method, here it is considered also whether a word,\n",
        "# or sequence of words, is in the set of known entities. The features in this \n",
        "# section include also the part-of-speech tags.\n",
        "sentences_features_merged_train_gaz_pos_IO = transform_sentences_to_features(sentences_merged_train_IO,\n",
        "                                                                             postags = get_pos_tags(sentences_merged_train_IO),\n",
        "                                                                             previous_words_to_embed=2, \n",
        "                                                                             next_words_to_embed=2,\n",
        "                                                                             gazetteer = gazetteer_dict,\n",
        "                                                                             gazetteer_entity_lenght=6,\n",
        "                                                                             bio_tags=False)\n",
        "\n",
        "# Converting the test dataset without BIO notation in the format required for\n",
        "# test the model to perform the named entity recognition task through the \n",
        "# conditional random fields method, here it is considered also whether a word,\n",
        "# or sequence of words, is in the set of known entities. The features in this \n",
        "# section include also the part-of-speech tags.\n",
        "sentences_features_merged_test_gaz_pos_IO = transform_sentences_to_features(sentences_merged_test_IO,\n",
        "                                                                            postags = get_pos_tags(sentences_merged_test_IO),\n",
        "                                                                            previous_words_to_embed=2,\n",
        "                                                                            next_words_to_embed=2,\n",
        "                                                                            gazetteer = gazetteer_dict,\n",
        "                                                                            gazetteer_entity_lenght=6,\n",
        "                                                                            bio_tags=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCLSiaJtQ5yY"
      },
      "outputs": [],
      "source": [
        "#Creating the CRF model for NER\n",
        "ner_crf_model_gaz_pos_IO = CRF(algorithm='lbfgs',\n",
        "                               c1=0.1,\n",
        "                               c2=0.1,\n",
        "                               max_iterations=200,\n",
        "                               verbose=True,\n",
        "                               all_possible_transitions=False)\n",
        "\n",
        "# Fitting the CRF model using the training set\n",
        "ner_crf_model_gaz_pos_IO.fit(sentences_features_merged_train_gaz_pos_IO, labels_merged_train_IO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byptwoy0dkeW"
      },
      "outputs": [],
      "source": [
        "# Defining the labels on which the performance evaluation has to be done\n",
        "classes_gaz_pos_IO = list(ner_crf_model_gaz_pos_IO.classes_)\n",
        "# Removing the class of words that are not entities in order to have the \n",
        "# evaluation of the performance considering only the classes of the entities\n",
        "classes_gaz_pos_IO.remove('O')\n",
        "classes_gaz_pos_IO.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o78s9gfbUoWe"
      },
      "outputs": [],
      "source": [
        "# Predicting the lables on the train set\n",
        "prediction_merged_train_gaz_pos_IO = ner_crf_model_gaz_pos_IO.predict(sentences_features_merged_train_gaz_pos_IO)\n",
        "# Predicting the lables on the test set\n",
        "prediction_merged_test_gaz_pos_IO = ner_crf_model_gaz_pos_IO.predict(sentences_features_merged_test_gaz_pos_IO)\n",
        "\n",
        "# Computing the flat f1-score on the train set\n",
        "f1_score_train_gaz_pos_IO = crf_metrics.flat_f1_score(labels_merged_train_IO, prediction_merged_train_gaz_pos_IO,\n",
        "                                                      average='weighted', labels=classes_gaz_pos_IO)\n",
        "# Computing the flat f1-score on the test set\n",
        "f1_score_test_gaz_pos_IO = crf_metrics.flat_f1_score(labels_merged_test_IO, prediction_merged_test_gaz_pos_IO,\n",
        "                                                     average='weighted', labels=classes_gaz_pos_IO)\n",
        "\n",
        "print('Flat f1-score on the train set: ' + str(f1_score_train_gaz_pos_IO))\n",
        "print('Flat f1-score on the test set: ' + str(f1_score_test_gaz_pos_IO))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the main performance metrics on the train set\n",
        "classification_report_train_gaz_pos_IO = crf_metrics.flat_classification_report(labels_merged_train_IO,\n",
        "                                                                                 prediction_merged_train_gaz_pos_IO,\n",
        "                                                                                 labels=classes_gaz_pos_IO)\n",
        "print('Classification report on the train set: \\n')\n",
        "print(classification_report_train_gaz_pos_IO)"
      ],
      "metadata": {
        "id": "O9ISRJRIltGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the main performance metrics on the test set\n",
        "classification_report_test_gaz_pos_IO = crf_metrics.flat_classification_report(labels_merged_test_IO,\n",
        "                                                                                prediction_merged_test_gaz_pos_IO,\n",
        "                                                                                labels=classes_gaz_pos_IO)\n",
        "print('Classification report on the test set: ')\n",
        "print(classification_report_test_gaz_pos_IO)"
      ],
      "metadata": {
        "id": "yNPgslmpltIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0jFz8dUktU"
      },
      "outputs": [],
      "source": [
        "# Displaying the most important features in terms of weights assigned to them\n",
        "eli5.explain_weights(ner_crf_model_gaz_pos_IO, top=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The considerations done for the section that uses the CRF with gazetteers and part-of-speech tags can be applied also in this case.\n",
        "\n",
        "The only thing to notice is an improvement in terms of performance measure since it is more easy for the model to distinguish between 4 classes with respect to 7 classes."
      ],
      "metadata": {
        "id": "2g7pIFyXqB1T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tREvMQ9jFp21"
      },
      "source": [
        "### Model evaluation\n",
        "In this section, we trained 15 CRF models by representing the datasets using gazetteers and POS tagging. Each of the 15 models was trained on every possible subset of the dataset set and tested on the corresponding test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unordered_tuples(s, limit=10000000):\n",
        "  '''\n",
        "  Generate unordered tuples from a set of elements.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  s : set\n",
        "      The input set of elements.\n",
        "  limit : int, optional\n",
        "      The maximum number of tuples to generate. Defaults to a large value. Usefull for debugging.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuples : list\n",
        "      A list of tuples representing the generated unordered combinations.\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Convert set to list to ensure consistent ordering\n",
        "  elements = list(s)\n",
        "  n = len(elements)\n",
        "\n",
        "  # Generate tuples of different lengths from 0 to n\n",
        "  tuples = []\n",
        "  for r in range(1, min(n + 1, limit)):\n",
        "      tuples.extend(combinations(elements, r))\n",
        "  \n",
        "  return tuples"
      ],
      "metadata": {
        "id": "5tkvy51AWJ5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_labels(datasets_train_dict_BIO, datasets_test_dict_BIO):\n",
        "  '''\n",
        "  Extract sentences and labels from training and test datasets.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  datasets_train_dict_BIO : dict\n",
        "      A dictionary containing the training datasets in BIO format.\n",
        "  datasets_test_dict_BIO : dict\n",
        "      A dictionary containing the test datasets in BIO format.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  sentences_train : list\n",
        "      A list of sentences from the training datasets.\n",
        "  labels_train : list\n",
        "      A list of corresponding labels for the sentences in the training datasets.\n",
        "  sentences_test : list\n",
        "      A list of sentences from the test datasets.\n",
        "  labels_test : list\n",
        "      A list of corresponding labels for the sentences in the test datasets.\n",
        "  '''\n",
        "\n",
        "  # Getting the sentences and the labels from the training dataset\n",
        "  sentences_train, labels_train, _ = get_all_sentences_from_datasets(datasets_train_dict_BIO, lower=False)\n",
        "  # Getting the sentences and the labels from the test dataset\n",
        "  sentences_test, labels_test, _ = get_all_sentences_from_datasets(datasets_test_dict_BIO, lower=False)\n",
        "  \n",
        "  return sentences_train, labels_train, sentences_test, labels_test"
      ],
      "metadata": {
        "id": "2kqfCr5GWv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_CRF_POSandGazetters_model(sentences_train, labels_train, sentences_test): \n",
        "  '''\n",
        "  Converting the training dataset in the one required for training the model to\n",
        "  perform the named entity recognition task through the conditional random fields method\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentences_train : list\n",
        "      A list of sentences from the training dataset.\n",
        "  labels_train : list\n",
        "      A list of corresponding labels for the sentences in the training dataset.\n",
        "  sentences_test : list\n",
        "      A list of sentences from the test dataset.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ner_crf_model_gaz_pos : CRF model\n",
        "      A trained CRF model for named entity recognition using POS tags and gazetteers.\n",
        "  sentences_features_train_gaz_pos : list\n",
        "      Transformed features of the training sentences for CRF modeling.\n",
        "  sentences_features_test_gaz_pos : list\n",
        "      Transformed features of the test sentences for CRF modeling.\n",
        "  '''\n",
        "    \n",
        "  sentences_features_train_gaz_pos = transform_sentences_to_features(sentences_train, postags = get_pos_tags(sentences_train), gazetteer = gazetteer_dict)\n",
        "\n",
        "  # Converting the test dataset in the one required for performing the named \n",
        "  # entity recognition task through the conditional random fields method\n",
        "  sentences_features_test_gaz_pos = transform_sentences_to_features(sentences_test, postags = get_pos_tags(sentences_test), gazetteer = gazetteer_dict)\n",
        "\n",
        "  sentences_features_train_gaz_pos[0]\n",
        "\n",
        "  #Creating the CRF model for NER\n",
        "  ner_crf_model_gaz_pos = CRF(algorithm='lbfgs',\n",
        "                          c1=0.5,\n",
        "                          c2=0.5,\n",
        "                          max_iterations=100,\n",
        "                          verbose=True,\n",
        "                          all_possible_transitions=False)\n",
        "\n",
        "  # Fitting the CRF model using the training set\n",
        "  ner_crf_model_gaz_pos.fit(sentences_features_train_gaz_pos, labels_train)\n",
        "\n",
        "  return ner_crf_model_gaz_pos, sentences_features_train_gaz_pos, sentences_features_test_gaz_pos"
      ],
      "metadata": {
        "id": "0gLvxh28XDda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_evaluate(sentences_features_train_gaz_pos, sentences_features_test_gaz_pos, labels_train, labels_test, ner_crf_model_gaz_pos):\n",
        "  '''\n",
        "  Predict labels using a trained CRF model and evaluate the performance.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sentences_features_train_gaz_pos : list\n",
        "      Transformed features of the training sentences for CRF modeling.\n",
        "  sentences_features_test_gaz_pos : list\n",
        "      Transformed features of the test sentences for CRF modeling.\n",
        "  labels_train : list\n",
        "      A list of corresponding labels for the sentences in the training dataset.\n",
        "  labels_test : list\n",
        "      A list of corresponding labels for the sentences in the test dataset.\n",
        "  ner_crf_model_gaz_pos : CRF model\n",
        "      A trained CRF model for named entity recognition using POS tags and gazetteers.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  '''\n",
        "  performance_train : dict\n",
        "  performance_test : dict\n",
        "  # Predicting the lables on the train set\n",
        "  prediction_train_BIO = ner_crf_model_gaz_pos.predict(sentences_features_train_gaz_pos)\n",
        "  \n",
        "  # Predicting the lables on the test set\n",
        "  prediction_test_BIO = ner_crf_model_gaz_pos.predict(sentences_features_test_gaz_pos)\n",
        "  \n",
        "  eli5.show_weights(ner_crf_model_gaz_pos, top=30)\n",
        "  performance_train = performance_metrics(prediction_train_BIO, labels_train)\n",
        "  performance_test = performance_metrics(prediction_test_BIO, labels_test)\n",
        "\n",
        "  return performance_train, performance_test"
      ],
      "metadata": {
        "id": "g_lHUd9FXoeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of a gazetteer set\n",
        "PATH_TO_GAZETTEER = '/content/KIND_project/datasets/Entities'\n",
        "gazetteer_loc = set(line.replace('LOC','').strip() for line in open(PATH_TO_GAZETTEER + '/it-LOC-wikipedia.txt', encoding='utf-8'))\n",
        "gazetteer_org = set(line.replace('ORG','').strip() for line in open(PATH_TO_GAZETTEER + '/it-ORG-wikipedia.txt', encoding='utf-8'))\n",
        "gazetteer_per = set(line.replace('PER','').strip() for line in open(PATH_TO_GAZETTEER + '/it-PER-wikipedia.txt', encoding='utf-8'))\n",
        "gazetteer_dict = {\n",
        "    'LOC': gazetteer_loc,\n",
        "    'ORG': gazetteer_org,\n",
        "    'PER': gazetteer_per,\n",
        "}\n",
        "gazetteer = gazetteer_loc.union(gazetteer_org).union(gazetteer_per)"
      ],
      "metadata": {
        "id": "_mK96GI1YWI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate subsets of datasets, train and evaluate CRF models on each subset, and store the results.\n",
        "\n",
        "The code generates subsets of datasets by creating combinations of dataset names from the training and test datasets.\n",
        "For each subset, it extracts the corresponding sentences and labels from the training and test datasets.\n",
        "\n",
        "Then, it trains a CRF model for named entity recognition using the extracted sentences and labels, along with\n",
        "POS tags and gazetteer features. The transformed features are obtained using the 'transform_sentences_to_features'\n",
        "function.\n",
        "\n",
        "The trained CRF model is used to predict labels for both the training and test datasets.\n",
        "\n",
        "Performance metrics are calculated by calling the 'predict_and_evaluate' function, which takes the predicted labels,\n",
        "true labels, and the CRF model as input.\n",
        "\n",
        "The results are stored in a list of dictionaries, where each dictionary contains the dataset name and the performance\n",
        "metrics for both the training and test datasets.\n",
        "\n",
        "Finally, the results are saved in a pickle file named 'data.pkl'."
      ],
      "metadata": {
        "id": "WfLOZ-M_ZdhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subsets_dataset_names_train_BIO = generate_unordered_tuples(set(dataset_names_train_BIO))\n",
        "subsets_dataset_names_test_BIO = generate_unordered_tuples(set(dataset_names_test_BIO))\n",
        "\n",
        "subsets_datasets_train_dict_BIO = []\n",
        "subsets_datasets_test_dict_BIO = []\n",
        "\n",
        "for i in range(0, len(subsets_dataset_names_train_BIO)):\n",
        "    current_subset_train_dict = {}\n",
        "    current_subset_test_dict = {}\n",
        "    for j in range(0, len(subsets_dataset_names_train_BIO[i])):\n",
        "      current_subset_train_dict[subsets_dataset_names_train_BIO[i][j]] = datasets_train_dict_BIO[subsets_dataset_names_train_BIO[i][j]]\n",
        "      current_subset_test_dict[subsets_dataset_names_test_BIO[i][j]] = datasets_test_dict_BIO[subsets_dataset_names_test_BIO[i][j]]\n",
        "    subsets_datasets_train_dict_BIO.append(current_subset_train_dict)\n",
        "    subsets_datasets_test_dict_BIO.append(current_subset_test_dict)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in range(0, len(subsets_datasets_train_dict_BIO)):\n",
        "    print('#'*30 + ' ' + str(i) + ' ' + '#'*30)\n",
        "    sentences_train, labels_train, sentences_test, labels_test = extract_sentence_labels(subsets_datasets_train_dict_BIO[i], subsets_datasets_test_dict_BIO[i])\n",
        "\n",
        "    ner_crf_model_gaz_pos, sentences_features_train_gaz_pos, sentences_features_test_gaz_pos = get_CRF_POSandGazetters_model(sentences_train, labels_train, sentences_test)\n",
        "\n",
        "    res_train, res_test = predict_and_evaluate(sentences_features_train_gaz_pos, \n",
        "                                                    sentences_features_test_gaz_pos, \n",
        "                                                    labels_train, \n",
        "                                                    labels_test, \n",
        "                                                    ner_crf_model_gaz_pos)\n",
        "    results.append({'dataset_name': str(subsets_dataset_names_train_BIO[i]).replace('_train_BIO.tsv', '').replace('(', '').replace(')', '').replace(',', '+').replace(\"'\", \"\"),\n",
        "                    'results_train': res_train,\n",
        "                    'results_test': res_test\n",
        "                    })\n",
        "\n",
        "with open('data.pkl', 'wb') as file:\n",
        "    pickle.dump(results, file)\n"
      ],
      "metadata": {
        "id": "tbdJ-wcvZOoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are stored in a file, and then saved also in an excel-readable format. All the results are reported here:\n",
        "\n",
        "https://polimi365-my.sharepoint.com/:x:/g/personal/10698684_polimi_it/EdaThkYp9yRHq2lP5zpqp_0Bk08nJXpgZZFQ0n13fFFnOQ?e=5b0rDU"
      ],
      "metadata": {
        "id": "tZCkzlMveUqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.pkl', 'rb') as file:\n",
        "    results = pickle.load(file)\n",
        "\n",
        "for i in results:\n",
        "    filename = './crf_tables/' + i['dataset_name']\n",
        "    i['results_train'].to_excel(filename + '_train.xlsx', index=False)\n",
        "    i['results_test'].to_excel(filename + '_test.xlsx', index=False)"
      ],
      "metadata": {
        "id": "XFB1rkaLeQru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates spider plots to visualize the performance metrics of a model trained on different datasets. \n",
        "The code loads the results from a pickle file named 'data.pkl' and extracts the performance metrics for each dataset. \n",
        "The metrics include accuracy, recall, precision, and F1 score. The datasets are predefined in a list called 'datasets'. \n",
        "The code then creates spider plots for each metric, where each dataset is represented as a vertex on the plot. \n",
        "The train and test performance values for each dataset are connected to form a polygon. Finally, the plots are displayed."
      ],
      "metadata": {
        "id": "tNbyTn13ffcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.pkl', 'rb') as file:\n",
        "    results = pickle.load(file)\n",
        "\n",
        "datasets            = ['wiki', 'deGa', 'fict', 'moro', 'wiki+dega', 'wiki+fict', 'wiki+moro', 'dega+fict', 'dega+moro', 'fict+moro', 'wiki+dega+fict', 'wiki+dega+moro', 'wiki+fict+moro', 'dega+fict+moro', 'all']\n",
        "train_accuracy      = []\n",
        "test_accuracy       = []\n",
        "train_recall        = []\n",
        "test_recall         = []\n",
        "train_precision     = []\n",
        "test_precision      = []\n",
        "train_f1_score      = []\n",
        "test_f1_score       = []\n",
        "\n",
        "for i in results:\n",
        "\n",
        "    train_accuracy.append(np.mean(i['results_train']['accuracy']))\n",
        "    train_recall.append(np.mean(i['results_train']['recall']))\n",
        "    train_precision.append(np.mean(i['results_train']['precision']))\n",
        "    train_f1_score.append(np.mean(i['results_train']['F1-score']))\n",
        "\n",
        "    test_accuracy.append(np.mean(i['results_test']['accuracy']))\n",
        "    test_recall.append(np.mean(i['results_test']['recall']))\n",
        "    test_precision.append(np.mean(i['results_test']['precision']))\n",
        "    test_f1_score.append(np.mean(i['results_test']['F1-score']))\n",
        "\n",
        "datasets = np.array(datasets)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "train_recall = np.array(train_recall)\n",
        "test_recall = np.array(test_recall)\n",
        "train_precision = np.array(train_precision)\n",
        "test_precision = np.array(test_precision)\n",
        "train_f1_score = np.array(train_f1_score)\n",
        "test_f1_score = np.array(test_f1_score)\n",
        "metrics = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\n",
        "\n",
        "# Create spider plots\n",
        "\n",
        "for metric in metrics:\n",
        "    # Prepare train and test data\n",
        "    if metric == 'Accuracy':\n",
        "        train_data = train_accuracy\n",
        "        test_data = test_accuracy\n",
        "    elif metric == 'Recall':\n",
        "        train_data = train_recall\n",
        "        test_data = test_recall\n",
        "    elif metric == 'Precision':\n",
        "        train_data = train_precision\n",
        "        test_data = test_precision\n",
        "    elif metric == 'F1 Score':\n",
        "        train_data = train_f1_score\n",
        "        test_data = test_f1_score\n",
        "\n",
        "\n",
        "    # Plot spider plot\n",
        "    angles = np.linspace(0, 2 * np.pi, len(datasets), endpoint=False)\n",
        "    plt.figure(figsize=(7,7))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    ax.plot(angles.tolist() + angles[:1].tolist(), train_data.tolist() + train_data[:1].tolist(), label='Train set')\n",
        "    ax.plot(angles.tolist() + angles[:1].tolist(), test_data.tolist() + test_data[:1].tolist(), label='Test set')\n",
        "    ax.fill(angles.tolist() + angles[:1].tolist(), train_data.tolist() + train_data[:1].tolist(), alpha=0.25)\n",
        "    ax.fill(angles.tolist() + angles[:1].tolist(), test_data.tolist() + test_data[:1].tolist(), alpha=0.25)\n",
        "    ax.set_xticks(angles)\n",
        "    ax.set_xticklabels(datasets)\n",
        "    bt = min(train_data.tolist() + test_data.tolist())//10 * 10\n",
        "    ax.set_ylim(bottom = bt)\n",
        "    plt.title(metric)\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "# Show all the plots\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "\"\"\n",
        "'''"
      ],
      "metadata": {
        "id": "8fqLZAb8eqFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, the datasets are heavily unbalanced, especially towards the \"not an entity\" class. \n",
        " Therefore, calculating the overall metrics of the model in the standard way is not meaningful. For this reason, we calculated the metrics in\n",
        "   a balanced manner. For example, the balanced accuracy is obtained by averaging the accuracies of all classes. \n",
        "   \n",
        "  The results are shown in the \n",
        "   following graphs.\n",
        "   \n",
        "   Firstly, we can observe that there is no overfitting of the model on the dataset in any case, although some models did not \n",
        "   generalize well (it can be noted that all the models trained on a dataset containing the \"fiction\" dataset experienced a significant decrease \n",
        "   in all metrics). \n",
        "   \n",
        "   The model trained on all datasets except \"fiction\" achieved the best performance."
      ],
      "metadata": {
        "id": "3aRS-oW3fox7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvMVjgT4flfT"
      },
      "source": [
        "\n",
        "## NER Transformers\n",
        "\n",
        "A transformer is a deep learning model that is different from the classical models since it adopts **self-attention.** It is used primarly in the fields of NLP and computer vision.\n",
        "In general there are two big families of transformers:\n",
        "- Transformer encoder layers, like BERT.\n",
        "- Transformer decoder layers, like GPT.\n",
        "\n",
        "Bert was pre-trained for two tasks:\n",
        "- Language modeling;\n",
        "- Next sentence prediction\n",
        "\n",
        "As a result of this training process, BERT learns latent representations of words and sentences in context. After pre-training, BERT can be **fine-tuned** with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference, text classification) and sequence-to-sequence based language generation tasks (question-answering, conversational response generation).\n",
        "\n",
        "In our case we will fine-tune BERT for Token Classification, that takes as input a sequence of tokens and the respective entity tag.\n",
        "\n",
        "Since the dataset is already tokenized and cleaned by the authors we won't apply any type of pre-processing.\n",
        "\n",
        "We will use two different models pre-trained for the italian language:\n",
        "- bert italian xxl cased\n",
        "- bert base multilingual.\n",
        "\n",
        "For each of them we performed cross-validation to choose the most important hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpdjHxmxpwv9"
      },
      "outputs": [],
      "source": [
        "def createDataset(train, test):\n",
        "\n",
        "  '''\n",
        "  Takes two dataframes and it creates an Hugging Face Dictionary of Datasets\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train: DataFrame\n",
        "    Train DataFrame\n",
        "  test: DataFrame\n",
        "    Test DataFrame\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  dataset_hf: DatasetDict\n",
        "    Dictionary of Hugging Face Datasets\n",
        "  '''\n",
        "  \n",
        "  train_dataset = Dataset.from_pandas(train)\n",
        "  test_dataset = Dataset.from_pandas(test)\n",
        "  dataset_hf = DatasetDict()\n",
        "  \n",
        "  dataset_hf['train'] = train_dataset\n",
        "  dataset_hf['test'] = test_dataset\n",
        "\n",
        "  return dataset_hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMEFpcr5XkKW"
      },
      "source": [
        "In the following code we create all the data structures that we will need to \n",
        "fine-tune our model.\n",
        "We decided to use as train dataset the union of all the datasets given.\n",
        "Then, we will predict and evaluate the performances of each model over all the test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb5P1UfzhXux"
      },
      "outputs": [],
      "source": [
        "# Create a dataset composed of all the train datasets\n",
        "datasets_merged = pd.concat(datasets_train_dict_BIO.values(), ignore_index=True)\n",
        "\n",
        "# Create of a dictionary that maps all the entity inside the dataset with an incremental id\n",
        "entity_names = pd.unique(datasets_merged['Entity'])\n",
        "entity_names_dict = {}\n",
        "for i, label in enumerate(entity_names):\n",
        "  entity_names_dict[label] = i\n",
        "\n",
        "id2label = {}\n",
        "for i, label in enumerate(entity_names):\n",
        "  id2label[i] = label\n",
        "\n",
        "# Creation of all the sentence and the respected labels for the entire dataset\n",
        "sentences, labels, keys = get_sentences_list_from_df(datasets_merged)\n",
        "train_dataset = pd.DataFrame(columns=['Tokens', 'Labels'])\n",
        "\n",
        "# Creation of a DataFrame with all the sentences\n",
        "for i in range(len(sentences)):\n",
        "  train_dataset.at[i, 'Tokens'] = sentences[i]\n",
        "  train_dataset.at[i, 'Labels'] = labels[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4iMtx2ci2Qu"
      },
      "outputs": [],
      "source": [
        "# Controll of the possibility to use cuda\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Download bert models from hugging face\n",
        "model_name_xxl = 'dbmdz/bert-base-italian-xxl-cased'\n",
        "model_name_multi = 'bert-base-multilingual-cased'\n",
        "\n",
        "model_xxl = AutoModelForTokenClassification.from_pretrained(model_name_xxl, num_labels=len(entity_names), label2id=entity_names_dict, id2label=id2label).to(device)\n",
        "model_multi = AutoModelForTokenClassification.from_pretrained(model_name_multi, num_labels=len(entity_names), label2id=entity_names_dict, id2label=id2label).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4kQLn48jh1z"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "\n",
        "    '''\n",
        "    Takes a batch of examples. Tokenize the examples and since the bert is performing a sub-tokenization we have to allign the original labels to the actual sub-tokens.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    examples: dict\n",
        "      Dictionary having as values for the key Tokens a list of list of tokenized senteces.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tokenized_inputs: dict\n",
        "      dictionary that containt the input_ids created by the bert tokenizer and the respected labels for each token alligned as the initial ones.\n",
        "  \n",
        "    '''\n",
        "    # Tokenization of the sentences\n",
        "    label_all_tokens = True\n",
        "    tokenized_inputs = tokenizer(list(examples[\"Tokens\"]), truncation=True, is_split_into_words=True)\n",
        "\n",
        "    # Alignment of the labels with the original tokens\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['Labels']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif label[word_idx] == '0':\n",
        "                label_ids.append(0)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(entity_names_dict[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(entity_names_dict[label[word_idx]] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "        \n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4eIA2Ktsvu8"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "\n",
        "    '''\n",
        "    It calculates the metrics of a batch of predictions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p: tuple\n",
        "      Tuple composed of the predictions by the transformer and the respected original labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    entity_metrics: dict\n",
        "      dictionary of dictionaries that contains for each entity the respected computed metrics\n",
        "\n",
        "    '''\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    entity_metrics = {}\n",
        "    for entity_name in entity_names:\n",
        "\n",
        "        # Take the predictions and the labels of the specific entity (when the original one was that entity)\n",
        "        true_predictions = [[entity_names[p] for (p, l) in zip(prediction, label) if l != -100 and entity_names[l] == entity_name] for prediction, label in zip(predictions, labels)]\n",
        "        true_labels = [[entity_names[l] for (p, l) in zip(prediction, label) if l != -100 and entity_names[l] == entity_name] for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "        # Compute the metrics for that specific entity\n",
        "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "        entity_metrics[entity_name] = {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }\n",
        "\n",
        "    return entity_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqSauVJuygP5"
      },
      "outputs": [],
      "source": [
        "def compute_mean_metrics_cv(results):\n",
        "\n",
        "  '''\n",
        "  It calculates the metrics of a cross-validation.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  results: list\n",
        "    list that contains all the metrics calculated for each entity every round of the cross-validation.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  metrics: dict\n",
        "    dictionary of dictionaries that contains for each entity the mean of the metrics.\n",
        "\n",
        "  '''\n",
        "\n",
        "  metrics = {}\n",
        "  for entity_name in entity_names:\n",
        "    metrics[entity_name] = {\n",
        "            \"precision\": sum([scores[f'eval_{entity_name}']['precision'] for scores in results]) / len(results),\n",
        "            \"recall\":  sum([scores[f'eval_{entity_name}']['recall'] for scores in results]) / len(results),\n",
        "            \"f1\":  sum([scores[f'eval_{entity_name}']['f1'] for scores in results]) / len(results),\n",
        "            \"accuracy\":  sum([scores[f'eval_{entity_name}']['accuracy'] for scores in results]) / len(results)\n",
        "        }\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpu2JiOvXznM"
      },
      "source": [
        "### Cross-Validation\n",
        "Here we did a cross-validation to find the best hyperparameters for our model.\n",
        "Calculating the performances of each round and then the mean of all the rounds for that specific pair of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alQ8OBv7mumU"
      },
      "outputs": [],
      "source": [
        "learning_rates = [2e-5, 3e-5]\n",
        "epochs = [2,3,4]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_xxl)\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "results = []\n",
        "# Perform of cross-validation\n",
        "for lr in learning_rates:\n",
        "  for epoch in epochs:\n",
        "    results_cv = []\n",
        "    for i in range(3):    # Cross-validation made of three rounds\n",
        "      train, val = train_test_split(train_dataset, test_size=0.2, shuffle=True)\n",
        "      dataset_hf = createDataset(train, val)\n",
        "      tokenized_datasets = dataset_hf.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "      # Definition of the Transformer's arguments\n",
        "      args = TrainingArguments(output_dir='ner',\n",
        "                               evaluation_strategy = \"no\",\n",
        "                               learning_rate=lr,\n",
        "                               per_device_train_batch_size=32,\n",
        "                               per_device_eval_batch_size=32,\n",
        "                               num_train_epochs=epoch,\n",
        "                               weight_decay=0.01,\n",
        "                               push_to_hub=False,\n",
        "                               disable_tqdm=False\n",
        "                              )\n",
        "\n",
        "      # Creation of the trainer\n",
        "      trainer = Trainer(\n",
        "          model_xxl,\n",
        "          args,\n",
        "          train_dataset=tokenized_datasets['train'],\n",
        "          eval_dataset = tokenized_datasets['test'],\n",
        "          data_collator=data_collator,\n",
        "          compute_metrics=compute_metrics,\n",
        "          tokenizer=tokenizer,\n",
        "      )\n",
        "      trainer.train()\n",
        "      results_cv.append(trainer.evaluate())             # Add the matrics for that round to the results of the actual cross-validation\n",
        "    results.append(compute_mean_metrics_cv(results_cv)) # Calculate the metrics' mean of the cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es9ivtESX8fn"
      },
      "source": [
        "### Build the final model\n",
        "Now, that we found the best hyperparameter, we can train the model over the entire training set.\n",
        "In this case, first of all, we train the italian model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1TjLp_AMs4"
      },
      "outputs": [],
      "source": [
        "dataset_hf = Dataset.from_pandas(train_dataset)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_xxl)\n",
        "tokenized_dataset = dataset_hf.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "args = TrainingArguments(output_dir='ner',\n",
        "                          evaluation_strategy = \"no\",\n",
        "                          learning_rate=2e-5,\n",
        "                          per_device_train_batch_size=32,\n",
        "                          per_device_eval_batch_size=32,\n",
        "                          num_train_epochs=4,\n",
        "                          weight_decay=0.01,\n",
        "                          push_to_hub=False,\n",
        "                          disable_tqdm=False\n",
        "                        )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_xxl,\n",
        "    args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAVOgYPtYAoR"
      },
      "source": [
        "We evaluate the model over all the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJFafjESGd4f"
      },
      "outputs": [],
      "source": [
        "results_test = []\n",
        "for test in datasets_test_dict_BIO.values():\n",
        "  sentences, labels, keys = get_sentences_list_from_df(test)\n",
        "  test_dataset = pd.DataFrame(columns=['Tokens', 'Labels'])\n",
        "  for i in range(len(sentences)):\n",
        "    test_dataset.at[i, 'Tokens'] = sentences[i]\n",
        "    test_dataset.at[i, 'Labels'] = labels[i]\n",
        "    \n",
        "  test_dataset = Dataset.from_pandas(test_dataset)\n",
        "  test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "  results_test.append(trainer.evaluate(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9zAa8xeGpsX"
      },
      "outputs": [],
      "source": [
        "datasets_test_merged = pd.concat(datasets_test_dict_BIO.values(), ignore_index=True)\n",
        "sentences, labels, keys = get_sentences_list_from_df(datasets_test_merged)\n",
        "test_dataset = pd.DataFrame(columns=['Tokens', 'Labels'])\n",
        "for i in range(len(sentences)):\n",
        "  test_dataset.at[i, 'Tokens'] = sentences[i]\n",
        "  test_dataset.at[i, 'Labels'] = labels[i]\n",
        "test_dataset = Dataset.from_pandas(test_dataset)\n",
        "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CECQUBYoYJhr"
      },
      "source": [
        "We repeat the same for the multilingual model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcamNzjVYMol"
      },
      "outputs": [],
      "source": [
        "dataset_hf = Dataset.from_pandas(train_dataset)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_multi)\n",
        "tokenized_dataset = dataset_hf.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "args = TrainingArguments(output_dir='ner',\n",
        "                          evaluation_strategy = \"no\",\n",
        "                          learning_rate=2e-5,\n",
        "                          per_device_train_batch_size=16,\n",
        "                          per_device_eval_batch_size=16,\n",
        "                          num_train_epochs=4,\n",
        "                          weight_decay=0.01,\n",
        "                          push_to_hub=False,\n",
        "                          disable_tqdm=False\n",
        "                        )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_multi,\n",
        "    args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gmsk5BEYOxN"
      },
      "outputs": [],
      "source": [
        "results_test = []\n",
        "for test in datasets_test_dict_BIO.values():\n",
        "  sentences, labels, keys = get_sentences_list_from_df(test)\n",
        "  test_dataset = pd.DataFrame(columns=['Tokens', 'Labels'])\n",
        "  for i in range(len(sentences)):\n",
        "    test_dataset.at[i, 'Tokens'] = sentences[i]\n",
        "    test_dataset.at[i, 'Labels'] = labels[i]\n",
        "    \n",
        "  test_dataset = Dataset.from_pandas(test_dataset)\n",
        "  test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "  results_test.append(trainer.evaluate(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCOsAC0MYQ9T"
      },
      "outputs": [],
      "source": [
        "datasets_test_merged = pd.concat(datasets_test_dict_BIO.values(), ignore_index=True)\n",
        "sentences, labels, keys = get_sentences_list_from_df(datasets_test_merged)\n",
        "test_dataset = pd.DataFrame(columns=['Tokens', 'Labels'])\n",
        "for i in range(len(sentences)):\n",
        "  test_dataset.at[i, 'Tokens'] = sentences[i]\n",
        "  test_dataset.at[i, 'Labels'] = labels[i]\n",
        "test_dataset = Dataset.from_pandas(test_dataset)\n",
        "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation\n"
      ],
      "metadata": {
        "id": "65qmPdLtt7wX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\"Merged\" model evaluation\n",
        "\n",
        "In these bar graphs, we can see the results obtained by the transformer trained on all datasets in a more detailed manner. Each graph represents a metric, each group of columns represents a test set, and each column represents a class. We can observe that the 'moro' test set is the easiest to classify, and the classes that are generally better classified across the various metrics are the entity types 'person,' while the most challenging to detect are the 'organization' types\n"
      ],
      "metadata": {
        "id": "YI-JQxNxuQN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transf_barplot(bars, group_names, column_names,  title):\n",
        "\n",
        "\n",
        "    column_names = [i.replace('eval_', ' ') for i in column_names]\n",
        "    # Set the positions and width of the bars\n",
        "    bar_width = 0.1\n",
        "    bar_positions = np.arange(len(group_names))\n",
        "    # Plot the bars\n",
        "    plt.figure()\n",
        "    for i in range(len(bars)):\n",
        "        plt.bar(bar_positions + (3-i)*bar_width, bars[i], width=bar_width, label=column_names[i])\n",
        "    # Set the x-axis labels and title\n",
        "    plt.xticks(bar_positions, group_names)\n",
        "    plt.xlabel('Test Sets')\n",
        "    plt.ylabel('Values (%)')\n",
        "    plt.title(title)\n",
        "    plt.ylim(50, 100)\n",
        "    # Add a legend\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "U93ZJ4lAuPar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('single.pkl', 'rb') as file:\n",
        "    single = pickle.load(file)\n",
        "\n",
        "with open('all.pkl', 'rb') as file:\n",
        "    all = pickle.load(file)\n",
        "\n",
        "data_dicts = single + [all]\n",
        "labels = ['eval_B-ORG', 'eval_I-ORG', 'eval_B-LOC', 'eval_I-LOC', 'eval_B-PER', 'eval_I-PER']\n",
        "metrics = ['accuracy', 'recall', 'precision', 'f1']\n",
        "\n",
        "graphs = []\n",
        "\n",
        "for metric in metrics:\n",
        "    bars = []\n",
        "    for label in labels:\n",
        "        values = []\n",
        "        for res in data_dicts:\n",
        "            values.append(res[label][metric]*100)\n",
        "        bars.append(values)\n",
        "    graphs.append(bars)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(graphs)):\n",
        "    transf_barplot(graphs[i],['deGasperi', 'Fiction', 'Moro', 'Wikinews', 'Merged'], labels , metrics[i])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFaJNLHxuegT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\"Single\" vs \"merged\" models evaluation"
      ],
      "metadata": {
        "id": "gh6dC2Rkwj6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_spiderplot(data, datasets):\n",
        "    '''\n",
        "    Generates spider plots for evaluation metrics based on the provided data and datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: list\n",
        "        List of dictionaries containing evaluation metrics for each dataset.\n",
        "    datasets: list\n",
        "        List of dataset names corresponding to the evaluation metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Displays spider plots for each evaluation metric.\n",
        "    '''\n",
        "\n",
        "    accuracy = []\n",
        "    recall = []\n",
        "    precision = []\n",
        "    f1_score = []\n",
        "\n",
        "    labels = ['eval_B-LOC', 'eval_B-ORG', 'eval_B-PER', 'eval_I-LOC', 'eval_I-ORG', 'eval_I-PER']\n",
        "    for i in data:\n",
        "        acc = []\n",
        "        rec = []\n",
        "        pre = []\n",
        "        f1 = []\n",
        "        for j in labels:\n",
        "            acc.append(i[j]['accuracy']*100)\n",
        "            rec.append(i[j]['recall']*100)\n",
        "            pre.append(i[j]['precision']*100)\n",
        "            f1.append(i[j]['f1']*100)\n",
        "\n",
        "        accuracy.append(np.mean(acc))\n",
        "        recall.append(np.mean(rec))\n",
        "        precision.append(np.mean(pre))\n",
        "        f1_score.append(np.mean(f1))\n",
        "\n",
        "    accuracy = np.array(accuracy)\n",
        "    recall = np.array(recall)\n",
        "    precision = np.array(precision)\n",
        "    f1_score = np.array(f1_score)\n",
        "\n",
        "    # Create spider plots\n",
        "    metrics_name = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\n",
        "    metrics = [accuracy, recall, precision, f1_score]\n",
        "\n",
        "\n",
        "    for k in range(0, len(metrics)):\n",
        "        i = metrics[k]\n",
        "        j = metrics_name[k]\n",
        "        # Plot spider plot\n",
        "        angles = np.linspace(0, 2 * np.pi, len(datasets), endpoint=False)\n",
        "        plt.figure(figsize = (7,7))\n",
        "        ax = plt.subplot(111, polar=True)\n",
        "        ax.plot(angles.tolist() + angles[:1].tolist(), i.tolist() + i[:1].tolist(), label='Train')\n",
        "        ax.fill(angles.tolist() + angles[:1].tolist(), i.tolist() + i[:1].tolist(), alpha=0.25)\n",
        "        ax.set_xticks(angles)\n",
        "        ax.set_xticklabels(datasets)\n",
        "        bt = min(i.tolist())//10 * 10\n",
        "        ax.set_ylim(bottom = bt, top = 100)\n",
        "        plt.title(j)\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "    # Show all the plots\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "e_DL81eXuuL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_spiderplot_allmetrics(data, datasets, title):\n",
        "    '''\n",
        "    Generates spider plots for all evaluation metrics based on the provided data, datasets, and title.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: list\n",
        "        List of dictionaries containing evaluation metrics for each dataset.\n",
        "    datasets: list\n",
        "        List of dataset names corresponding to the evaluation metrics.\n",
        "    title: str\n",
        "        Title for the spider plot.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Displays the spider plot for all evaluation metrics.\n",
        "    '''\n",
        "    accuracy = []\n",
        "    recall = []\n",
        "    precision = []\n",
        "    f1_score = []\n",
        "\n",
        "    labels = ['eval_B-LOC', 'eval_B-ORG', 'eval_B-PER', 'eval_I-LOC', 'eval_I-ORG', 'eval_I-PER']\n",
        "    for i in data:\n",
        "        acc = []\n",
        "        rec = []\n",
        "        pre = []\n",
        "        f1 = []\n",
        "        for j in labels:\n",
        "            acc.append(i[j]['accuracy']*100)\n",
        "            rec.append(i[j]['recall']*100)\n",
        "            pre.append(i[j]['precision']*100)\n",
        "            f1.append(i[j]['f1']*100)\n",
        "\n",
        "        accuracy.append(np.mean(acc))\n",
        "        recall.append(np.mean(rec))\n",
        "        precision.append(np.mean(pre))\n",
        "        f1_score.append(np.mean(f1))\n",
        "\n",
        "    accuracy = np.array(accuracy)\n",
        "    recall = np.array(recall)\n",
        "    precision = np.array(precision)\n",
        "    f1_score = np.array(f1_score)\n",
        "\n",
        "    # Create spider plots\n",
        "    metrics_name = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\n",
        "    metrics = [accuracy, recall, precision, f1_score]\n",
        "\n",
        "    # Plot spider plot\n",
        "    angles = np.linspace(0, 2 * np.pi, len(datasets), endpoint=False)\n",
        "    plt.figure(figsize = (7,7))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    for k in range(0, len(metrics)):\n",
        "        i = metrics[k]\n",
        "        ax.plot(angles.tolist() + angles[:1].tolist(), i.tolist() + i[:1].tolist(), label=metrics_name[k])\n",
        "        ax.fill(angles.tolist() + angles[:1].tolist(), i.tolist() + i[:1].tolist(), alpha=0.25)\n",
        "    ax.set_xticks(angles)\n",
        "    ax.set_xticklabels(datasets)\n",
        "    bt = min(i.tolist())//10 * 10 - 5\n",
        "    ax.set_ylim(bottom = bt, top = 100)\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "Xc-HdO-Kv7Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_single_test = ['all_degasperi.pkl', 'all_moro.pkl', 'all_fiction.pkl', 'all_wikinews.pkl']\n",
        "single_train_all_test = ['single_degasperi.pkl', 'single_moro.pkl', 'single_fiction.pkl', 'single_wikinews.pkl']\n",
        "\n",
        "######################################################################################################################################################\n",
        "#plots results of the transformer trained on the merged dataset, tested on the 4 single test sets\n",
        "data1 = []\n",
        "for i in all_train_single_test:\n",
        "    with open(i, 'rb') as file:\n",
        "        single = pickle.load(file)\n",
        "    data1.append(single)\n",
        "\n",
        "transformer_spiderplot_allmetrics(data1, ['degasperi', 'moro', 'fiction', 'wikinews'], \"mergedTransformer on all test sets\")\n",
        "\n",
        "######################################################################################################################################################\n",
        "#plots results of the transformers trained on the single datasets and tested on their relative test set\n",
        "data2 = []\n",
        "for i in range(len(single_train_all_test)):\n",
        "    with open(single_train_all_test[i], 'rb') as file:\n",
        "        single = pickle.load(file)\n",
        "    data2.append(single[i])\n",
        "\n",
        "with open('all.pkl', 'rb') as file:\n",
        "    all_on_all = pickle.load(file)\n",
        "data2.append(all_on_all)\n",
        "\n",
        "transformer_spiderplot_allmetrics(data2, ['degasperi', 'moro', 'fiction', 'wikinews', 'merged'], \"singleTransformer on all test sets\")\n",
        "\n",
        "######################################################################################################################################################\n",
        "#plot the results of the CRF and the transformed, trained on the single datasets and on the merged dataset, tested on the respective teste sets\n",
        "df_data = []\n",
        "with open('data.pkl', 'rb') as file:\n",
        "    crf_data = pickle.load(file)\n",
        "\n",
        "allowed = ['degasperi+', 'moro+', 'fiction+', 'wikinews+', 'wikinews+ degasperi+ fiction+ moro']\n",
        "for i in range(len(allowed)):\n",
        "    for j in crf_data:\n",
        "        if j['dataset_name'] == allowed[i]:\n",
        "            df_data.append(j['results_test'])\n",
        "\n",
        "labels = ['eval_B-LOC', 'eval_B-ORG', 'eval_B-PER', 'eval_I-LOC', 'eval_I-ORG', 'eval_I-PER']\n",
        "columns = ['set_of_feature', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "\n",
        "data3 = []\n",
        "data1.append(all_on_all)\n",
        "for dict in data1:\n",
        "    metrics_df = None\n",
        "    for inner_dict in labels:\n",
        "        AC = dict[inner_dict]['accuracy']\n",
        "        RC = dict[inner_dict]['recall']\n",
        "        PR = dict[inner_dict]['precision'] \n",
        "        F1 = dict[inner_dict]['f1']\n",
        "        if metrics_df is None:\n",
        "          metrics_df = pd.DataFrame([[inner_dict, AC, RC, PR, F1]], columns=columns)\n",
        "        else:\n",
        "          metrics_df = pd.concat([metrics_df,pd.DataFrame([[inner_dict, AC, RC, PR, F1]], columns=columns)])\n",
        "    data3.append(metrics_df)\n",
        "\n",
        "\n",
        "with open('data.pkl', 'rb') as file:\n",
        "    results = pickle.load(file)\n",
        "\n",
        "datasets            = ['wiki', 'deGa', 'fict', 'moro', 'all']\n",
        "CRF_accuracy      = []\n",
        "transf_accuracy       = []\n",
        "CRF_recall        = []\n",
        "transf_recall         = []\n",
        "CRF_precision     = []\n",
        "transf_precision      = []\n",
        "CRF_f1_score      = []\n",
        "transf_f1_score       = []\n",
        "\n",
        "for i in df_data:\n",
        "    i.reset_index(drop=True, inplace=True)\n",
        "    i.drop(i.index[2], inplace=True)\n",
        "    CRF_accuracy.append(np.mean(i['accuracy']))\n",
        "    CRF_recall.append(np.mean(i['recall']))\n",
        "    CRF_precision.append(np.mean(i['precision']))\n",
        "    CRF_f1_score.append(np.mean(i['F1-score']))\n",
        "\n",
        "for i in data3:\n",
        "    transf_accuracy.append(np.mean(i['accuracy'])*100)\n",
        "    transf_recall.append(np.mean(i['recall'])*100)\n",
        "    transf_precision.append(np.mean(i['precision'])*100)\n",
        "    transf_f1_score.append(np.mean(i['F1-score'])*100)\n",
        "\n",
        "\n",
        "datasets = np.array(datasets)\n",
        "CRF_accuracy = np.array(CRF_accuracy)\n",
        "transf_accuracy = np.array(transf_accuracy)\n",
        "CRF_recall = np.array(CRF_recall)\n",
        "transf_recall = np.array(transf_recall)\n",
        "CRF_precision = np.array(CRF_precision)\n",
        "transf_precision = np.array(transf_precision)\n",
        "CRF_f1_score = np.array(CRF_f1_score)\n",
        "transf_f1_score = np.array(transf_f1_score)\n",
        "metrics = ['Accuracy', 'Recall', 'Precision', 'F1 Score']\n",
        "\n",
        "# Create spider plots\n",
        "\n",
        "for metric in metrics:\n",
        "    # Prepare train and test data\n",
        "    if metric == 'Accuracy':\n",
        "        CRF_data = CRF_accuracy\n",
        "        transformer_data = transf_accuracy\n",
        "    elif metric == 'Recall':\n",
        "        CRF_data = CRF_recall\n",
        "        transformer_data = transf_recall\n",
        "    elif metric == 'Precision':\n",
        "        CRF_data = CRF_precision\n",
        "        transformer_data = transf_precision\n",
        "    elif metric == 'F1 Score':\n",
        "        CRF_data = CRF_f1_score\n",
        "        transformer_data = transf_f1_score\n",
        "\n",
        "    # Plot spider plot\n",
        "    angles = np.linspace(0, 2 * np.pi, len(datasets), endpoint=False)\n",
        "    plt.figure(figsize = (7,7))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    ax.plot(angles.tolist() + angles[:1].tolist(), CRF_data.tolist() + CRF_data[:1].tolist(), label='CRF')\n",
        "    ax.plot(angles.tolist() + angles[:1].tolist(), transformer_data.tolist() + transformer_data[:1].tolist(), label='Transformer')\n",
        "    ax.fill(angles.tolist() + angles[:1].tolist(), CRF_data.tolist() + CRF_data[:1].tolist(), alpha=0.25)\n",
        "    ax.fill(angles.tolist() + angles[:1].tolist(), transformer_data.tolist() + transformer_data[:1].tolist(), alpha=0.25)\n",
        "    ax.set_xticks(angles)\n",
        "    ax.set_xticklabels(datasets)\n",
        "    bt = min(CRF_data.tolist() + transformer_data.tolist())//10 * 10\n",
        "    ax.set_ylim(bottom = bt, top=100)\n",
        "    plt.title(metric)\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "# Show all the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j6Ozd-Iyw9SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained a transformer on a dataset composed of all the original datasets (due to computational constraints, it was not possible to test all the combinations as \n",
        "done for CFR), and we used it to classify all the test sets both separately and collectively. For the \n",
        "same reason explained in the CFR section, we calculated the balanced metrics.\n",
        "\n",
        "In the first two graphs, we can see the following: the results of the transformer trained on the dataset composed of the four original datasets and tested on the \n",
        "individual test sets, and the results of the transformers trained on the individual datasets (including the one containing all four) and tested on their respective test sets. \n",
        "We can observe that, in general, the models trained on a specific dataset perform better on their corresponding test set, while the transformer trained on all datasets simultaneously \n",
        "seems to have poorer results. However, it should be noted that the datasets differ considerably in size. Based on these analyses, we can conclude that, at the cost of training multiple \n",
        "models, it is more advantageous to use specific models for each dataset.\n",
        "\n",
        "In the last four graphs, we can observe a comparison of the results between CFR and transformer models. In this case, each axis represents a dataset on which the model was trained \n",
        "and tested. We can notice that CRFs achieve significantly better results in terms of accuracy, while transformers generally outperform in terms of precision, recall, and F1 score.  \n",
        "Based on the meanings of these metrics, we can reasonably conclude that the CRF model might have better overall classification performance in terms of correctly classifying instances, but the \n",
        "transformer model is more effective in capturing positive instances (higher recall) and avoiding false positives (higher precision), leading to a better balance between the two (higher F1 score)."
      ],
      "metadata": {
        "id": "dFs_2RtUuu5M"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "08G5PAYnAUOr",
        "jfZAu7CMRzP7",
        "WtEjHNHVAUOs",
        "0u32r8WVcKps",
        "AIj4qBulAUOt",
        "qdsB2Wg9393M",
        "Xny9ERYY8q6a",
        "sHpQ7aEQBsAk",
        "PfbNEkJZgDJe",
        "6Ls1yQMgQ_GL",
        "ObHE-HlrRST4",
        "nnJ0Rfx5AUO2",
        "ubxUwELuuXda",
        "_iW_yMemOo1l",
        "ax5JhSgR7lJ-",
        "3ck75C9xO1cF",
        "qZOf_7G_WKHD",
        "iTPlai-0O14n",
        "WCuMssv9yT-v",
        "Db99sXL081Lp",
        "tREvMQ9jFp21",
        "lpu2JiOvXznM",
        "es9ivtESX8fn",
        "65qmPdLtt7wX",
        "YI-JQxNxuQN8",
        "gh6dC2Rkwj6r"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}