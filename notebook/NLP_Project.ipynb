{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliaFeltrin/KIND_project/blob/main/notebook/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKwSQ3yjxA6I"
      },
      "source": [
        "# Name Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uSdgHpkxIGR"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08G5PAYnAUOr"
      },
      "source": [
        "### Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLQhCXCbxy-f"
      },
      "outputs": [],
      "source": [
        "# Installing the library needed in the following part of the project\n",
        "\n",
        "# Installing the utlity packages\n",
        "#! pip install scipy\n",
        "#! pip install numpy\n",
        "#! pip install pandas\n",
        "\n",
        "# Installing a natural language processing library\n",
        "! pip install nltk\n",
        "# Installing the packages for creating amazing plots\n",
        "#! pip install matplotlib\n",
        "#! pip install wordcloud\n",
        "! pip install plotly\n",
        "! pip install --upgrade nbformat\n",
        "\n",
        "#Â Installing a package for sequence labeling, used for POS tagging and NER\n",
        "! pip install -U spacy\n",
        "\n",
        "# Installing the packages for creating the word embeddings\n",
        "! pip install --upgrade gensim\n",
        "! pip install fasttext\n",
        "\n",
        "# Installing the packages for doing dimensionality reduction\n",
        "#! pip install sklearn\n",
        "! pip install umap-learn\n",
        "! pip install python-terrier\n",
        "\n",
        "# Installing packages for transformers\n",
        "! pip install transformers==4.28.0\n",
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PbNJyzNAUOs"
      },
      "outputs": [],
      "source": [
        "# Dowloading an italian model from spacy\n",
        "! spacy download it_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtEjHNHVAUOs"
      },
      "source": [
        "### Package import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/EliaFeltrin/KIND_project"
      ],
      "metadata": {
        "id": "kiR2_ghKCpKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXMMltVMVFQO"
      },
      "outputs": [],
      "source": [
        "# Importing the main packages\n",
        "\n",
        "# Importing the utlity packages\n",
        "import string\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Importing a natural language processing library\n",
        "import nltk\n",
        "\n",
        "# Importing the packages for creating amazing plots\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "\n",
        "# Importing the packages for creating the word embeddings\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "# Importing the packages for doing dimensionality reduction\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "# Importing a package for the tf-idf representation of the sentences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Importing a package for clustering\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "\n",
        "# Importing the packages for POS tagging\n",
        "import spacy as spc\n",
        "\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XLYGT2fNAUOt"
      },
      "outputs": [],
      "source": [
        "# Defining the names of the datasets\n",
        "dataset_names = ['degasperi_train.tsv', 'degasperi_test.tsv', 'fiction_train.tsv',\\\n",
        "            'fiction_test.tsv', 'moro_train.tsv', 'moro_test.tsv',\\\n",
        "            'wikinews_train.tsv', 'wikinews_test.tsv']\n",
        "          \n",
        "dataset_names_train_BIO =\n",
        " ['degasperi_train_BIO.tsv', 'fiction_train_BIO.tsv','moro_train_BIO.tsv', 'wikinews_train_BIO.tsv']\n",
        "dataset_names_test_BIO = ['degasperi_test_BIO.tsv', 'fiction_test_BIO.tsv','moro_test_BIO.tsv', 'wikinews_test_BIO.tsv']\n",
        "# Defining the path to datasets\n",
        "#PATH_TO_DATASETS = '../datasets/Inside_outside_NER_notation'\n",
        "PATH_TO_DATASETS = '/content/KIND_project/datasets/BIO_tag_NER_notation'\n",
        "# Importing all the datasets in a dictionary\n",
        "#datasets_dict = {name: pd.read_csv(PATH_TO_DATASETS+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names}\n",
        "\n",
        "datasets_train_dict_BIO = {name: pd.read_csv(PATH_TO_DATASETS+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names_train_BIO}\n",
        "datasets_test_dict_BIO = {name: pd.read_csv(PATH_TO_DATASETS+'/'+name, sep='[\\t|\\n]', names=['Token', 'Entity'], engine='python') for name in dataset_names_test_BIO}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGkcqAEwVDL9"
      },
      "outputs": [],
      "source": [
        "# Defining the path to datasets\n",
        "PATH_TO_DATASETS = '../datasets/Inside_outside_NER_notation'\n",
        "\n",
        "# Loading the datasets\n",
        "dataset_degasperi = pd.read_csv(PATH_TO_DATASETS+'/degasperi_train.tsv', sep='\\t', header=None)\n",
        "dataset_degasperi = dataset_degasperi.rename(columns={0: 'Token', 1: 'Entity'})\n",
        "\n",
        "dataset_moro = pd.read_csv(PATH_TO_DATASETS+'/moro_train.tsv', sep='\\t', header=None)\n",
        "dataset_moro = dataset_moro.rename(columns={0: 'Token', 1: 'Entity'})\n",
        "\n",
        "dataset_fiction = pd.read_csv(PATH_TO_DATASETS+'/fiction_train.tsv', sep='\\t', header=None)\n",
        "dataset_fiction = dataset_fiction.rename(columns={0: 'Token', 1: 'Entity'})\n",
        "\n",
        "dataset_wikinews = pd.read_csv(PATH_TO_DATASETS+'/wikinews_train.tsv', sep='\\t', header=None)\n",
        "dataset_wikinews = dataset_wikinews.rename(columns={0: 'Token', 1: 'Entity'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GCIlq8UAUOt"
      },
      "outputs": [],
      "source": [
        "a=len(dataset_degasperi.where(dataset_degasperi['Token'].str.contains('\\n')))\n",
        "\n",
        "b=len(dataset_degasperi)\n",
        "print(str(a))\n",
        "print(str(b))\n",
        "\n",
        "get_string_from_df()\n",
        "dataset_degasperi['Token'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for dealing with the datasets"
      ],
      "metadata": {
        "collapsed": false,
        "id": "AIj4qBulAUOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def get_string_from_df(dataframe, puntuaction):\n",
        "  '''\n",
        "  Transforms the tokenized dataset into a single string.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataframe: DataFrame\n",
        "    structure containing the tokenized dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  text: str\n",
        "    string concatenating all the tokens of the dataset\n",
        "  '''\n",
        "  text_df = dataframe.loc[:,'Token']\n",
        "  text = text_df[0]\n",
        "  for token in text_df[1:]:\n",
        "    text += (' ' + token) if token not in puntuaction else token\n",
        "  return text"
      ],
      "metadata": {
        "id": "dbJQ8lSLAUOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "# Defining some funtion useful for having the correct structure of the dataset in order to define the word embeddings\n",
        "def get_sentences_list_from_df(dataset_df, key=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_df: DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sentences_list: list\n",
        "\n",
        "    '''\n",
        "    punctuation = string.punctuation\n",
        "    sentences_list = [[]]\n",
        "    labels_list = [[]]\n",
        "    keys_list = None\n",
        "    count = 0\n",
        "\n",
        "    for element in dataset_df.iterrows():\n",
        "        if str(element[1]['Token']) == '.':\n",
        "            sentences_list.append([])\n",
        "            labels_list.append([])\n",
        "            count += 1\n",
        "        elif str(element[1]['Token']) not in punctuation:\n",
        "            sentences_list[count].append(element[1]['Token'].lower())\n",
        "            labels_list[count].append(element[1]['Entity'])\n",
        "    if key != None:\n",
        "        keys_list = [key for sentence in range(len(sentences_list))]\n",
        "    return sentences_list, labels_list, keys_list\n",
        "\n",
        "def remove_short_sentences(sentences_list, labels_list, keys_list = None, min_length=3):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentences_list: list\n",
        "\n",
        "    min_lenght: int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sentences_list: list\n",
        "\n",
        "    '''\n",
        "    new_sentences_list = list()\n",
        "    new_labels_list = list()\n",
        "    new_keys_list = list()\n",
        "    for idx in range(len(sentences_list)):\n",
        "        if len(sentences_list[idx]) >= min_length:\n",
        "            new_sentences_list.append(sentences_list[idx])\n",
        "            new_labels_list.append(labels_list[idx])\n",
        "            if keys_list != None:\n",
        "                new_keys_list.append(keys_list[idx])\n",
        "    return new_sentences_list, new_labels_list, new_keys_list\n",
        "\n",
        "def get_all_sentences_from_datasets(datasets):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets: dict\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    overall_sentences: list\n",
        "    overall_labels: list\n",
        "    overall_keys: list\n",
        "\n",
        "    '''\n",
        "    overall_sentences = list()\n",
        "    overall_labels = list()\n",
        "    overall_keys = list()\n",
        "    for key in datasets.keys():\n",
        "        sentences, labels, keys = get_sentences_list_from_df(datasets[key], key=key)\n",
        "        sentences, labels, keys = remove_short_sentences(sentences, labels, keys_list=keys)\n",
        "        overall_sentences += sentences\n",
        "        overall_labels += labels\n",
        "        overall_keys += keys\n",
        "    return overall_sentences, overall_labels, overall_keys"
      ],
      "metadata": {
        "id": "UAUrIMICAUOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Defining some function useful for having the correct structure of the dataset in order to define the tf-idf representation\n",
        "def concatenate_sentences_tokens(sentences):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentences: list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    concatenated_sentences: list\n",
        "\n",
        "    '''\n",
        "    concatenated_sentences = list()\n",
        "    for sentence in sentences:\n",
        "        new_sentence = ''\n",
        "        for token in sentence:\n",
        "            new_sentence += (token + ' ')\n",
        "        concatenated_sentences.append(new_sentence)\n",
        "    return concatenated_sentences"
      ],
      "metadata": {
        "id": "CnEtUn5rAUOu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PMsuTVlfsNw"
      },
      "source": [
        "## Word embeddings representation\n",
        "\n",
        "A word embedding is the vectorial representation of a word. It is used for achieving a dense representation of the words in an high dimensional space. Other types of representation of the words, such as the bag of words representation, are sparse compared to word embeddings.\n",
        "The word embeddings allow to achieve better results in many fields of natural language processing.\n",
        "\n",
        "In particular we use Word2Vec in order to obtain the word embeddings of out dataset.\n",
        "Basic Word2Vec is a artificial neural network composed by two layers\n",
        "\n",
        "The inputs of the neural network are the words of the sentence to convert in the word embeddings representation.\n",
        "The first layer is a linear layer.\n",
        "The linear activation functions values are summed and put as outputs.\n",
        "At the end we appply a softmax layer.\n",
        "We want the model to prefict the next word in the sentence.\n",
        "I train the NN using the cross entropy as loss function.\n",
        "At the end of the training the weights connecting the inputs to the first hidden layer are the values of the dimensions of the word embedding.\n",
        "\n",
        "\n",
        "Two of the most used architecture of the Word2Vec are CBOW and Skip-Gram.\n",
        "\n",
        "The Continuous Bag of Words method uses many words surrounding the word I want to use in the prediction in the training step.\n",
        "The Skip-gram uses a word to predict the word in the surroundings.\n",
        "\n",
        "In some way the distribution in the various dimension is based on the similarity of the words in terms of semantics and usage.\n",
        "\n",
        "With word embeddings we can embed the context of the word inside its representation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqz9yOMGAUOv"
      },
      "source": [
        "We start from the dataframe containing the tokens of the dataset.\n",
        "The pipeline used for the definition of the input of the word2vec model is the following:\n",
        "- (previous) the tokenization has already been done\n",
        "- merging the single elements in sentences splitting on the single dots\n",
        "- lowercasing all the tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word embeddings trained on all the data\n",
        "\n",
        "We choose to try the definition of the word embeddings model on different portions of the dataset. Firstly we try to define the word embeddings on the entire data we have since the larger is the dataset the better will be the our model and the wider will be the dictionary."
      ],
      "metadata": {
        "collapsed": false,
        "id": "ijraczejAUOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UOkb_6GAUOv"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences in the correct format to create the embeddings representation of the words\n",
        "sentences, labels, sentences_keys = get_all_sentences_from_datasets(datasets_dict)\n",
        "# Printing 10 lists of token\n",
        "for i in range(10):\n",
        "    print(str(sentences[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ks3ebJKAUOv"
      },
      "outputs": [],
      "source": [
        "# To check that there are no tabs or new lines inside the tokens\n",
        "found = False\n",
        "for name in dataset_names:\n",
        "    for i, el in datasets_dict[name].iterrows():\n",
        "        if '\\t' in el['Token'] or '\\n' in el['Token']:\n",
        "            found = True\n",
        "            print(el['Token'])\n",
        "if found:\n",
        "    print('There is something wrong, there tab or new line characters, are check the import of the dataset :(')\n",
        "else:\n",
        "    print('No tab or new line characters found, Great job!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwWF6C3sAUOv"
      },
      "outputs": [],
      "source": [
        "# Printing some information about the list of lists of token\n",
        "print('The total number of sentences in the dataset is ' + str(len(sentences)))\n",
        "length_list= []\n",
        "for idx,i in enumerate(sentences):\n",
        "    length_list.append(len(i))\n",
        "print('The maximum lenght of a sentence is ' + str(max(length_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hli5YYiaAUOv"
      },
      "outputs": [],
      "source": [
        "# Definition of the Word2Vec model\n",
        "embeddings_model = Word2Vec(sentences, vector_size=30, min_count=2, window=20)\n",
        "# Printing the length of the vocabulary\n",
        "len(embeddings_model.wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT1DIssKAUOv"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "term = 'governo'\n",
        "embeddings_model.wv.most_similar(term.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiLRR5MzAUOw"
      },
      "outputs": [],
      "source": [
        "# Computing the dimensionality reduction of the word embeddings space\n",
        "word_samples = random.sample(list(embeddings_model.wv.key_to_index), 500)\n",
        "word_vectors = embeddings_model.wv[word_samples]\n",
        "\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embeddings = tsne.fit_transform(word_vectors)\n",
        "x, y, z = np.transpose(tsne_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHcBd5-zAUOw"
      },
      "outputs": [],
      "source": [
        "# Plotting the word embeddings of the model\n",
        "fig = px.scatter_3d(x=x, y=y, z=z, text=word_samples)\n",
        "fig.update_traces(marker=dict(size=3, line=dict(width=2)), textfont_size=6)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8At4tFwAUOw"
      },
      "source": [
        "Now it is analyzed the word embeddings trained only on the training datasets that could be useful in the next part of the project since they can be used for performing name-entity recognition. Then it is also computed and analyzed the word embeddings trained on the datasets separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wArKi3xiAUOw"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences in the correct format to create the embeddings representation of the words\n",
        "dataset_train_names = [key for key in dataset_names if 'train' in key]\n",
        "sentences_train, labels_train = get_all_sentences_from_datasets({name:datasets_dict[name] for name in dataset_train_names})\n",
        "# Definition of the Word2Vec model\n",
        "embeddings_model_train = Word2Vec(sentences_train, vector_size=30, min_count=2, window=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9PDHWg7AUOw"
      },
      "outputs": [],
      "source": [
        "# Printing the length of the vocabulary\n",
        "len(embeddings_model_train.wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaUHxOhOAUOw"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "term = 'governo'\n",
        "embeddings_model_train.wv.most_similar(term.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWYjNVIYAUOw"
      },
      "outputs": [],
      "source": [
        "# Computing the dimensionality reduction of the word embeddings space\n",
        "word_samples_train = random.sample(list(embeddings_model_train.wv.key_to_index), 500)\n",
        "word_vectors_train = embeddings_model_train.wv[word_samples_train]\n",
        "\n",
        "tsne = TSNE(n_components=3, n_iter=2000)\n",
        "tsne_embeddings = tsne.fit_transform(word_vectors_train)\n",
        "x_train, y_train, z_train = np.transpose(tsne_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcQAoHSnAUOx"
      },
      "outputs": [],
      "source": [
        "# Plotting the word embeddings of the model\n",
        "fig = px.scatter_3d(x=x_train, y=y_train, z=z_train, text=word_samples_train)\n",
        "fig.update_traces(marker=dict(size=3, line=dict(width=2)), textfont_size=6)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcJ9uK3yAUOx"
      },
      "source": [
        "The dictionary is a bit smaller but still big and the results are quite good. Train set and test set are usually separate and so I could create the embeddings model for the train set first and then model inside it the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q7Qqy1wAUOx"
      },
      "outputs": [],
      "source": [
        "# Getting the sentences in the correct format to create the embeddings representation of the words\n",
        "dataset_train_names = [key for key in dataset_names if 'train' in key]\n",
        "sentences_train_separate = {}\n",
        "labels_train_separate = {}\n",
        "embeddings_model_train_separate = {}\n",
        "for name in dataset_train_names:\n",
        "    sentences_train_sep, labels_train_sep = get_all_sentences_from_datasets({name:datasets_dict[name]})\n",
        "    sentences_train_separate[name] = sentences_train_sep\n",
        "    labels_train_separate[name] = labels_train_sep\n",
        "\n",
        "    # Definition of the Word2Vec model\n",
        "    embeddings_model_train_separate[name] = Word2Vec(sentences_train_sep, vector_size=30, min_count=2, window=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyBDbF5LAUOx"
      },
      "outputs": [],
      "source": [
        "# Printing the length of the vocabularies of the various datasets\n",
        "for name in dataset_train_names:\n",
        "    print('The dictionary of the dataset ' + name + ' is long ' + str(len(embeddings_model_train_separate[name].wv)) + ' words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "monjRwjuAUOx"
      },
      "outputs": [],
      "source": [
        "# Searching the most similar word to a specific word\n",
        "for name in dataset_train_names:\n",
        "    term = 'governo'\n",
        "    print(name + ':' + str(embeddings_model_train_separate[name].wv.most_similar(term.lower())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9W3_o-AUOx"
      },
      "source": [
        "It is easy to understand that the dictionaries in this case are smaller since the overall words are splitted in many models.\n",
        "Trying to search the most similar embeddings to a given word in many different fields and for the different datasets we can see that better performace are achieved by the datasets that are specialized in that field, e.g. fiction_train finds worst results (less related word embeddings) for the word 'governo' than the other datasets, which deal with news and politics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKHUhOncAUOx"
      },
      "outputs": [],
      "source": [
        "# Computing the dimensionality reduction of the word embeddings space\n",
        "#word_samples_train = embeddings_model_train.wv.key_to_index\n",
        "#word_vectors_train = embeddings_model_train.wv[list(embeddings_model_train.wv.key_to_index)]\n",
        "\n",
        "#tsne = TSNE(n_components=3, n_iter=1000)\n",
        "#tsne_embeddings = tsne.fit_transform(word_vectors_train)\n",
        "#x_train, y_train, z_train = np.transpose(tsne_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpI-ond6AUOx"
      },
      "outputs": [],
      "source": [
        "# Plotting the word embeddings of the model\n",
        "#fig = px.scatter_3d(x=x_train, y=y_train, z=z_train, text=word_samples_train)\n",
        "#fig.update_traces(marker=dict(size=3, line=dict(width=2)), textfont_size=6)\n",
        "#fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGZqBwgiAUOy"
      },
      "outputs": [],
      "source": [
        "# TO DOOOOOOO\n",
        "\n",
        "#fig = plt.figure(figsize=(15, 10))\n",
        "#ax = fig.add_subplot(111, projection='3d')\n",
        "#ax.scatter(x, y, z, c=LabelEncoder().fit_transform(labels), marker='.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6FpGcvnAUOy"
      },
      "source": [
        "## Computing the embeddings using a pretrained model FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPReFIjnAUOy"
      },
      "outputs": [],
      "source": [
        "#!wget http://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "#!gzip -d cc.en.300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyIeWHLxAUOy"
      },
      "outputs": [],
      "source": [
        "#ft = fasttext.load_model('cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_C3wIdLAUOy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgn1LECxAUOy"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "The clustering is an unsupervided machine learning technique useful for inspecting not labeled data in order to find inside them hidden patterns or other information.\n",
        "In our case clustering is not needed for performing name-entity recognition but it is useful for gain some additional knowledge about the dataset before proceeding in the sequence labeling task.\n",
        "\n",
        "Maybe we could improve in some way the NER applying it to some specific cluster? bohhh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since it is not possible to identify the various documents we decide to apply the clustering techniqes on the sentences. We assume that the sentences were separate by the dot and we extract in this way them from the input files.\n",
        "We can investigate whether the entities creates some cluster or not.\n",
        "We can also investigate whether the four dataset are distinguishable and so in some way they have different characteristics.\n",
        "\n",
        "We decide to try apply the clustering on the datasets and see whether the classification gives us good performances."
      ],
      "metadata": {
        "collapsed": false,
        "id": "BbrC9zJ_AUOy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUMklbsdAUOy"
      },
      "outputs": [],
      "source": [
        "# Importing the italian stopwords (taken from https://github.com/stopwords-iso/stopwords-it.git)\n",
        "with open('stopwords-it.txt', 'r') as f:\n",
        "    italian_stopwords = f.read()\n",
        "italian_stopwords_github = italian_stopwords.split('\\n')\n",
        "len(italian_stopwords_github)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoyJuKM9AUOz"
      },
      "outputs": [],
      "source": [
        "# Downloading and importing the italian stopwords in the package nltk\n",
        "nltk.download('stopwords')\n",
        "italian_stopwords_nltk = nltk.corpus.stopwords.words('italian')\n",
        "len(italian_stopwords_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# All the stopwords od nltk are in the other dataset, better result with more stopwords\n",
        "count = 0\n",
        "for i in italian_stopwords_nltk:\n",
        "    if i in italian_stopwords_github:\n",
        "        count += 1\n",
        "print(count)\n",
        "italian_stopwords = italian_stopwords_github"
      ],
      "metadata": {
        "id": "HlcGWK72AUOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Defining the vectorizer model\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=3, stop_words=italian_stopwords_github, use_idf=True)\n",
        "# Fitting the vectorizer model\n",
        "vectorizer.fit(concatenate_sentences_tokens(sentences))\n",
        "\n",
        "# Given that we are evaluating sentences the frequncy can be lower for saying that we have a stopword"
      ],
      "metadata": {
        "id": "bWG5z2noAUOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Extracting the vocabulary\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "print('The vocabulary is long ' + str(len(vocabulary)) + ' words')"
      ],
      "metadata": {
        "id": "2Bh3XX3aAUOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# The words look reasonable and the vocabulary seems to not have many stopwords inside\n",
        "sorted(random.sample(vocabulary.tolist(),100))"
      ],
      "metadata": {
        "id": "jdsVliRNAUOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Converting the strings into vectors\n",
        "sentences_vector = vectorizer.transform(concatenate_sentences_tokens(sentences))"
      ],
      "metadata": {
        "id": "52kRljRQAUOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "sentences_vector[0].multiply(sentences_vector[0]).sum()"
      ],
      "metadata": {
        "id": "023nFyxlAUOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "max_score = 0\n",
        "sentence_idx = 1000\n",
        "for i in range(sentences_vector.shape[0]):\n",
        "    if i != sentence_idx:\n",
        "        score = sentences_vector[sentence_idx].multiply(sentences_vector[i]).sum()\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            max_score_idx = i\n",
        "print('Sentence index ' + str(sentence_idx) + ': ' + str(sentences[sentence_idx]))\n",
        "print('Most similar sentence is the one with index ' + str(max_score_idx) + ': ' + str(sentences[max_score_idx]))\n",
        "print('The score is ' + str(max_score))"
      ],
      "metadata": {
        "id": "0_NeyGPrAUOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentences obtain not so good metrics of similarity, the retrieved sentences have some similarities, same words, but they are not very similar in the semantics or in the field of application of the sentence."
      ],
      "metadata": {
        "collapsed": false,
        "id": "JykECfayAUO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we try to cluster the sentences in the 4 datsets, so we expect four clasters, we use the K-means clustering method."
      ],
      "metadata": {
        "collapsed": false,
        "id": "CNXFHEy7AUO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Defining the labels of the clustering\n",
        "keys_datasets = list()\n",
        "for key in sentences_keys:\n",
        "    keys_datasets.append(key.replace('_train.tsv', '').replace('_test.tsv', ''))\n",
        "# Applying the K-means clustering\n",
        "num_clusters= len(set(keys_datasets))\n",
        "kmeans_model = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=2, verbose=True, random_state=2307)\n",
        "kmeans_model.fit(sentences_vector)"
      ],
      "metadata": {
        "id": "Hnzfig-tAUO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "for i in range(num_clusters):\n",
        "    centroid = kmeans_model.cluster_centers_[i]\n",
        "    sorted_terms = centroid.argsort()[::-1]\n",
        "    print('Centroid of cluster ' + str(i))\n",
        "    print([vocabulary[j] for j in sorted_terms[:20]])"
      ],
      "metadata": {
        "id": "uRfuh3a3AUO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF9qbE8MAUO0"
      },
      "outputs": [],
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i in range(kmeans_model.n_clusters):\n",
        "    centroid = kmeans_model.cluster_centers_[i]\n",
        "    sorted_terms = centroid.argsort()[::-1]\n",
        "    print(f\"Cluster {i}:\\t{[vocabulary[j] for j in sorted_terms[:10]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Number of sentences in: ')\n",
        "\n",
        "for i in range(kmeans_model.n_clusters):\n",
        "    print(f\"Cluster {i}: {np.sum(kmeans_model.labels_ == i)}\")"
      ],
      "metadata": {
        "id": "UFnTQM3vAUO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"Intrinsic evaluation measures:\")\n",
        "print(\"Within-cluster sum-of-squares:\", str(kmeans_model.inertia_))\n",
        "print(\"Silhouette coefficient:\", str(metrics.silhouette_score(sentences_vector, kmeans_model.labels_)))"
      ],
      "metadata": {
        "id": "CcGDnhv4AUO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very bad results :("
      ],
      "metadata": {
        "collapsed": false,
        "id": "NJAOdYITAUO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Extrinsic evaluation measures:')\n",
        "print(\"Homogeneity:\", str(metrics.homogeneity_score(keys_datasets, kmeans_model.labels_)))\n",
        "print(\"Completeness:\", str(metrics.completeness_score(keys_datasets, kmeans_model.labels_)))\n",
        "print(\"V-measure:\", str(metrics.v_measure_score(keys_datasets, kmeans_model.labels_)))\n",
        "print(\"Adjusted Rand-Index:\", str(metrics.adjusted_rand_score(keys_datasets, kmeans_model.labels_)))"
      ],
      "metadata": {
        "id": "DvvsSvHxAUO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "02-GHS6nAUO1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfbNEkJZgDJe"
      },
      "source": [
        "## POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL0AA1eVdT3o"
      },
      "outputs": [],
      "source": [
        "# Loading the italian model\n",
        "nlp_model = it_core_news_sm.load()\n",
        "\n",
        "text_degasperi = ' '.join(dataset_degasperi.iloc[:, 0].tolist())\n",
        "parsed_text_degasperi = nlp_model(text_degasperi)\n",
        "\n",
        "text_fiction = ' '.join(dataset_fiction.iloc[:, 0].tolist())\n",
        "parsed_text_fiction = nlp_model(text_fiction)\n",
        "\n",
        "text_wikinews = ' '.join(dataset_wikinews.iloc[:, 0].tolist())\n",
        "parsed_text_wiki = nlp_model(text_wikinews[:1000000])\n",
        "\n",
        "text_moro = ' '.join(dataset_moro.iloc[:, 0].tolist())\n",
        "parsed_text_moro = nlp_model(text_moro[:1000000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPRbor2ATNqR"
      },
      "outputs": [],
      "source": [
        "def merge_counters(counter1, counter2):\n",
        "\n",
        "  '''\n",
        "  Takes 2 counters with different shapes and in the smallest one ad also the key that are currently inside with a value of 0\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  counter1: Counter\n",
        "    Counter with smallest shape that has to be incremented\n",
        "  counter2: Counter\n",
        "    Counter with the larger shape\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  new_counter: Counter\n",
        "    Counter containing all the tuple of the smallest one and the tuples (key, 0) of the larger one that were no present in the small one\n",
        "  '''\n",
        "  new_counter = counter1    \n",
        "\n",
        "  for key, value in counter2.items():\n",
        "    if key not in new_counter.keys():\n",
        "      new_counter[key] = 0 \n",
        "\n",
        "  return new_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzmUyHYDw-eR"
      },
      "outputs": [],
      "source": [
        "def plot_wordCloud_counters(counters):\n",
        "\n",
        "  '''\n",
        "  Takes as input a list of counters and it plots the wordCloud\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  cunters: list(Counter)\n",
        "    List of counters that has to be plotted. It does not require that all the counters has the same shape\n",
        "\n",
        "  '''\n",
        "\n",
        "  word_cloud_counter = Counter()\n",
        "  for counter in list_counters:\n",
        "    word_cloud_counter.update(counter)\n",
        "  # Generate a word cloud from the POS counts\n",
        "  wordcloud = WordCloud(background_color='white').generate_from_frequencies(word_cloud_counter)\n",
        "\n",
        "  # Plot the word cloud\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oFiL_p2unFX"
      },
      "outputs": [],
      "source": [
        "def plot_groupedBar_counters(counters):\n",
        "\n",
        "  '''\n",
        "  Takes as input a list of counters and it plots in the same bar chart the counts\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  cunters: list(Counter)\n",
        "    List of counters that has to be plotted. It does not require that all the counters has the same shape\n",
        "\n",
        "  '''\n",
        "  \n",
        "  max_length = max(map(len, list_counters))                         # max length of the counters\n",
        "  max_position = list(map(len, list_counters)).index(max_length)    # position in the list of the counter with max length\n",
        "\n",
        "  # For each counter that is not the one of maximum dimension I merge it with all the other ones. The result is a list with counters having all the same keys\n",
        "  for i in range(len(list_counters)):\n",
        "    if i != max_position:\n",
        "      list_counters[i] = merge_counters(list_counters[i], list_counters[max_position])\n",
        "\n",
        "  # We plot each counter inside the bar chart\n",
        "  x = np.arange(max_length)\n",
        "  width=0.2\n",
        "  multiplier = 0\n",
        "  for counter in list_counters:\n",
        "    offset = width * multiplier\n",
        "    labels, values = zip(*sorted(counter.items()))\n",
        "    plt.bar(x + offset, values, width=width)\n",
        "    multiplier += 1\n",
        "\n",
        "  plt.title(\"POS Tag Frequency Distribution Degasperi\")\n",
        "  plt.xlabel(\"POS Tag\")\n",
        "  plt.ylabel(\"Frequency\")\n",
        "  plt.xticks(x + width, sorted(list_counters[max_position]), rotation='vertical')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lIMU-RRiMsf"
      },
      "outputs": [],
      "source": [
        "# Get the frequency distribution of POS tags\n",
        "pos_freq_degasperi = Counter([token.pos_ for token in parsed_text_degasperi])\n",
        "pos_freq_fiction = Counter([token.pos_ for token in parsed_text_fiction])\n",
        "pos_freq_wiki = Counter([token.pos_ for token in parsed_text_wiki])\n",
        "pos_freq_moro = Counter([token.pos_ for token in parsed_text_moro])\n",
        "\n",
        "list_counters = [pos_freq_degasperi, pos_freq_fiction, pos_freq_wiki, pos_freq_moro]\n",
        "plot_groupedBar_counters(list_counters)\n",
        "plot_wordCloud_counters(list_counters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnJ0Rfx5AUO2"
      },
      "source": [
        "# Indexing\n",
        "\n",
        "Indexing is the process by which search engines organize information before a search to enable super-fast responses to queries. \n",
        "Searching through individual pages for keywords and topics would be a very slow process for search engines to identify relevant information. Instead, search engines use an inverted index, also known as a reverse index.\n",
        "An inverted index is a system wherein a database of text elements is compiled along with pointers to the documents which contain those elements\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, since we don't have a dataset of documents we decided to index the sentences inside our datasets.\n",
        "To do that we have to create the sentences from the dictionary of datasets."
      ],
      "metadata": {
        "id": "dYtA4wp9LXrN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjbbdpR0AUO2"
      },
      "outputs": [],
      "source": [
        "sentences, labels, keys = get_all_sentences_from_datasets(datasets_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we create the dataset of sentences adding also the Id to indetify them while the query part."
      ],
      "metadata": {
        "id": "FOedSyMGMnVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIJ856g5AUO2"
      },
      "outputs": [],
      "source": [
        "sentences_df = pd.DataFrame(columns=['docno', 'text'])\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences_df.loc[i] = [f's{i}', ' '.join(sentences[i])]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we index the sentences dataframe. The index, with all its data structures, is written into a directory that we will call `index`.\n",
        "`Index_Ref` provides the location where the index is stored."
      ],
      "metadata": {
        "id": "zimvJV0gM3Kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeAsq2sHAUO5"
      },
      "outputs": [],
      "source": [
        "index_path = \"./index\"\n",
        "\n",
        "indexer = pt.DFIndexer(index_path, overwrite=True)\n",
        "index_ref = indexer.index(sentences_df['text'], sentences_df['docno'])\n",
        "index_ref.toString()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can now load the index and print it.\n",
        "This is a Terrier index structure, which provides methods such as:\n",
        " - `getCollectionStatistics()`\n",
        " - `getInvertedIndex()`\n",
        " - `getLexicon()`\n",
        "\n",
        " Let's see what is returned by the `CollectionStatistics()` method."
      ],
      "metadata": {
        "id": "BBT349HRNRmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTfdsLQ8AUO5"
      },
      "outputs": [],
      "source": [
        "index = pt.IndexFactory.of(index_ref)\n",
        "print(index.getCollectionStatistics().toString())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have indexed our documents, we can run a search over the document collection.\n",
        "Here we used the TF-IDF weighting formula to rank the results. \n",
        "\n",
        "The `search()` method returns a dataframe with columns:\n",
        " - `qid`: the query identifier\n",
        " - `docid`: integer identifier for document \n",
        " - `docno`: string identifier for document\n",
        " - `rank`: rank position\n",
        " - `score`: tf-idf score\n",
        " - `query`: the input query"
      ],
      "metadata": {
        "id": "AkadPwj6O0Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "br = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "\n",
        "queries = pd.DataFrame([[\"query1\", \"festa\"], [\"query2\", \"ammiraglio\"], [\"query3\", \"messaggio audio\"]], columns=[\"qid\", \"query\"])\n",
        "br(queries)"
      ],
      "metadata": {
        "id": "sjvmRsOdG4QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER Transformers\n"
      ],
      "metadata": {
        "id": "rvMVjgT4flfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from datasets import load_metric\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification"
      ],
      "metadata": {
        "id": "xc41KXAqiv-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_merged = pd.concat(datasets_train_dict_BIO.values(), ignore_index=True)\n",
        "entity_names = pd.unique(datasets_merged['Entity'])\n",
        "entity_names_dict = {}\n",
        "for i, label in enumerate(entity_names):\n",
        "  entity_names_dict[label] = i\n",
        "\n",
        "sentences, labels, keys = get_sentences_list_from_df(datasets_merged)\n",
        "train_dataset = pd.DataFrame(columns=['Tokens', 'Labels'])\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  train_dataset.at[i, 'Tokens'] = sentences[i]\n",
        "  train_dataset.at[i, 'Labels'] = labels[i]\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_dataset)"
      ],
      "metadata": {
        "id": "kb5P1UfzhXux"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'dbmdz/bert-base-italian-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "s4iMtx2ci2Qu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    label_all_tokens = True\n",
        "    tokenized_inputs = tokenizer(list(examples[\"Tokens\"]), truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['Labels']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif label[word_idx] == '0':\n",
        "                label_ids.append(0)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(entity_names_dict[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(entity_names_dict[label[word_idx]] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "        \n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "02c39e51caef4e5d91607f6920fe5674",
            "2aa9815521414b8289569ccf50bd6e61",
            "4961b491aa5444fd933d0068caa662ee",
            "1b5dfed60a1d4a8697259d5384b9ee6f",
            "6f2c57c80e734304b4b5d6cc05e97340",
            "454d623a66784bbf95c3a90b765ea073",
            "bbd7fece75ff4aeca1f94fe026f89a30",
            "161a1970929947f8b40ab45e81eb9d3b",
            "5b1595a597054ce38909ed3020b62094",
            "7165a3c3270e4dfdb9e82cccc9e5c90a",
            "f621770e7ab342f784a02de4588b519c"
          ]
        },
        "id": "C4kQLn48jh1z",
        "outputId": "74d6c1a2-90c6-4523-b2cb-1802cbd3c1ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/27538 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02c39e51caef4e5d91607f6920fe5674"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(entity_names))\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='ner',\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "alQ8OBv7mumU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoDrRGFIgFpZ"
      },
      "source": [
        "# CODICE ELIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUVSRpgmEVNV"
      },
      "source": [
        "# KIND DATASET\n",
        "\n",
        "## Brief Description\n",
        "KIND (Kessler Italian Named-entities Dataset) is a dataset released in 2022 by researchers from Fondazione Bruno Kessler and the University of Trento. It contains 1 million tokens, of which 600K name-entities are manually annotated. The entities belong to 3 classes (people, location, organization). The texts come from various sources of the Italian language, such as news articles, literature, and political speeches, making it a multi-domain dataset. \n",
        "The following table shows the dataset composition:\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th rowspan=\"2\">Dataset</th>\n",
        "      <th rowspan=\"2\">Documents</th>\n",
        "      <th rowspan=\"2\">Tokens</th>\n",
        "      <th colspan=\"4\">Train</th>\n",
        "      <th colspan=\"4\">Test</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Total</th>\n",
        "      <th>PER</th>\n",
        "      <th>ORG</th>\n",
        "      <th>LOC</th>\n",
        "      <th>Total</th>\n",
        "      <th>PER</th>\n",
        "      <th>ORG</th>\n",
        "      <th>LOC</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Wikinews</td>\n",
        "      <td>1,000</td>\n",
        "      <td>308,622</td>\n",
        "      <td>247,528</td>\n",
        "      <td>8,928</td>\n",
        "      <td>7,593</td>\n",
        "      <td>6,862</td>\n",
        "      <td>61,094</td>\n",
        "      <td>1,802</td>\n",
        "      <td>1,823</td>\n",
        "      <td>1,711</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Fiction</td>\n",
        "      <td>86</td>\n",
        "      <td>192,448</td>\n",
        "      <td>170,942</td>\n",
        "      <td>3,439</td>\n",
        "      <td>182</td>\n",
        "      <td>733</td>\n",
        "      <td>21,506</td>\n",
        "      <td>636</td>\n",
        "      <td>284</td>\n",
        "      <td>463</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Aldo Moro</td>\n",
        "      <td>250</td>\n",
        "      <td>392,604</td>\n",
        "      <td>309,798</td>\n",
        "      <td>1,459</td>\n",
        "      <td>4,842</td>\n",
        "      <td>2,024</td>\n",
        "      <td>82,806</td>\n",
        "      <td>282</td>\n",
        "      <td>934</td>\n",
        "      <td>807</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Alcide De Gasperi</td>\n",
        "      <td>158</td>\n",
        "      <td>150,632</td>\n",
        "      <td>117,997</td>\n",
        "      <td>1,129</td>\n",
        "      <td>2,396</td>\n",
        "      <td>1,046</td>\n",
        "      <td>32,635</td>\n",
        "      <td>253</td>\n",
        "      <td>533</td>\n",
        "      <td>274</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td><strong>Total</strong></td>\n",
        "      <td><strong>1494</strong></td>\n",
        "      <td><strong>1,044,306</strong></td>\n",
        "      <td><strong>846,265</strong></td>\n",
        "      <td><strong>14,955</strong></td>\n",
        "      <td><strong>15,013</strong></td>\n",
        "      <td><strong>10,665</strong></td>\n",
        "      <td><strong>198,041</strong></td>\n",
        "      <td><strong>2,973</strong></td>\n",
        "      <td><strong>3,574</strong></td>\n",
        "      <td><strong>3,255</strong></td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "##Annotation Tagging scheme\n",
        "The tokens of all datasets, except for Aldo Moro, have been manually labeled using the IOB (Inside-Outside-Beginning) convention: each entity is labeled as begin-of-entity (B-[ent]) or continuation-of-entity (I-[ent]). The annotations of the Aldo Moro dataset, instead, were carried out with a mixed process that used both manual and automatic annotations (subsequently checked by hand); due to some differences in the convention for annotation, this dataset does not contain information for composite entities (beginning, continuation). For more details, please refer to the paper related with the dataset release: https://arxiv.org/abs/2112.15099\n",
        "\n",
        "An example of the annotations is reported here: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "durfizxkrnhW"
      },
      "source": [
        "# IOB annotation (from ADG_dev.tsv)\n",
        "Nel\tO\n",
        "nostro\tO\n",
        "Trentino\tB-LOC\n",
        "attraversiamo\tO\n",
        "un\tO\n",
        "momento\tO\n",
        "storico\tO\n",
        "importante\tO\n",
        ".\tO\n",
        "\n",
        "# non-IOB annotation (from moro_test.tsv)\n",
        "Dal\tO\n",
        "Consiglio\tORG\n",
        "nazionale\tORG\n",
        "del\tO\n",
        "â\tO\n",
        "75\tO\n",
        "la\tO\n",
        "grande\tO\n",
        "stampa\tO\n",
        "parla\tO\n",
        "di\tO\n",
        "due\tO\n",
        "anime\tO\n",
        "contrapposte\tO\n",
        "del\tO\n",
        "partito\tO\n",
        ".\tO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9ejTFLxdf_z"
      },
      "source": [
        "CODICE PER CHARTS. RIMUOVO LE MBRERIE IN SEGUITO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUxTIXvXdgoV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi\n",
        "from matplotlib import rc\n",
        "\n",
        "\n",
        "\n",
        "BIO_PATH = '../datasets/BIO_tag_NER_notation/'\n",
        "nonBIO_PATH = '../datasets/Inside_outside_NER_notation/'\n",
        "\n",
        "\n",
        "SPIDER_ROW_N = 2\n",
        "SPIDER_COL_N = 2\n",
        "BAR_ROW_N = 1 \n",
        "BAR_COL_N = 2\n",
        "\n",
        "def to_lowerCase(df):\n",
        "    return pd.DataFrame({'Token': df['Token'].str.lower(), 'Entity': df['Entity']})\n",
        "\n",
        "def add_column_names(df):\n",
        "    return  df.rename(columns={0: 'Token', 1: 'Entity'})\n",
        "\n",
        "def spider_plot(df, group, title, subplot_idx):\n",
        "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "    categories=list(df)[:]\n",
        "    N = len(categories)\n",
        "    \n",
        "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
        "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "    \n",
        "    # Initialise the spider plot\n",
        "    ax = plt.subplot(SPIDER_ROW_N, SPIDER_COL_N, subplot_idx, polar=True)\n",
        "    \n",
        "    # first axis to be on top:\n",
        "    ax.set_theta_offset(pi / 2)\n",
        "    ax.set_theta_direction(-1)\n",
        "    \n",
        "    # Draw one axe per variable + add labels\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "    \n",
        "    # Draw ylabels\n",
        "    #ax.set_yscale('log')\n",
        "    ax.set_rlabel_position(0)\n",
        "    min = df.min().min()\n",
        "    max = df.max().max()\n",
        "    plt.ylim(min -(max-min)/10, max + (max-min)/10)\n",
        " \n",
        "    # Plot each individual = each line of the data\n",
        "    # I don't make a loop, because plotting more than 3 groups makes the chart unreadable\n",
        "    \n",
        "    for i in range(len(group)):\n",
        "        values=df.loc[i].values.flatten().tolist()\n",
        "        values += values[:1]\n",
        "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=group[i])\n",
        "        ax.fill(angles, values, 'b', alpha=0.1)\n",
        "\n",
        "    \n",
        "    # Add legend\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "    plt.title(title)\n",
        "\n",
        "    # Show the graph\n",
        "    #plt.show()\n",
        "\n",
        "############################################################### reading datasets ###############################################################\n",
        "\n",
        "ds = {'ds_mr'           : pd.read_csv(nonBIO_PATH + 'moro_train.tsv', sep='\\t', header=None),\n",
        "      'ds_mr_test'      : pd.read_csv(nonBIO_PATH + 'moro_test.tsv', sep='\\t', header=None),\n",
        "      \n",
        "      'ds_dg_IOB'       : pd.read_csv(BIO_PATH + 'degasperi_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_fc_IOB'       : pd.read_csv(BIO_PATH + 'fiction_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_wn_IOB'       : pd.read_csv(BIO_PATH + 'wikinews_train_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_dg_IOB_test'  : pd.read_csv(BIO_PATH + 'degasperi_test_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_fc_IOB_test'  : pd.read_csv(BIO_PATH + 'fiction_test_BIO.tsv', sep='\\t', header=None),\n",
        "      'ds_wn_IOB_test'  : pd.read_csv(BIO_PATH + 'wikinews_test_BIO.tsv', sep='\\t', header=None)\n",
        "}\n",
        "\n",
        "############################################################### calculating stats ###############################################################\n",
        "stats = {}\n",
        "\n",
        "for i in ds.keys():     \n",
        "    ds[i] = add_column_names(ds[i])\n",
        "    ds[i] = to_lowerCase(ds[i])\n",
        "\n",
        "    stats[i] = {\n",
        "        'doc_len' : ds[i]['Token'].count(),\n",
        "        'voc_size' : ds[i]['Token'].nunique(),\n",
        "        'n_punct': sum(1 for k in ds[i]['Token'] if all(char in string.punctuation for char in k))\n",
        "    }\n",
        "\n",
        "    if('IOB' in str(i)):\n",
        "        \n",
        "        stats[i]['n_I-PER'] = sum(1 for k in ds[i]['Entity'] if k == 'I-PER') \n",
        "        stats[i]['n_I-ORG'] = sum(1 for k in ds[i]['Entity'] if k == 'I-ORG')\n",
        "        stats[i]['n_I-LOC'] = sum(1 for k in ds[i]['Entity'] if k == 'I-LOC')\n",
        "\n",
        "        stats[i]['n_B-PER'] = sum(1 for k in ds[i]['Entity'] if k == 'B-PER') \n",
        "        stats[i]['n_B-ORG'] = sum(1 for k in ds[i]['Entity'] if k == 'B-ORG')\n",
        "        stats[i]['n_B-LOC'] = sum(1 for k in ds[i]['Entity'] if k == 'B-LOC')\n",
        "\n",
        "        stats[i]['n_PER'] = stats[i]['n_I-PER'] + stats[i]['n_B-PER']\n",
        "        stats[i]['n_ORG'] = stats[i]['n_I-ORG'] + stats[i]['n_B-ORG']\n",
        "        stats[i]['n_LOC'] = stats[i]['n_I-LOC'] + stats[i]['n_B-LOC']\n",
        "        \n",
        "    else:\n",
        "        stats[i]['n_PER'] = sum(1 for k in ds[i]['Entity'] if k == 'PER') \n",
        "        stats[i]['n_ORG'] = sum(1 for k in ds[i]['Entity'] if k == 'ORG')\n",
        "        stats[i]['n_LOC'] = sum(1 for k in ds[i]['Entity'] if k == 'LOC')\n",
        "\n",
        "    stats[i]['n_O'] = sum(1 for k in ds[i]['Entity'] if k == 'O')\n",
        "        \n",
        "\n",
        "avg_doc_len = sum(stats[i]['doc_len'] for i in stats.keys()) / len(stats.keys())\n",
        "avg_voc_size = sum(stats[i]['voc_size'] for i in stats.keys()) / len(stats.keys())\n",
        "\n",
        "############################################################### preparing data for plotting ###############################################################\n",
        " \n",
        "# Values of each group\n",
        "iper = [stats[i]['n_I-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iorg = [stats[i]['n_I-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iloc = [stats[i]['n_I-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "\n",
        "bper = [stats[i]['n_B-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "borg = [stats[i]['n_B-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "bloc = [stats[i]['n_B-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "\n",
        "iob_punct = [stats[i]['n_punct'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iob_o = [stats[i]['n_O'] for i in stats.keys() if 'IOB' in str(i) and 'test' not in str(i)]\n",
        "iob_o = [iob_o[i] - iob_punct[i] for i in range(len(iob_o))]\n",
        "\n",
        "iper_test = [stats[i]['n_I-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iorg_test = [stats[i]['n_I-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iloc_test = [stats[i]['n_I-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "\n",
        "bper_test = [stats[i]['n_B-PER'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "borg_test = [stats[i]['n_B-ORG'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "bloc_test = [stats[i]['n_B-LOC'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "\n",
        "iob_punct_test = [stats[i]['n_punct'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iob_o_test = [stats[i]['n_O'] for i in stats.keys() if 'IOB' in str(i) and 'test' in str(i)]\n",
        "iob_o_test = [iob_o_test[i] - iob_punct_test[i] for i in range(len(iob_o_test))]\n",
        "\n",
        "iob_voc_size = [stats[i]['voc_size'] for i in stats.keys() if 'IOB' in str(i)]\n",
        "\n",
        "per = [stats[i]['n_PER'] for i in stats.keys() if 'test' not in str(i)]\n",
        "org = [stats[i]['n_ORG'] for i in stats.keys() if 'test' not in str(i)]\n",
        "loc = [stats[i]['n_LOC'] for i in stats.keys() if 'test' not in str(i)]\n",
        "\n",
        "per_test = [stats[i]['n_PER'] for i in stats.keys() if 'test' in str(i)]\n",
        "org_test = [stats[i]['n_ORG'] for i in stats.keys() if 'test' in str(i)]\n",
        "loc_test = [stats[i]['n_LOC'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "voc_size = [stats[i]['voc_size'] for i in stats.keys() if 'test' not in str(i)]\n",
        "voc_size_test = [stats[i]['voc_size'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "doc_len = [stats[i]['doc_len'] for i in stats.keys() if 'test' not in str(i)]\n",
        "doc_len_test = [stats[i]['doc_len'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "n_punct = [stats[i]['n_punct'] for i in stats.keys() if 'test' not in str(i)]\n",
        "n_punct_test = [stats[i]['n_punct'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "o = [stats[i]['n_O'] for i in stats.keys() if 'test' not in str(i)]\n",
        "punct = [stats[i]['n_punct'] for i in stats.keys() if 'test' not in str(i)]\n",
        "\n",
        "\n",
        "o_test = [stats[i]['n_O'] for i in stats.keys() if 'test' in str(i)]\n",
        "punct_test = [stats[i]['n_punct'] for i in stats.keys() if 'test' in str(i)]\n",
        "\n",
        "############################################################### spider plots ###############################################################\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': bper,\n",
        "    'I-PER': iper,\n",
        "    'B-ORG': borg,\n",
        "    'I-ORG': iorg,\n",
        "    'B-LOC': bloc,\n",
        "    'I-LOC': iloc }),\n",
        "    ['deGasperi', 'Fiction', 'Wikinews'],\n",
        "    'Train set IOB tags',\n",
        "    1)\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': bper_test,\n",
        "    'I-PER': iper_test,\n",
        "    'B-ORG': borg_test,\n",
        "    'I-ORG': iorg_test,\n",
        "    'B-LOC': bloc_test,\n",
        "    'I-LOC': iloc_test }),\n",
        "    ['deGasperi', 'Fiction', 'Wikinews'],\n",
        "    'Test set IOB tags',\n",
        "    2)\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': per,\n",
        "    'B-ORG': org,\n",
        "    'B-LOC': loc}),\n",
        "    ['Moro', 'deGasperi', 'Fiction', 'Wikinews'],\n",
        "    'Train set non-IOB tags',\n",
        "    3)\n",
        "\n",
        "spider_plot( pd.DataFrame({\n",
        "    'B-PER': per_test,\n",
        "    'B-ORG': org_test,\n",
        "    'B-LOC': loc_test}),\n",
        "    ['Moro', 'deGasperi', 'Fiction', 'Wikinews'],\n",
        "    'Test set non-IOB tags',\n",
        "    4)\n",
        "\n",
        "#spider_plot( pd.DataFrame({\n",
        "#    'doc_len': doc_len,\n",
        "#    'voc_size': voc_size,\n",
        "#    'n_punct': n_punct,}),\n",
        "#    ['Moro', 'deGasperi', 'Fiction', 'Wikinews'],\n",
        "#    'Train set document statistics',\n",
        "#    5)\n",
        "#\n",
        "#spider_plot( pd.DataFrame({\n",
        "#    'doc_len': doc_len_test,\n",
        "#    'voc_size': voc_size_test,\n",
        "#    'n_punct': n_punct_test,}),\n",
        "#    ['Moro', 'deGasperi', 'Fiction', 'Wikinews'],\n",
        "#    'Test set document statistics',\n",
        "#    6)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def stacked_bar_plot(col_names, data, labels_name, title, subplot_idx):\n",
        "    plt.rcParams[\"figure.figsize\"] = (15,10)\n",
        "    # Names of group and bar width\n",
        "    barWidth = 1\n",
        "    bars = np.zeros(len(col_names))\n",
        "    n_col = np.arange(len(col_names))\n",
        "    plt.subplot(BAR_ROW_N, BAR_COL_N, subplot_idx)\n",
        "\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        plt.bar(n_col, data[i], bottom=bars, edgecolor='white', width=barWidth, label=labels_name[i])\n",
        "        bars = np.add(bars, data[i]).tolist()\n",
        "\n",
        "    bars = np.add(bper, iper).tolist()\n",
        "    \n",
        "    # Custom X axis\n",
        "    plt.xticks(n_col, col_names, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "\n",
        "    \n",
        "    # Show graphic\n",
        "stacked_bar_plot(['Moro', 'deGasperi', 'Fiction', 'Wikinews'],\n",
        "                 [punct, [o[i]-punct[i] for i in range(len(o))]],\n",
        "                 ['punct', 'O'],\n",
        "                 'Train sets',\n",
        "                 1)\n",
        "\n",
        "stacked_bar_plot(['Moro', 'deGasperi', 'Fiction', 'Wikinews'],\n",
        "                 [punct_test, [o_test[i]-punct_test[i] for i in range(len(o_test))]],\n",
        "                 ['punct', 'O'],\n",
        "                 'Test sets',\n",
        "                 2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyT4H448AUO6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#BIO_PATH = 'BIO_tag_NER_notation'\n",
        "#nonBIO_PATH = 'Inside_outside_NER_notation'\n",
        "\n",
        "#function to check if 2 tsv files are equal\n",
        "def check_equal(file1, file2):\n",
        "\tdf1 = pd.read_csv(file1, sep='\\t')\n",
        "\tdf2 = pd.read_csv(file2, sep='\\t')\n",
        "\t#printf the path of the two filr, length of the 2 files, and if they are equal\n",
        "\tprint(file1, \"\\n\", \n",
        "       \t  file2, \"\\n\",\n",
        "\t\t    \"equal: \", df1.equals(df2), \"\\n\\n\")\n",
        "\n",
        "\n",
        "#########################Ã  BIO TAGGER ########################################\n",
        "\n",
        "oTag = \"O\"  \n",
        "types = set()\n",
        "\n",
        "files = {\n",
        "\t\"wikinews_train.tsv\"\t: BIO_PATH + \"automatic/WN_train.tsv\",\n",
        "\t\"wikinews_test.tsv\"\t\t: BIO_PATH + \"automatic/WN_dev.tsv\",\n",
        "\t\"fiction_train.tsv\"\t\t: BIO_PATH + \"automatic/FIC_train.tsv\",\n",
        "\t\"fiction_test.tsv\"\t\t: BIO_PATH + \"automatic/FIC_dev.tsv\",\n",
        "\t\"degasperi_train.tsv\"\t: BIO_PATH + \"automatic/ADG_train.tsv\",\n",
        "\t\"degasperi_test.tsv\"\t: BIO_PATH + \"automatic/ADG_dev.tsv\",\n",
        "\t\"moro_train.tsv\"\t\t: BIO_PATH + \"automatic/MR_train.tsv\",\n",
        "\t\"moro_test.tsv\"\t\t\t: BIO_PATH + \"automatic/MR_dev.tsv\",\n",
        "}\n",
        "\n",
        "count = {}\n",
        "\n",
        "for file in files:\n",
        "\twith open(os.path.join(nonBIO_PATH, file), \"r\") as f:\n",
        "\t\toutFile = files[file]\n",
        "\t\tcount[outFile] = {\"sentences\": 0, \"tags\": {}, \"tokens\": 0}\n",
        "\n",
        "\t\tsentences = []\n",
        "\t\tthisSentence = []\n",
        "\n",
        "\t\tfor line in f:\n",
        "\t\t\tline = line.strip()\n",
        "\t\t\tif len(line) == 0:\n",
        "\t\t\t\tif len(thisSentence) > 0:\n",
        "\t\t\t\t\tsentences.append(thisSentence)\n",
        "\t\t\t\t\tthisSentence = []\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tparts = line.split(\"\\t\")\n",
        "\t\t\tthisSentence.append(parts)\n",
        "\t\t\tcount[outFile][\"tokens\"] += 1\n",
        "\n",
        "\t\tif len(thisSentence) > 0:\n",
        "\t\t\tsentences.append(thisSentence)\n",
        "\n",
        "\t\tcount[outFile][\"sentences\"] = len(sentences)\n",
        "\n",
        "\t\tfor sentence in sentences:\n",
        "\t\t\tpreviousNer = oTag\n",
        "\t\t\tfor token in sentence:\n",
        "\t\t\t\tner = token[1]\n",
        "\t\t\t\tnewNer = ner\n",
        "\t\t\t\tif ner != oTag:\n",
        "\t\t\t\t\tif previousNer != ner:\n",
        "\t\t\t\t\t\tif ner not in count[outFile][\"tags\"]:\n",
        "\t\t\t\t\t\t\tcount[outFile][\"tags\"][ner] = 0\n",
        "\t\t\t\t\t\tnewNer = \"B-\" + ner\n",
        "\t\t\t\t\t\tcount[outFile][\"tags\"][ner] += 1\n",
        "\t\t\t\t\t\ttypes.add(ner)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tnewNer = \"I-\" + ner\n",
        "\t\t\t\ttoken[1] = newNer\n",
        "\t\t\t\tpreviousNer = ner\n",
        "\n",
        "\t\twith open(outFile, \"w\") as fw:\n",
        "\t\t\tfor sentence in sentences:\n",
        "\t\t\t\tfor token in sentence:\n",
        "\t\t\t\t\tfw.write(token[0])\n",
        "\t\t\t\t\tfw.write(\"\\t\")\n",
        "\t\t\t\t\tfw.write(token[1])\n",
        "\t\t\t\t\tfw.write(\"\\n\")\n",
        "\t\t\t\tfw.write(\"\\n\")\n",
        "\n",
        "#########################Ã  CHECK ########################################\n",
        "comp = [\n",
        "\t[BIO_PATH + \"automatic/WN_train.tsv\", \t'./' + BIO_PATH + '/wikinews_train_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/WN_dev.tsv\", \t'./' + BIO_PATH + '/wikinews_test_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/FIC_train.tsv\", \t'./' + BIO_PATH + '/fiction_train_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/FIC_dev.tsv\", \t'./' + BIO_PATH + '/fiction_test_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/ADG_train.tsv\", \t'./' + BIO_PATH + '/degasperi_train_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/ADG_dev.tsv\", \t'./' + BIO_PATH + '/degasperi_test_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/MR_train.tsv\", \t'./' + BIO_PATH + '/moro_train_BIO.tsv'],\n",
        "\t[BIO_PATH + \"automatic/MR_dev.tsv\", \t'./' + BIO_PATH + '/moro_test_BIO.tsv'],\n",
        "]\n",
        "\n",
        "for i in comp:\n",
        "\tcheck_equal(i[0], i[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcVTkqQIAUO7"
      },
      "source": [
        "# Extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl5msGpYtt5K"
      },
      "outputs": [],
      "source": [
        "# Elia valuta se puÃ² essere utile, altrimenti eliminiamo\n",
        "def compute_statistics(dataset_name, dataset, statistics_df):\n",
        "  '''\n",
        "  Computes the statistics of the dataset and adds it into an aggregated structure\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_name: str\n",
        "  dataset: DataFrame\n",
        "  statistics_df: DataFrame\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  statistics_df: DataFrame\n",
        "  '''\n",
        "  if statistics_df is None:\n",
        "    columns = ['Dataset', 'Number of tokens', 'Number of unique tokens', 'Entity tag types']\n",
        "    statistics_df = pd.DataFrame([[dataset_name, len(dataset), len(dataset['Token'].unique()), sorted(dataset['Tag'].unique())]], columns=columns)\n",
        "  else:\n",
        "    columns = ['Dataset', 'Number of tokens', 'Number of unique tokens', 'Entity tag types']\n",
        "    statistics_df = pd.concat([statistics_df, pd.DataFrame([[dataset_name, len(dataset), len(dataset['Token'].unique()), sorted(dataset['Tag'].unique())]], columns=columns)])\n",
        "  return statistics_df\n",
        "\n",
        "\n",
        "\n",
        "  # Rimuovi l'hardcoding dei nomi delle colonne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo7B5swCz1V2"
      },
      "outputs": [],
      "source": [
        "# Elia valuta se puÃ² essere utile, altrimenti eliminiamo\n",
        "datasets_df = []\n",
        "dataset_stats_df = None\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "  dataset_df = pd.read_csv(dataset_name, sep='\\t', names=['Token', 'Tag'])\n",
        "  datasets_df.append(dataset_df)\n",
        "  dataset_stats_df = compute_statistics(dataset_name, dataset_df, dataset_stats_df)\n",
        "\n",
        "dataset_stats_df.style.hide(axis='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZQIONEdAUO7"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02c39e51caef4e5d91607f6920fe5674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2aa9815521414b8289569ccf50bd6e61",
              "IPY_MODEL_4961b491aa5444fd933d0068caa662ee",
              "IPY_MODEL_1b5dfed60a1d4a8697259d5384b9ee6f"
            ],
            "layout": "IPY_MODEL_6f2c57c80e734304b4b5d6cc05e97340"
          }
        },
        "2aa9815521414b8289569ccf50bd6e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454d623a66784bbf95c3a90b765ea073",
            "placeholder": "â",
            "style": "IPY_MODEL_bbd7fece75ff4aeca1f94fe026f89a30",
            "value": "Map:  98%"
          }
        },
        "4961b491aa5444fd933d0068caa662ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161a1970929947f8b40ab45e81eb9d3b",
            "max": 27538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b1595a597054ce38909ed3020b62094",
            "value": 27538
          }
        },
        "1b5dfed60a1d4a8697259d5384b9ee6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7165a3c3270e4dfdb9e82cccc9e5c90a",
            "placeholder": "â",
            "style": "IPY_MODEL_f621770e7ab342f784a02de4588b519c",
            "value": " 27000/27538 [00:03&lt;00:00, 7658.62 examples/s]"
          }
        },
        "6f2c57c80e734304b4b5d6cc05e97340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "454d623a66784bbf95c3a90b765ea073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd7fece75ff4aeca1f94fe026f89a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "161a1970929947f8b40ab45e81eb9d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1595a597054ce38909ed3020b62094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7165a3c3270e4dfdb9e82cccc9e5c90a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f621770e7ab342f784a02de4588b519c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}