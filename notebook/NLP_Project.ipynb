{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EliaFeltrin/KIND_project/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iKwSQ3yjxA6I"
   },
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oMU9U9-GzVvR"
   },
   "outputs": [],
   "source": [
    "def get_string_from_df(dataframe, puntuaction):\n",
    "  '''\n",
    "  Transforms the tokenized dataset into a single string.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  dataframe: DataFrame\n",
    "    structure containing the tokenized dataset\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  text: str\n",
    "    string concatenating all the tokens of the dataset\n",
    "  '''\n",
    "  text_df = dataframe.loc[:,'Token']\n",
    "  text = text_df[0]\n",
    "  for token in text_df[1:]:\n",
    "    text += (' ' + token) if token not in puntuaction else token\n",
    "  return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_uSdgHpkxIGR"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I7o1jqSjx0mN"
   },
   "outputs": [],
   "source": [
    "# Cloning the github repository (TO BE DELETED?)\n",
    "#! git clone https://github.com/EliaFeltrin/KIND_project.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nLQhCXCbxy-f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (6.19.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (6.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (8.12.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (0.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (23.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (1.5.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (1.5.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (25.0.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (5.7.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipykernel) (8.1.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: stack-data in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.36)\n",
      "Requirement already satisfied: backcall in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.11.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel) (305.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-client>=6.1.12->ipykernel) (2.5.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.9.1.1-cp310-cp310-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.1.1\n",
      "Requirement already satisfied: plotly in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (5.7.0)\n",
      "Collecting nbformat\n",
      "  Using cached nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from nbformat) (5.3.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from nbformat) (4.17.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jsonschema>=2.6->nbformat) (22.1.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jupyter-core->nbformat) (305.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jupyter-core->nbformat) (2.5.2)\n",
      "Installing collected packages: nbformat\n",
      "  Attempting uninstall: nbformat\n",
      "    Found existing installation: nbformat 5.7.0\n",
      "    Uninstalling nbformat-5.7.0:\n",
      "      Successfully uninstalled nbformat-5.7.0\n",
      "Successfully installed nbformat-5.8.0\n",
      "Collecting python-terrier\n",
      "  Using cached python_terrier-0.9.2-py3-none-any.whl\n",
      "Collecting statsmodels\n",
      "  Using cached statsmodels-0.14.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "Collecting ir-datasets>=0.3.2\n",
      "  Using cached ir_datasets-0.5.4-py3-none-any.whl (311 kB)\n",
      "Collecting ir-measures>=0.3.1\n",
      "  Using cached ir_measures-0.3.1-py3-none-any.whl\n",
      "Collecting nptyping==1.4.4\n",
      "  Using cached nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
      "Collecting pytrec-eval-terrier>=0.5.3\n",
      "  Using cached pytrec_eval_terrier-0.5.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (1.1.1)\n",
      "Collecting wget\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (1.5.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (2.29.0)\n",
      "Collecting deprecated\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (1.10.1)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (3.1.2)\n",
      "Collecting chest\n",
      "  Using cached chest-0.2.3-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (1.2.2)\n",
      "Collecting matchpy\n",
      "  Using cached matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from python-terrier) (1.24.3)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pyjnius>=1.4.2\n",
      "  Using cached pyjnius-1.4.2-cp310-cp310-win_amd64.whl (197 kB)\n",
      "Collecting typish>=1.7.0\n",
      "  Using cached typish-1.9.3-py3-none-any.whl (45 kB)\n",
      "Collecting pyyaml>=5.3.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting trec-car-tools>=2.5.4\n",
      "  Using cached trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Collecting warc3-wet>=0.2.3\n",
      "  Using cached warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
      "Collecting pyautocorpus>=0.1.1\n",
      "  Using cached pyautocorpus-0.1.9-cp310-cp310-win_amd64.whl (7.1 kB)\n",
      "Collecting ijson>=3.1.3\n",
      "  Using cached ijson-3.2.0.post0-cp310-cp310-win_amd64.whl (48 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (4.12.2)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5\n",
      "  Using cached warc3_wet_clueweb09-0.2.5-py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=4.5.2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from ir-datasets>=0.3.2->python-terrier) (4.9.2)\n",
      "Collecting lz4>=3.1.1\n",
      "  Using cached lz4-4.3.2-cp310-cp310-win_amd64.whl (99 kB)\n",
      "Collecting zlib-state>=0.1.3\n",
      "  Using cached zlib_state-0.1.5-cp310-cp310-win_amd64.whl (12 kB)\n",
      "Collecting unlzw3>=0.2.1\n",
      "  Using cached unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
      "Collecting cwl-eval>=1.0.10\n",
      "  Using cached cwl_eval-1.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.7.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from pyjnius>=1.4.2->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests->python-terrier) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests->python-terrier) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests->python-terrier) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests->python-terrier) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from tqdm->python-terrier) (0.4.6)\n",
      "Collecting heapdict\n",
      "  Using cached HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jinja2->python-terrier) (2.1.1)\n",
      "Collecting multiset<3.0,>=2.0\n",
      "  Using cached multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from pandas->python-terrier) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from pandas->python-terrier) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from scikit-learn->python-terrier) (2.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from statsmodels->python-terrier) (23.0)\n",
      "Collecting patsy>=0.5.2\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.4)\n",
      "Collecting cbor>=1.0.0\n",
      "  Using cached cbor-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: wget, warc3-wet-clueweb09, warc3-wet, typish, multiset, ijson, heapdict, cbor, zlib-state, wrapt, unlzw3, trec-car-tools, tqdm, pyyaml, pytrec-eval-terrier, pyjnius, pyautocorpus, patsy, nptyping, more-itertools, matchpy, lz4, dill, cwl-eval, chest, ir-measures, ir-datasets, deprecated, statsmodels, python-terrier\n",
      "Successfully installed cbor-1.0.0 chest-0.2.3 cwl-eval-1.0.12 deprecated-1.2.13 dill-0.3.6 heapdict-1.0.1 ijson-3.2.0.post0 ir-datasets-0.5.4 ir-measures-0.3.1 lz4-4.3.2 matchpy-0.5.5 more-itertools-9.1.0 multiset-2.1.1 nptyping-1.4.4 patsy-0.5.3 pyautocorpus-0.1.9 pyjnius-1.4.2 python-terrier-0.9.2 pytrec-eval-terrier-0.5.5 pyyaml-6.0 statsmodels-0.14.0 tqdm-4.65.0 trec-car-tools-2.6 typish-1.9.3 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 wget-3.2 wrapt-1.15.0 zlib-state-0.1.5\n"
     ]
    }
   ],
   "source": [
    "# Installing the library needed in the following part of the project\n",
    "\n",
    "# Installing the utlity packages\n",
    "#! pip install scipy\n",
    "#! pip install numpy\n",
    "#! pip install pandas\n",
    "! pip install ipykernel\n",
    "\n",
    "# Installing the packages for creating amazing plots\n",
    "#! pip install matplotlib\n",
    "! pip install wordcloud\n",
    "! pip install plotly\n",
    "! pip install --upgrade nbformat\n",
    "\n",
    "# Installing a package for sequence labeling, used for POS tagging and NER\n",
    "#! pip install -U spacy\n",
    "\n",
    "# Installing the packages for creating the word embeddings\n",
    "#! pip install --upgrade gensim\n",
    "\n",
    "# Installing the packages for doing dimensionality reduction\n",
    "#! pip install sklearn\n",
    "\n",
    "! pip install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.5.0/it_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/13.0 MB 10.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.2/13.0 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.6/13.0 MB 10.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.5/13.0 MB 10.3 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/13.0 MB 10.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/13.0 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.3/13.0 MB 11.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.7/13.0 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.6/13.0 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.3/13.0 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/13.0 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.9/13.0 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/13.0 MB 12.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  13.0/13.0 MB 12.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 13.0/13.0 MB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from it-core-news-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (66.0.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.29.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mike\\anaconda3\\envs\\nlp_project\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->it-core-news-sm==3.5.0) (2.1.1)\n",
      "\u001B[38;5;2m[+] Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# Dowloading an italian model from spacy\n",
    "! spacy download it_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iXMMltVMVFQO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.2 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the main packages\n",
    "\n",
    "# Importing the utlity packages\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Importing the packages for creating amazing plots\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# Importing the packages for POS tagging\n",
    "import spacy as spc\n",
    "\n",
    "# Importing the packages for creating the word embeddings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Importing the packages for doing dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "  pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the names of the datasets\n",
    "dataset_names = ['degasperi_train.tsv', 'degasperi_test.tsv', 'fiction_train.tsv',\\\n",
    "            'fiction_test.tsv', 'moro_train.tsv', 'moro_test.tsv',\\\n",
    "            'wikinews_train.tsv', 'wikinews_test.tsv']\n",
    "# Defining the path to datasets\n",
    "PATH_TO_DATASETS = '../datasets/Inside_outside_NER_notation'\n",
    "# Enrico's path, scusate ragazzi, sistemerò TO DOOOOOOOOOOOOOOOOO\n",
    "#PATH_TO_DATASETS = '/Users/enricosimionato/Desktop/KIND_project/datasets/Inside_outside_NER_notation'\n",
    "# Importing all the datasets in a dictionary\n",
    "datasets_dict = {name: pd.read_csv(PATH_TO_DATASETS+'/'+name, sep='\\t', header=None).rename(columns={0: 'Token', 1: 'Entity'}) for name in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZGkcqAEwVDL9"
   },
   "outputs": [],
   "source": [
    "# Defining the path to datasets\n",
    "PATH_TO_DATASETS = '../datasets/Inside_outside_NER_notation'\n",
    "# Enrico's path, scusate ragazzi, sistemerò TO DOOOOOOOOOOOOOOOOO\n",
    "#PATH_TO_DATASETS = '/Users/enricosimionato/Desktop/KIND_project/datasets/Inside_outside_NER_notation'\n",
    "\n",
    "# Loading the datasets\n",
    "dataset_degasperi = pd.read_csv(PATH_TO_DATASETS+'/degasperi_train.tsv', sep='\\t', header=None)\n",
    "dataset_degasperi = dataset_degasperi.rename(columns={0: 'Token', 1: 'Entity'})\n",
    "\n",
    "dataset_moro = pd.read_csv(PATH_TO_DATASETS+'/moro_train.tsv', sep='\\t', header=None)\n",
    "dataset_moro = dataset_moro.rename(columns={0: 'Token', 1: 'Entity'})\n",
    "\n",
    "dataset_fiction = pd.read_csv(PATH_TO_DATASETS+'/fiction_train.tsv', sep='\\t', header=None)\n",
    "dataset_fiction = dataset_fiction.rename(columns={0: 'Token', 1: 'Entity'})\n",
    "\n",
    "dataset_wikinews = pd.read_csv(PATH_TO_DATASETS+'/wikinews_train.tsv', sep='\\t', header=None)\n",
    "dataset_wikinews = dataset_wikinews.rename(columns={0: 'Token', 1: 'Entity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123504\n",
      "123504\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_string_from_df() missing 2 required positional arguments: 'dataframe' and 'puntuaction'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mstr\u001B[39m(a))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mstr\u001B[39m(b))\n\u001B[1;32m----> 7\u001B[0m \u001B[43mget_string_from_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m dataset_degasperi[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mToken\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: get_string_from_df() missing 2 required positional arguments: 'dataframe' and 'puntuaction'"
     ]
    }
   ],
   "source": [
    "a=len(dataset_degasperi.where(dataset_degasperi['Token'].str.contains('\\n')))\n",
    "\n",
    "b=len(dataset_degasperi)\n",
    "print(str(a))\n",
    "print(str(b))\n",
    "\n",
    "get_string_from_df()\n",
    "dataset_degasperi['Token'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6PMsuTVlfsNw"
   },
   "source": [
    "## Word embeddings representation\n",
    "\n",
    "A word embedding is the vectorial representation of a word. It is used for achieving a dense representation of the words in an high dimensional space. Other types of representation of the words, such as the bag of words representation, are sparse compared to word embeddings.\n",
    "The word embeddings allow to achieve better results in many fields of natural language processing.\n",
    "\n",
    "In particular we use Word2Vec in order to obtain the word embeddings of out dataset.\n",
    "Basic Word2Vec is a artificial neural network composed by two layers\n",
    "\n",
    "The inputs of the neural network are the words of the sentence to convert in the word embeddings representation.\n",
    "The first layer is a linear layer.\n",
    "The linear activation functions values are summed and put as outputs.\n",
    "At the end we appply a softmax layer.\n",
    "We want the model to prefict the next word in the sentence.\n",
    "I train the NN using the cross entropy as loss function.\n",
    "At the end of the training the weights connecting the inputs to the first hidden layer are the values of the dimensions of the word embedding.\n",
    "\n",
    "\n",
    "Two of the most used architecture of the Word2Vec are CBOW and Skip-Gram.\n",
    "\n",
    "The Continuous Bag of Words method uses many words surrounding the word I want to use in the prediction in the training step.\n",
    "The Skip-gram uses a word to predict the word in the surroundings.\n",
    "\n",
    "In some way the distribution in the various dimension is based on the similarity of the words in terms of semantics and usage.\n",
    "\n",
    "With word embeddings we can embed the context of the word inside its representation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the dataframe containing the tokens of the dataset.\n",
    "The pipeline used for the definition of the input of the word2vec model is the following:\n",
    "- (previous) the tokenization has already been done\n",
    "- merging the single elements in sentences splitting on the single dots\n",
    "- lowercasing all the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_list_from_df(dataset_df):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_df: DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sentences_list: list\n",
    "\n",
    "    '''\n",
    "    punctuation = string.punctuation\n",
    "    sentences_list = [[]]\n",
    "    labels_list = [[]]\n",
    "    count = 0\n",
    "    \n",
    "    for element in dataset_df.iterrows():\n",
    "        if str(element[1]['Token']) == '.':\n",
    "            sentences_list.append([])\n",
    "            labels_list.append([])\n",
    "            count += 1\n",
    "        elif str(element[1]['Token']) not in punctuation:\n",
    "            sentences_list[count].append(element[1]['Token'].lower())\n",
    "            labels_list[count].append(element[1]['Entity'])\n",
    "            \n",
    "    return sentences_list\n",
    "\n",
    "def remove_short_sentences(sentences_list, min_length=3):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences_list: list\n",
    "\n",
    "    min_lenght: int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sentences_list: list\n",
    "    \n",
    "    '''\n",
    "    for sentence in sentences_list:\n",
    "        if len(sentence) < min_length:\n",
    "            sentences_list.remove(sentence)\n",
    "    return sentences_list\n",
    "\n",
    "def get_all_sentences_from_datasets(datasets):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets: list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    overall_sentences: list\n",
    "    \n",
    "    '''\n",
    "    overall_sentences = list()\n",
    "    for key in datasets.keys():\n",
    "        sentences = get_sentences_list_from_df(datasets[key])\n",
    "        overall_sentences += remove_short_sentences(sentences)\n",
    "    return overall_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_all_sentences_from_datasets(datasets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in datasets_dict.keys():\n",
    "    print(datasets_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datasets_dict.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict['degasperi_train.tsv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "length_list= []\n",
    "for i in sentences:\n",
    "    length_list.append(len(i))\n",
    "max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Word2Vec model\n",
    "embeddings_model = Word2Vec(sentences, vector_size=768, min_count=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the length of the vocabulary\n",
    "len(embeddings_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'danno'\n",
    "embeddings_model.wv.most_similar(term)\n",
    "\n",
    "# There is some problem maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(list(embeddings_model.wv.key_to_index), 500)\n",
    "word_vectors = embeddings_model.wv[sample]\n",
    "\n",
    "tsne = TSNE(n_components=3, n_iter=2000)\n",
    "tsne_embedding = tsne.fit_transform(word_vectors)\n",
    "x, y, z = np.transpose(tsne_embedding)\n",
    "\n",
    "fig = px.scatter_3d(x=x[:200],y=y[:200],z=z[:200],text=sample[:200])\n",
    "fig.update_traces(marker=dict(size=3,line=dict(width=2)),textfont_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the embeddings using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##DA VALUTARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl5msGpYtt5K"
   },
   "outputs": [],
   "source": [
    "# Elia valuta se può essere utile, altrimenti eliminiamo\n",
    "def compute_statistics(dataset_name, dataset, statistics_df):\n",
    "  '''\n",
    "  Computes the statistics of the dataset and adds it into an aggregated structure\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  dataset_name: str\n",
    "  dataset: DataFrame\n",
    "  statistics_df: DataFrame\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  statistics_df: DataFrame\n",
    "  '''\n",
    "  if statistics_df is None:\n",
    "    columns = ['Dataset', 'Number of tokens', 'Number of unique tokens', 'Entity tag types']\n",
    "    statistics_df = pd.DataFrame([[dataset_name, len(dataset), len(dataset['Token'].unique()), sorted(dataset['Tag'].unique())]], columns=columns)\n",
    "  else:\n",
    "    columns = ['Dataset', 'Number of tokens', 'Number of unique tokens', 'Entity tag types']\n",
    "    statistics_df = pd.concat([statistics_df, pd.DataFrame([[dataset_name, len(dataset), len(dataset['Token'].unique()), sorted(dataset['Tag'].unique())]], columns=columns)])\n",
    "  return statistics_df\n",
    "\n",
    "\n",
    "\n",
    "  # Rimuovi l'hardcoding dei nomi delle colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo7B5swCz1V2"
   },
   "outputs": [],
   "source": [
    "# Elia valuta se può essere utile, altrimenti eliminiamo\n",
    "datasets_df = []\n",
    "dataset_stats_df = None\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "  dataset_df = pd.read_csv(dataset_name, sep='\\t', names=['Token', 'Tag'])\n",
    "  datasets_df.append(dataset_df)\n",
    "  dataset_stats_df = compute_statistics(dataset_name, dataset_df, dataset_stats_df)\n",
    "\n",
    "dataset_stats_df.style.hide(axis='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PfbNEkJZgDJe"
   },
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NL0AA1eVdT3o"
   },
   "outputs": [],
   "source": [
    "# Loading the italian model\n",
    "nlp_model = it_core_news_sm.load()\n",
    "\n",
    "text_degasperi = ' '.join(dataset_degasperi.iloc[:, 0].tolist())\n",
    "parsed_text_degasperi = nlp_model(text_degasperi)\n",
    "\n",
    "text_fiction = ' '.join(dataset_fiction.iloc[:, 0].tolist())\n",
    "parsed_text_fiction = nlp_model(text_fiction)\n",
    "\n",
    "text_wikinews = ' '.join(dataset_wikinews.iloc[:, 0].tolist())\n",
    "parsed_text_wiki = nlp_model(text_wikinews[:1000000])\n",
    "\n",
    "text_moro = ' '.join(dataset_moro.iloc[:, 0].tolist())\n",
    "parsed_text_moro = nlp_model(text_moro[:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPRbor2ATNqR"
   },
   "outputs": [],
   "source": [
    "def merge_counters(counter1, counter2):\n",
    "\n",
    "  '''\n",
    "  Takes 2 counters with different shapes and in the smallest one ad also the key that are currently inside with a value of 0\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  counter1: Counter\n",
    "    Counter with smallest shape that has to be incremented\n",
    "  counter2: Counter\n",
    "    Counter with the larger shape\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  new_counter: Counter\n",
    "    Counter containing all the tuple of the smallest one and the tuples (key, 0) of the larger one that were no present in the small one\n",
    "  '''\n",
    "  new_counter = counter1    \n",
    "\n",
    "  for key, value in counter2.items():\n",
    "    if key not in new_counter.keys():\n",
    "      new_counter[key] = 0 \n",
    "\n",
    "  return new_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzmUyHYDw-eR"
   },
   "outputs": [],
   "source": [
    "def plot_wordCloud_counters(counters):\n",
    "\n",
    "  '''\n",
    "  Takes as input a list of counters and it plots the wordCloud\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  cunters: list(Counter)\n",
    "    List of counters that has to be plotted. It does not require that all the counters has the same shape\n",
    "\n",
    "  '''\n",
    "\n",
    "  word_cloud_counter = Counter()\n",
    "  for counter in list_counters:\n",
    "    word_cloud_counter.update(counter)\n",
    "  # Generate a word cloud from the POS counts\n",
    "  wordcloud = WordCloud(background_color='white').generate_from_frequencies(word_cloud_counter)\n",
    "\n",
    "  # Plot the word cloud\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oFiL_p2unFX"
   },
   "outputs": [],
   "source": [
    "def plot_groupedBar_counters(counters):\n",
    "\n",
    "  '''\n",
    "  Takes as input a list of counters and it plots in the same bar chart the counts\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  cunters: list(Counter)\n",
    "    List of counters that has to be plotted. It does not require that all the counters has the same shape\n",
    "\n",
    "  '''\n",
    "  \n",
    "  max_length = max(map(len, list_counters))                         # max length of the counters\n",
    "  max_position = list(map(len, list_counters)).index(max_length)    # position in the list of the counter with max length\n",
    "\n",
    "  # For each counter that is not the one of maximum dimension I merge it with all the other ones. The result is a list with counters having all the same keys\n",
    "  for i in range(len(list_counters)):\n",
    "    if i != max_position:\n",
    "      list_counters[i] = merge_counters(list_counters[i], list_counters[max_position])\n",
    "\n",
    "  # We plot each counter inside the bar chart\n",
    "  x = np.arange(max_length)\n",
    "  width=0.2\n",
    "  multiplier = 0\n",
    "  for counter in list_counters:\n",
    "    offset = width * multiplier\n",
    "    labels, values = zip(*sorted(counter.items()))\n",
    "    plt.bar(x + offset, values, width=width)\n",
    "    multiplier += 1\n",
    "\n",
    "  plt.title(\"POS Tag Frequency Distribution Degasperi\")\n",
    "  plt.xlabel(\"POS Tag\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "  plt.xticks(x + width, sorted(list_counters[max_position]), rotation='vertical')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "6lIMU-RRiMsf",
    "outputId": "27b47598-98ff-4784-fbd2-e6c9cd5e889f"
   },
   "outputs": [],
   "source": [
    "# Get the frequency distribution of POS tags\n",
    "pos_freq_degasperi = Counter([token.pos_ for token in parsed_text_degasperi])\n",
    "pos_freq_fiction = Counter([token.pos_ for token in parsed_text_fiction])\n",
    "pos_freq_wiki = Counter([token.pos_ for token in parsed_text_wiki])\n",
    "pos_freq_moro = Counter([token.pos_ for token in parsed_text_moro])\n",
    "\n",
    "list_counters = [pos_freq_degasperi, pos_freq_fiction, pos_freq_wiki, pos_freq_moro]\n",
    "plot_groupedBar_counters(list_counters)\n",
    "plot_wordCloud_counters(list_counters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sentences = get_sentences_list_from_df(dataset_degasperi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame(columns=['docno', 'text'])\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences_df.loc[i] = [f'd{i}', ' '.join(sentences[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      docno                                               text\n0        d0  il ministro degli esteri al commissario capo d...\n1        d1  sono in possesso del testo italiano di detto o...\n2        d2  mi consenta caro ammiraglio stone di esporle b...\n3        d3  l' ordine n. 14 è senza dubbio una misura impo...\n4        d4  detto ordine però perde il suo carattere di mi...\n...     ...                                                ...\n3788  d3788  crede che la guerra non sia inevitabile ma que...\n3789  d3789  gli uomini si sono dimostrati inferiori agli e...\n3790  d3790  con l' aiuto della provvidenza e con lo sforzo...\n3791  d3791  [...] l' azione di governo non può essere sugg...\n3792  d3792                                                   \n\n[3793 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>docno</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d0</td>\n      <td>il ministro degli esteri al commissario capo d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d1</td>\n      <td>sono in possesso del testo italiano di detto o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d2</td>\n      <td>mi consenta caro ammiraglio stone di esporle b...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d3</td>\n      <td>l' ordine n. 14 è senza dubbio una misura impo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d4</td>\n      <td>detto ordine però perde il suo carattere di mi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3788</th>\n      <td>d3788</td>\n      <td>crede che la guerra non sia inevitabile ma que...</td>\n    </tr>\n    <tr>\n      <th>3789</th>\n      <td>d3789</td>\n      <td>gli uomini si sono dimostrati inferiori agli e...</td>\n    </tr>\n    <tr>\n      <th>3790</th>\n      <td>d3790</td>\n      <td>con l' aiuto della provvidenza e con lo sforzo...</td>\n    </tr>\n    <tr>\n      <th>3791</th>\n      <td>d3791</td>\n      <td>[...] l' azione di governo non può essere sugg...</td>\n    </tr>\n    <tr>\n      <th>3792</th>\n      <td>d3792</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>3793 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:27:27.443 [main] ERROR org.terrier.structures.Index - Cannot create new index: path C:\\Users\\Mike\\PycharmProjects\\NLP_Project\\notebook\\.\\var\\./index_3docs does not exist, or cannot be written to\r\n"
     ]
    },
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: Cannot create new index: path C:\\Users\\Mike\\PycharmProjects\\NLP_Project\\notebook\\.\\var\\./index_3docs does not exist, or cannot be written to java.lang.IllegalArgumentException",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mJavaException\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m indexer \u001B[38;5;241m=\u001B[39m pt\u001B[38;5;241m.\u001B[39mDFIndexer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./index_3docs\u001B[39m\u001B[38;5;124m\"\u001B[39m, overwrite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mindexer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentences_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdocno\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m indexer\u001B[38;5;241m.\u001B[39mfinalize()\n\u001B[0;32m      4\u001B[0m index_ref \u001B[38;5;241m=\u001B[39m pt\u001B[38;5;241m.\u001B[39mIndexRef(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./index_3docs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLP_Project\\lib\\site-packages\\pyterrier\\index.py:674\u001B[0m, in \u001B[0;36mDFIndexer.index\u001B[1;34m(self, text, *args, **kwargs)\u001B[0m\n\u001B[0;32m    672\u001B[0m     javaDocCollection \u001B[38;5;241m=\u001B[39m TQDMSizeCollection(javaDocCollection, \u001B[38;5;28mlen\u001B[39m(text)) \n\u001B[0;32m    673\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreateIndexer()\n\u001B[1;32m--> 674\u001B[0m \u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoclass\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.terrier.python.PTUtils\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmakeCollection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjavaDocCollection\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m lastdoc\n\u001B[0;32m    676\u001B[0m lastdoc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mjnius\\jnius_export_class.pxi:1177\u001B[0m, in \u001B[0;36mjnius.JavaMultipleMethod.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mjnius\\jnius_export_class.pxi:885\u001B[0m, in \u001B[0;36mjnius.JavaMethod.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mjnius\\jnius_export_class.pxi:982\u001B[0m, in \u001B[0;36mjnius.JavaMethod.call_method\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mjnius\\jnius_utils.pxi:91\u001B[0m, in \u001B[0;36mjnius.check_exception\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mJavaException\u001B[0m: JVM exception occurred: Cannot create new index: path C:\\Users\\Mike\\PycharmProjects\\NLP_Project\\notebook\\.\\var\\./index_3docs does not exist, or cannot be written to java.lang.IllegalArgumentException"
     ]
    }
   ],
   "source": [
    "indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True)\n",
    "indexer.index(sentences_df[\"text\"], sentences_df[\"docno\"])\n",
    "indexer.finalize()\n",
    "index_ref = pt.IndexRef(\"./index_3docs\")\n",
    "index_ref.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "index = pt.IndexFactory.of(index_ref)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "print(index.getCollectionStatistics().toString())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SoDrRGFIgFpZ"
   },
   "source": [
    "# CODICE ELIA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gUVSRpgmEVNV"
   },
   "source": [
    "# KIND DATASET\n",
    "\n",
    "## Brief Description\n",
    "KIND (Kessler Italian Named-entities Dataset) is a dataset released in 2022 by researchers from Fondazione Bruno Kessler and the University of Trento. It contains 1 million tokens, of which 600K name-entities are manually annotated. The entities belong to 3 classes (people, location, organization). The texts come from various sources of the Italian language, such as news articles, literature, and political speeches, making it a multi-domain dataset. \n",
    "The following table shows the dataset composition:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\">Dataset</th>\n",
    "      <th rowspan=\"2\">Documents</th>\n",
    "      <th rowspan=\"2\">Tokens</th>\n",
    "      <th colspan=\"4\">Train</th>\n",
    "      <th colspan=\"4\">Test</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Total</th>\n",
    "      <th>PER</th>\n",
    "      <th>ORG</th>\n",
    "      <th>LOC</th>\n",
    "      <th>Total</th>\n",
    "      <th>PER</th>\n",
    "      <th>ORG</th>\n",
    "      <th>LOC</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Wikinews</td>\n",
    "      <td>1,000</td>\n",
    "      <td>308,622</td>\n",
    "      <td>247,528</td>\n",
    "      <td>8,928</td>\n",
    "      <td>7,593</td>\n",
    "      <td>6,862</td>\n",
    "      <td>61,094</td>\n",
    "      <td>1,802</td>\n",
    "      <td>1,823</td>\n",
    "      <td>1,711</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Fiction</td>\n",
    "      <td>86</td>\n",
    "      <td>192,448</td>\n",
    "      <td>170,942</td>\n",
    "      <td>3,439</td>\n",
    "      <td>182</td>\n",
    "      <td>733</td>\n",
    "      <td>21,506</td>\n",
    "      <td>636</td>\n",
    "      <td>284</td>\n",
    "      <td>463</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Aldo Moro</td>\n",
    "      <td>250</td>\n",
    "      <td>392,604</td>\n",
    "      <td>309,798</td>\n",
    "      <td>1,459</td>\n",
    "      <td>4,842</td>\n",
    "      <td>2,024</td>\n",
    "      <td>82,806</td>\n",
    "      <td>282</td>\n",
    "      <td>934</td>\n",
    "      <td>807</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Alcide De Gasperi</td>\n",
    "      <td>158</td>\n",
    "      <td>150,632</td>\n",
    "      <td>117,997</td>\n",
    "      <td>1,129</td>\n",
    "      <td>2,396</td>\n",
    "      <td>1,046</td>\n",
    "      <td>32,635</td>\n",
    "      <td>253</td>\n",
    "      <td>533</td>\n",
    "      <td>274</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Total</strong></td>\n",
    "      <td><strong>1494</strong></td>\n",
    "      <td><strong>1,044,306</strong></td>\n",
    "      <td><strong>846,265</strong></td>\n",
    "      <td><strong>14,955</strong></td>\n",
    "      <td><strong>15,013</strong></td>\n",
    "      <td><strong>10,665</strong></td>\n",
    "      <td><strong>198,041</strong></td>\n",
    "      <td><strong>2,973</strong></td>\n",
    "      <td><strong>3,574</strong></td>\n",
    "      <td><strong>3,255</strong></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "##Annotation Tagging scheme\n",
    "The tokens of all datasets, except for Aldo Moro, have been manually labeled using the IOB (Inside-Outside-Beginning) convention: each entity is labeled as begin-of-entity (B-[ent]) or continuation-of-entity (I-[ent]). The annotations of the Aldo Moro dataset, instead, were carried out with a mixed process that used both manual and automatic annotations (subsequently checked by hand); due to some differences in the convention for annotation, this dataset does not contain information for composite entities (beginning, continuation). For more details, please refer to the paper related with the dataset release: https://arxiv.org/abs/2112.15099\n",
    "\n",
    "An example of the annotations is reported here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "durfizxkrnhW"
   },
   "outputs": [],
   "source": [
    "# IOB annotation (from ADG_dev.tsv)\n",
    "Nel\tO\n",
    "nostro\tO\n",
    "Trentino\tB-LOC\n",
    "attraversiamo\tO\n",
    "un\tO\n",
    "momento\tO\n",
    "storico\tO\n",
    "importante\tO\n",
    ".\tO\n",
    "\n",
    "# non-IOB annotation (from moro_test.tsv)\n",
    "Dal\tO\n",
    "Consiglio\tORG\n",
    "nazionale\tORG\n",
    "del\tO\n",
    "‘\tO\n",
    "75\tO\n",
    "la\tO\n",
    "grande\tO\n",
    "stampa\tO\n",
    "parla\tO\n",
    "di\tO\n",
    "due\tO\n",
    "anime\tO\n",
    "contrapposte\tO\n",
    "del\tO\n",
    "partito\tO\n",
    ".\tO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i9ejTFLxdf_z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUxTIXvXdgoV"
   },
   "outputs": [],
   "source": [
    "def to_lowerCase(df):\n",
    "    return pd.DataFrame({'Token': df['Token'].str.lower(), 'Entity': df['Entity']})\n",
    "\n",
    "def add_column_names(df):\n",
    "    return  df.rename(columns={0: 'Token', 1: 'Entity'})\n",
    "\n",
    "\n",
    "ds = {'ds_dg' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/degasperi_train.tsv', sep='\\t', header=None),\n",
    "      'ds_mr' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/moro_train.tsv', sep='\\t', header=None),\n",
    "      'ds_fc' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/fiction_train.tsv', sep='\\t', header=None),\n",
    "      'ds_wn' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/wikinews_train.tsv', sep='\\t', header=None),\n",
    "      \n",
    "      'ds_dg_test' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/degasperi_test.tsv', sep='\\t', header=None),\n",
    "      'ds_mr_test' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/moro_test.tsv', sep='\\t', header=None),\n",
    "      'ds_fc_test' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/fiction_test.tsv', sep='\\t', header=None),\n",
    "      'ds_wn_test' : pd.read_csv('./KIND_project/dataset/KIND-main/dataset/wikinews_test.tsv', sep='\\t', header=None),\n",
    "      \n",
    "      'ds_dg_IOB' : pd.read_csv('./KIND_project/dataset/KIND-main/evalita-2023/ADG_train.tsv', sep='\\t', header=None),\n",
    "      'ds_fc_IOB' : pd.read_csv('./KIND_project/dataset/KIND-main/evalita-2023/FIC_train.tsv', sep='\\t', header=None),\n",
    "      'ds_wn_IOB' : pd.read_csv('./KIND_project/dataset/KIND-main/evalita-2023/WN_train.tsv', sep='\\t', header=None),\n",
    "      \n",
    "      'ds_dg_IOB_test' : pd.read_csv('./KIND_project/dataset/KIND-main/evalita-2023/ADG_dev.tsv', sep='\\t', header=None),\n",
    "      'ds_fc_IOB_test' : pd.read_csv('./KIND_project/dataset/KIND-main/evalita-2023/FIC_dev.tsv', sep='\\t', header=None),\n",
    "      'ds_wn_IOB_test' : pd.read_csv('./KIND_project/dataset/KIND-main/evalita-2023/WN_dev.tsv', sep='\\t', header=None),\n",
    "}\n",
    "\n",
    "# lowercasing tokens, adding column names, compute some stats\n",
    "avg_doc_len = 0\n",
    "avg_voc_size = 0\n",
    "\n",
    "stats = {}\n",
    "\n",
    "for i in ds.keys():     \n",
    "\n",
    "    stats[i] = {\n",
    "        'doc_len' : ds[i]['Token'].count(),\n",
    "        'voc_size' : ds[i]['Token'].nunique(),\n",
    "\n",
    "        'n_O' : sum(1 for k in ds[i]['Entity'] if k == 'O'),\n",
    "        'n_PER' : sum(1 for k in ds[i]['Entity'] if k == 'PER'), \n",
    "        'n_ORG' : sum(1 for k in ds[i]['Entity'] if k == 'ORG'),\n",
    "        'n_LOC' : sum(1 for k in ds[i]['Entity'] if k == 'LOC'),\n",
    "\n",
    "        'n_I-PER' : sum(1 for k in ds[i]['Entity'] if k == 'I-PER'), \n",
    "        'n_I-ORG' : sum(1 for k in ds[i]['Entity'] if k == 'I-ORG'),\n",
    "        'n_I-LOC' : sum(1 for k in ds[i]['Entity'] if k == 'I-LOC'),\n",
    "\n",
    "        'n_B-PER' : sum(1 for k in ds[i]['Entity'] if k == 'B-PER'), \n",
    "        'n_B-ORG' : sum(1 for k in ds[i]['Entity'] if k == 'B-ORG'),\n",
    "        'n_B-LOC' : sum(1 for k in ds[i]['Entity'] if k == 'B-LOC'),\n",
    "\n",
    "        'n_punct': sum(1 for k in ds[i]['Token'] if all(char in string.punctuation for char in k)),\n",
    "    }\n",
    "  \n",
    "avg_doc_len = sum(stats[i]['doc_len'] for i in stats.keys()) / len(stats.keys())\n",
    "avg_voc_size = sum(stats[i]['voc_size'] for i in stats.keys()) / len(stats.keys())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
